{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Program Files (x86)\\Microsoft Visual Studio\\Shared\\Anaconda3_64\\lib\\site-packages\\sklearn\\cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn import metrics\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import xgboost as xgb\n",
    "import gc\n",
    "import datetime\n",
    "import sys\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "########################step1  原始数据提取##########################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#当有其他数据集合训练集合并后，务必更新索引\n",
    "test_a_data_y1 = pd.read_csv(\"test_a_data_y1.csv\")\n",
    "train_data = pd.concat([train_data, test_a_data_y1],axis=0)\n",
    "test_a_result.to_csv(\"test_a_result.csv\"\n",
    "train_data.reset_index(inplace=True,drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['id', 'org_label', 'date', 'label']"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "['id', 'date']"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#data_path = \"G:/program_monkey/ML/atec/\"\n",
    "data_path = \"D:/fangshengjie/jupyter_workspace/atec/data/\"\n",
    "train_data_path = data_path + \"atec_anti_fraud_train.csv\"\n",
    "test_b_data_path = data_path + \"atec_anti_fraud_test_b.csv\"\n",
    "test_a_data_path = data_path + \"atec_anti_fraud_test_a.csv\"\n",
    "train_data = pd.read_csv(train_data_path)\n",
    "test_b_data = pd.read_csv(test_b_data_path)\n",
    "\n",
    "#保留原始的label，并生成新的label，\n",
    "#将高风险的失败交易数据定义成坏样本。据说可以提高0.1\n",
    "train_data[\"org_label\"] = train_data[\"label\"]\n",
    "train_data.label.replace(to_replace=-1, value=1, inplace=True)\n",
    "\n",
    "#模型需要剔除的变量名和训练集划分时间戳\n",
    "id_columns = [\"id\"]\n",
    "custom_columns = [\"org_label\"]\n",
    "label_columns = [\"label\"]\n",
    "timestamp_columns_name = \"date\"\n",
    "timestamp_columns = [timestamp_columns_name]\n",
    "train_columns_drop = id_columns + custom_columns + timestamp_columns + label_columns\n",
    "test_columns_drop = id_columns + timestamp_columns\n",
    "train_data_timestamp = 20171020\n",
    "display(train_columns_drop)\n",
    "display(test_columns_drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20170905"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "20171105"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "20180206"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "20180306"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(train_data_f_rf.date.min())\n",
    "display(train_data_f_rf.date.max())\n",
    "display(test_b_data_f_rf.date.min())\n",
    "display(test_b_data_f_rf.date.max())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "########################step2  缺失值处理##########################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f5\n",
      "f20\n",
      "f21\n",
      "f22\n",
      "f23\n",
      "f24\n",
      "f25\n",
      "f26\n",
      "f27\n",
      "f28\n",
      "f29\n",
      "f30\n",
      "f31\n",
      "f32\n",
      "f33\n",
      "f34\n",
      "f35\n",
      "f48\n",
      "f49\n",
      "f50\n",
      "f51\n",
      "f52\n",
      "f53\n",
      "f54\n",
      "f55\n",
      "f56\n",
      "f57\n",
      "f58\n",
      "f59\n",
      "f60\n",
      "f61\n",
      "f62\n",
      "f63\n",
      "f64\n",
      "f65\n",
      "f66\n",
      "f67\n",
      "f68\n",
      "f69\n",
      "f70\n",
      "f71\n",
      "f72\n",
      "f73\n",
      "f74\n",
      "f75\n",
      "f76\n",
      "f77\n",
      "f78\n",
      "f79\n",
      "f80\n",
      "f81\n",
      "f82\n",
      "f83\n",
      "f84\n",
      "f85\n",
      "f86\n",
      "f87\n",
      "f88\n",
      "f89\n",
      "f90\n",
      "f91\n",
      "f92\n",
      "f93\n",
      "f94\n",
      "f95\n",
      "f96\n",
      "f97\n",
      "f98\n",
      "f99\n",
      "f100\n",
      "f101\n",
      "f102\n",
      "f103\n",
      "f104\n",
      "f105\n",
      "f106\n",
      "f107\n",
      "f108\n",
      "f109\n",
      "f110\n",
      "f111\n",
      "f112\n",
      "f113\n",
      "f114\n",
      "f115\n",
      "f116\n",
      "f117\n",
      "f118\n",
      "f119\n",
      "f120\n",
      "f121\n",
      "f122\n",
      "f123\n",
      "f124\n",
      "f125\n",
      "f126\n",
      "f127\n",
      "f128\n",
      "f129\n",
      "f130\n",
      "f131\n",
      "f132\n",
      "f133\n",
      "f134\n",
      "f135\n",
      "f136\n",
      "f137\n",
      "f138\n",
      "f139\n",
      "f140\n",
      "f141\n",
      "f142\n",
      "f143\n",
      "f144\n",
      "f145\n",
      "f146\n",
      "f147\n",
      "f148\n",
      "f149\n",
      "f150\n",
      "f151\n",
      "f152\n",
      "f153\n",
      "f154\n",
      "f155\n",
      "f156\n",
      "f157\n",
      "f158\n",
      "f159\n",
      "f160\n",
      "f161\n",
      "f162\n",
      "f163\n",
      "f164\n",
      "f165\n",
      "f166\n",
      "f167\n",
      "f168\n",
      "f169\n",
      "f170\n",
      "f171\n",
      "f172\n",
      "f173\n",
      "f174\n",
      "f175\n",
      "f176\n",
      "f177\n",
      "f178\n",
      "f179\n",
      "f180\n",
      "f181\n",
      "f182\n",
      "f183\n",
      "f184\n",
      "f185\n",
      "f186\n",
      "f187\n",
      "f188\n",
      "f189\n",
      "f190\n",
      "f191\n",
      "f192\n",
      "f193\n",
      "f194\n",
      "f195\n",
      "f196\n",
      "f197\n",
      "f198\n",
      "f199\n",
      "f200\n",
      "f201\n",
      "f202\n",
      "f203\n",
      "f204\n",
      "f205\n",
      "f206\n",
      "f207\n",
      "f208\n",
      "f209\n",
      "f210\n",
      "f211\n",
      "f212\n",
      "f213\n",
      "f214\n",
      "f215\n",
      "f216\n",
      "f217\n",
      "f218\n",
      "f219\n",
      "f220\n",
      "f221\n",
      "f222\n",
      "f223\n",
      "f224\n",
      "f225\n",
      "f226\n",
      "f227\n",
      "f228\n",
      "f229\n",
      "f230\n",
      "f231\n",
      "f232\n",
      "f233\n",
      "f234\n",
      "f235\n",
      "f236\n",
      "f237\n",
      "f238\n",
      "f239\n",
      "f240\n",
      "f241\n",
      "f242\n",
      "f243\n",
      "f244\n",
      "f245\n",
      "f246\n",
      "f247\n",
      "f248\n",
      "f249\n",
      "f250\n",
      "f251\n",
      "f252\n",
      "f253\n",
      "f254\n",
      "f255\n",
      "f256\n",
      "f257\n",
      "f258\n",
      "f259\n",
      "f260\n",
      "f261\n",
      "f262\n",
      "f263\n",
      "f264\n",
      "f265\n",
      "f266\n",
      "f267\n",
      "f268\n",
      "f269\n",
      "f270\n",
      "f271\n",
      "f272\n",
      "f273\n",
      "f274\n",
      "f275\n",
      "f276\n",
      "f277\n",
      "f278\n",
      "f279\n",
      "f280\n",
      "f281\n",
      "f282\n",
      "f283\n",
      "f284\n",
      "f285\n",
      "f286\n",
      "f287\n",
      "f288\n",
      "f289\n",
      "f290\n",
      "f291\n",
      "f292\n",
      "f293\n",
      "f294\n",
      "f295\n",
      "f296\n",
      "f297\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "6034"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "###################traindata#####################\n",
    "######2.1样本缺失处理 计算每条样本的非缺失个数，剔除缺失个数大于一定比例的样本\n",
    "#提出样本中变量缺失值个数大于240(300*0.2)的样本 约700个\n",
    "sample_miss_count_and_remove(train_data, \"dismiss_count\", 0.8)\n",
    "#2.2剔除缺失率超过0.9的变量\n",
    "s_traindata = variable_miss_count_and_remove(train_data, 0.9)\n",
    "missins_fill(train_data, s_traindata, distinct_threshold=0)\n",
    "train_data.reset_index(drop=True, inplace=True)\n",
    "#train_data.to_csv(\"train_data_clearned.csv\", index=False)\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "########################step3  随机森林筛选变量##########################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Program Files (x86)\\Microsoft Visual Studio\\Shared\\Anaconda3_64\\lib\\site-packages\\ipykernel_launcher.py:9: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  if __name__ == '__main__':\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=50, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=100, min_samples_split=100,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=1000, n_jobs=-1,\n",
       "            oob_score=False, random_state=0, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#划分训练集和验证集,并进行建模\n",
    "train_dataset = train_data.ix[train_data[timestamp_columns_name] < train_data_timestamp, :]\n",
    "verify_dataset = train_data.ix[train_data[timestamp_columns_name] >= train_data_timestamp, :]\n",
    "train_x = train_dataset.drop(labels=train_columns_drop, axis=1)\n",
    "train_y = train_dataset[label_columns]\n",
    "test_x = verify_dataset.drop(labels=train_columns_drop, axis=1)\n",
    "test_y = verify_dataset[label_columns]\n",
    "forest=RandomForestClassifier(n_estimators=1000,n_jobs=-1,random_state=0,max_depth=50,min_samples_split=100,min_samples_leaf=100)\n",
    "forest.fit(train_x,train_y)\n",
    "#rf_result = forest.predict(test_x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Program Files (x86)\\Microsoft Visual Studio\\Shared\\Anaconda3_64\\lib\\site-packages\\pandas\\core\\frame.py:2746: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  **kwargs)\n"
     ]
    }
   ],
   "source": [
    "#随即森林的模型特征评分\n",
    "rf_feature = pd.DataFrame(index=train_x.columns)\n",
    "importances = forest.feature_importances_\n",
    "rf_feature[\"score\"] = importances\n",
    "rf_feature.sort_values(\"score\", inplace=True, ascending=False)\n",
    "rf_feature_filt = rf_feature.loc[rf_feature.score > 0.001,:]\n",
    "rf_feature_filt.reset_index(inplace=True)\n",
    "rf_feature_filt.rename(columns={\"index\":\"feature\"}, inplace=True)\n",
    "rf_feature_columns = rf_feature_filt[\"feature\"].tolist()\n",
    "rf_feature_filt.to_csv(\"rf_feature_v2.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "102"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#生成清洗后的原始变量\n",
    "train_columns_f_rf = train_columns_drop + rf_feature_columns\n",
    "train_data_f_rf = train_data.loc[:, train_columns_f_rf]\n",
    "#train_data_f_rf.to_csv(\"train_data_f_rf.csv\", index=False)\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#从文件中读取随机森林变量\n",
    "rf_feature_filt = pd.read_csv(\"rf_feature.csv\")\n",
    "rf_feature_columns = rf_feature_filt[\"feature\"].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "########################step4  测试集处理##########################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'test_columns_drop' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-14-f88d2b3ee40a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0msample_miss_count_and_remove\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_a_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"dismiss_count\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;31m#获取随机森林筛选之后的变量\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[0mtest_columns_f_rf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtest_columns_drop\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mrf_feature_columns\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[0mtest_a_data_f_rf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtest_a_data\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_columns_f_rf\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;31m#获取缺失值统计表，不进行变量删除\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'test_columns_drop' is not defined"
     ]
    }
   ],
   "source": [
    "###################testdata a#####################\n",
    "#测试集原始数据\n",
    "test_a_data_path = data_path + \"atec_anti_fraud_test_a.csv\"\n",
    "test_a_data = pd.read_csv(test_a_data_path)\n",
    "#获取测试机的非缺失值个数作为一个变量，不进行样本删除\n",
    "sample_miss_count_and_remove(test_a_data, \"dismiss_count\", 1)\n",
    "#获取随机森林筛选之后的变量\n",
    "test_columns_f_rf = test_columns_drop + rf_feature_columns\n",
    "test_a_data_f_rf = test_a_data.loc[:, test_columns_f_rf]\n",
    "#获取缺失值统计表，不进行变量删除\n",
    "s_test_a_data = variable_miss_count_and_remove(test_a_data_f_rf, 1)\n",
    "#缺失值填充\n",
    "missins_fill(test_a_data_f_rf, s_test_a_data, distinct_threshold=0)\n",
    "\n",
    "#test_a_data_f_rf.to_csv(\"test_a_data_f_rf.csv\", index=False)\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f31\n",
      "f30\n",
      "f29\n",
      "f28\n",
      "f234\n",
      "f259\n",
      "f235\n",
      "f270\n",
      "f236\n",
      "f260\n",
      "f215\n",
      "f237\n",
      "f238\n",
      "f24\n",
      "f27\n",
      "f25\n",
      "f52\n",
      "f26\n",
      "f248\n",
      "f53\n",
      "f261\n",
      "f106\n",
      "f264\n",
      "f271\n",
      "f243\n",
      "f244\n",
      "f253\n",
      "f247\n",
      "f105\n",
      "f218\n",
      "f216\n",
      "f242\n",
      "f245\n",
      "f252\n",
      "f246\n",
      "f265\n",
      "f217\n",
      "f210\n",
      "f262\n",
      "f48\n",
      "f76\n",
      "f32\n",
      "f104\n",
      "f266\n",
      "f214\n",
      "f49\n",
      "f229\n",
      "f33\n",
      "f209\n",
      "f50\n",
      "f164\n",
      "f35\n",
      "f204\n",
      "f34\n",
      "f54\n",
      "f222\n",
      "f226\n",
      "f263\n",
      "f82\n",
      "f51\n",
      "f84\n",
      "f110\n",
      "f75\n",
      "f241\n",
      "f85\n",
      "f73\n",
      "f83\n",
      "f77\n",
      "f213\n",
      "f208\n",
      "f225\n",
      "f86\n",
      "f63\n",
      "f251\n",
      "f101\n",
      "f81\n",
      "f55\n",
      "f228\n",
      "f78\n",
      "f100\n",
      "f103\n",
      "f109\n",
      "f163\n",
      "f80\n",
      "f291\n",
      "f62\n",
      "f72\n",
      "f240\n",
      "f5\n",
      "f221\n",
      "f250\n",
      "f74\n",
      "f21\n",
      "f23\n",
      "f22\n",
      "f79\n",
      "f205\n",
      "f207\n",
      "f227\n",
      "f212\n",
      "f95\n",
      "f185\n",
      "f162\n",
      "f99\n",
      "f278\n",
      "f90\n",
      "f224\n",
      "f20\n",
      "f89\n",
      "f220\n",
      "f61\n",
      "f292\n",
      "f102\n",
      "f96\n",
      "f206\n",
      "f282\n",
      "f283\n",
      "f58\n",
      "f279\n",
      "f91\n",
      "f211\n",
      "f284\n",
      "f285\n",
      "f281\n",
      "f57\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1221"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "###################testdata b#####################\n",
    "#测试集原始数据\n",
    "test_b_data = pd.read_csv(test_b_data_path)\n",
    "#获取测试机的非缺失值个数作为一个变量，不进行样本删除\n",
    "sample_miss_count_and_remove(test_b_data, \"dismiss_count\", 1)\n",
    "#获取随机森林筛选之后的变量\n",
    "test_columns_f_rf = test_columns_drop + rf_feature_columns\n",
    "test_b_data_f_rf = test_b_data.loc[:, test_columns_f_rf]\n",
    "#获取缺失值统计表，不进行变量删除\n",
    "s_test_b_data = variable_miss_count_and_remove(test_b_data_f_rf, 1)\n",
    "#缺失值填充\n",
    "missins_fill(test_b_data_f_rf, s_test_b_data, distinct_threshold=0)\n",
    "\n",
    "#test_b_data_f_rf.to_csv(\"test_b_data_f_rf.csv\", index=False)\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#########################step 5对疑似因子型变量进行onehot##################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "org_label    4\n",
       "label        2\n",
       "f15          3\n",
       "f14          3\n",
       "f18          3\n",
       "f17          3\n",
       "f19          3\n",
       "f12          3\n",
       "f4           3\n",
       "f11          3\n",
       "f1           3\n",
       "f3           3\n",
       "f9           3\n",
       "Name: train_data, dtype: int64"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#区分离散型和连续型变量\n",
    "t_feature_distinct = train_data_f_rf.apply(lambda x: x.value_counts().size).to_frame()\n",
    "t_feature_distinct[\"test_data\"] = test_b_data_f_rf.apply(lambda x: x.value_counts().size)\n",
    "t_feature_distinct.rename(columns={0:\"train_data\"},inplace=True)\n",
    "t_feature_distinct.loc[t_feature_distinct.train_data <5,\"train_data\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "org_label    4\n",
       "label        2\n",
       "f7           8\n",
       "f6           5\n",
       "f15          3\n",
       "f14          3\n",
       "f18          3\n",
       "f17          3\n",
       "f19          3\n",
       "f12          3\n",
       "f4           3\n",
       "f11          3\n",
       "f1           3\n",
       "f3           3\n",
       "f9           3\n",
       "f99          9\n",
       "f61          9\n",
       "Name: train_data, dtype: int64"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_feature_distinct.loc[t_feature_distinct.train_data <10,\"train_data\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#根据数据分析，获得离散型特征列表\n",
    "factor_feature = ['f15', 'f18', 'f14', 'f19', 'f17', 'f12', 'f4',\n",
    "       'f1', 'f3', 'f9', 'f11']\n",
    "numeric_feature = list(set(rf_feature_filt.feature.tolist()) - set(factor_feature))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "#先对训练集和测试机合并，再进行onehot，避免出现onehot不一致\n",
    "train_and_test_factor_data = pd.concat([train_data_f_rf[[\"id\"]+ factor_feature],test_b_data_f_rf[[\"id\"]+ factor_feature]])\n",
    "train_and_test_factor_data.reset_index(inplace=True, drop=True)\n",
    "\n",
    "train_and_test_onehot_data = pd.get_dummies(train_and_test_factor_data[factor_feature], columns=factor_feature)\n",
    "train_and_test_onehot_data = pd.concat([train_and_test_factor_data[\"id\"],train_and_test_onehot_data],axis=1)\n",
    "display(train_and_test_onehot_data.shape)\n",
    "\n",
    "train_feature = pd.merge(train_data_f_rf[train_columns_drop + numeric_feature], train_and_test_onehot_data, how=\"left\", on=\"id\")\n",
    "test_feature =pd.merge(test_b_data_f_rf[test_columns_drop + numeric_feature], train_and_test_onehot_data,  how=\"left\", on=\"id\")\n",
    "\n",
    "train_feature.to_csv(\"train_feature_v2.csv\", index=False)\n",
    "test_feature.to_csv(\"test_feature_v2.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#单独进行训练集合测试集的onehot\n",
    "train_factor_data = pd.get_dummies(train_data_f_rf[factor_feature], columns=factor_feature)\n",
    "train_feature = pd.concat([train_data_f_rf[train_columns_drop + numeric_feature], train_factor_data], axis=1)\n",
    "display(train_feature.shape)\n",
    "\n",
    "test_factor_data = pd.get_dummies(test_b_data_f_rf[factor_feature], columns=factor_feature)\n",
    "test_feature = pd.concat([test_b_data_f_rf[test_columns_drop + numeric_feature], test_factor_data], axis=1)\n",
    "display(test_feature.shape)\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(491668, 165)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "84"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#单独进行测试集a的onehot\n",
    "test_factor_data = pd.get_dummies(test_a_data_f_rf[factor_feature], columns=factor_feature)\n",
    "test_a_feature = pd.concat([test_a_data_f_rf[test_columns_drop + numeric_feature], test_factor_data], axis=1)\n",
    "display(test_a_feature.shape)\n",
    "test_a_feature.to_csv(\"test_a_feature.csv\", index=False)\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_feature.to_csv(\"train_feature_v2.csv\", index=False)\n",
    "test_feature.to_csv(\"test_feature_v2.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5431"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#######################缺失值处理接口##########################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#计算每条样本的非缺失个数,,作为一个变量记录在原始表格中，剔除缺失个数大于一定比例的样本\n",
    "def sample_miss_count_and_remove(data, dismiss_count_name, miss_rate=0.2):\n",
    "    dismiss_rate = 1 - miss_rate\n",
    "    dismiss_row_range = data.shape[1] * dismiss_rate\n",
    "    data[dismiss_count_name] = data.apply(lambda x: x.count(), axis=1)\n",
    "    drop_row_index = data.loc[data.dismiss_count<dismiss_row_range,\"dismiss_count\"].index\n",
    "    data.drop(drop_row_index, inplace=True, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#剔除缺失率超过0.9的变量\n",
    "def variable_miss_count_and_remove(data, dismiss_count_name, miss_rate = 0.9):\n",
    "    s_data = get_summary_data(data, np.NaN)\n",
    "    drop_columns_index = s_data.loc[s_data.miss_rate > miss_rate,:].index\n",
    "    data.drop(drop_columns_index, inplace=True, axis=1)\n",
    "\n",
    "    #剔除汇总表中的数据\n",
    "    s_data.drop(drop_columns_index, inplace=True, axis=0)\n",
    "    return s_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#同分布缺失值随机填充\n",
    "def missins_fill(data, summary_data, distinct_threshold = 0):\n",
    "    summary_missing_data = summary_data.loc[summary_data.miss_count > 0, :]\n",
    "    miss_feature = summary_missing_data.loc[summary_missing_data.distinct_count > distinct_threshold,:]\n",
    "    #miss_feature = summary_missing_data.loc[(summary_missing_data.distinct_count >600) &(summary_missing_data.distinct_count <700),:]\n",
    "    data_row_num = data.shape[0]\n",
    "    for feature in miss_feature.index[:]:\n",
    "        print(feature)\n",
    "        feature_dismiss_num = data[feature].count()\n",
    "        miss_num = data_row_num - feature_dismiss_num\n",
    "        tmp1 = data[feature].value_counts()\n",
    "        tmp1 = tmp1.to_frame()\n",
    "        tmp1[\"count\"] = (tmp1/feature_dismiss_num * summary_missing_data.loc[feature,\"miss_count\"]).astype(int)\n",
    "        #按非缺失值比例填充，填充个数转换为整形后会出现填充个数小于实际缺失值的情况，\n",
    "        #因此将剩余缺失个数补充到变量的最大个数组\n",
    "        tmp1.iloc[0, 1] = tmp1.iloc[0, 1] + miss_num - tmp1[\"count\"].sum()\n",
    "        #当缺失个数按比例计算的到的值大于0时才进行相应的填充，避免不必要的循环\n",
    "        tmp1 = tmp1.loc[tmp1[\"count\"]>0,:]\n",
    "        miss_index = data.loc[data[feature].isnull(),[feature]]\n",
    "        np.random.seed(5)\n",
    "        miss_index[\"rand\"] = np.random.randn(miss_index.shape[0])\n",
    "        miss_index.sort_values(\"rand\", inplace=True)\n",
    "        start_index = 0\n",
    "        end_index = 0\n",
    "        for i in tmp1.itertuples():\n",
    "            end_index = start_index + i.count\n",
    "            miss_index.iloc[start_index:end_index,0] = np.full(i.count, i.Index)\n",
    "            start_index = end_index\n",
    "        data.loc[miss_index.index, feature] = miss_index[feature]\n",
    "        summary_data.drop(feature, inplace=True, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#变量简要分析\n",
    "def get_summary_data(data, miss_symbol):\n",
    "    input_data = data\n",
    "    input_data.replace(miss_symbol, np.NaN, inplace=True)\n",
    "    distinct_count = input_data.apply(lambda x: x.unique().size)\n",
    "    dismiss_count = input_data.apply(lambda x: x.count())\n",
    "    distinct_count.rename(\"distinct_count\", inplace=True)\n",
    "    dismiss_count.rename(\"dismiss_count\", inplace=True)\n",
    "    summary_data = pd.concat([distinct_count, dismiss_count],axis=1)\n",
    "    summary_data.insert(loc=0, column=\"row_index\",value=range(summary_data.index.size))\n",
    "    summary_data[\"miss_count\"] = input_data.index.size - summary_data.dismiss_count\n",
    "    summary_data[\"variable_type\"] = input_data.dtypes\n",
    "    summary_data[\"miss_rate\"] = summary_data[\"miss_count\"] / input_data.shape[0]\n",
    "    #summary_data[\"min_value\"] = input_data.min()\n",
    "    #summary_data[\"max_value\"] = input_data.max()\n",
    "    #summary_data[\"row_index\"] = range(summary_data.index.size)\n",
    "    #不能先重建索引，否则后面就无法根据索引新增数据了\n",
    "    #summary_data.reset_index(inplace=True)\n",
    "    #summary_data.rename(columns={\"index\":\"column_name\"}, inplace=True)\n",
    "    return summary_data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
