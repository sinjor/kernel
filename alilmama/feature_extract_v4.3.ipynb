{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\anaconda\\lib\\site-packages\\sklearn\\cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn import datasets\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn import metrics\n",
    "import xgboost as xgb\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "import gc\n",
    "import sys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##############################round 2###########################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "summary = train_data.loc[(train_data.is_trade == 1.0) & (train_data.is_trade.isnull() ==False),\"context_timestamp_day\"].value_counts()\n",
    "summary = summary.to_frame()\n",
    "summary.rename(columns={\"context_timestamp_day\":\"y1_sample_count\"}, inplace=True)\n",
    "summary.sort_index(inplace=True)\n",
    "summary[\"sample_count\"] = train_data.loc[train_data.is_trade.isnull() ==False,\"context_timestamp_day\"].value_counts()\n",
    "summary[\"y1_sample_rate\"] = summary[\"y1_sample_count\"] / summary[\"sample_count\"]\n",
    "user_id_dist = list()\n",
    "for i in range(8):\n",
    "    user_id_dist.append(train_data.loc[train_data.context_timestamp_day == i, \"user_id\"].unique().size)\n",
    "summary[\"user_id_dist_count\"] = user_id_dist\n",
    "\n",
    "timestamp_list = list()\n",
    "time_str_list = [\"20180831_120000\", \"20180901_120000\", \"20180902_120000\", \"20180903_120000\", \n",
    "             \"20180904_120000\", \"20180905_120000\", \"20180906_120000\",\"20180907_120000\"]\n",
    "for timestr in time_str_list:\n",
    "    timestamp_list.append(int(datetime.datetime.strptime(timestr,\"%Y%m%d_%H%M%S\").timestamp())) \n",
    "    \n",
    "# is_trade_am = list()\n",
    "# is_trade_pm = list()\n",
    "# click_count_am = list()\n",
    "# click_count_pm = list()\n",
    "# for i in range(8):\n",
    "#     is_trade_am.append(train_data.loc[(train_data.context_timestamp_day == i) & (train_data.context_timestamp < timestamp_list[i]) & (train_data.is_trade == 1.0) , \"is_trade\"].count())\n",
    "#     click_count_am.append(train_data.loc[(train_data.context_timestamp_day == i) & (train_data.context_timestamp < timestamp_list[i]), \"is_trade\"].count())\n",
    "#     is_trade_pm.append(train_data.loc[(train_data.context_timestamp_day == i) & (train_data.context_timestamp >= timestamp_list[i]) & (train_data.is_trade == 1.0) , \"is_trade\"].count())\n",
    "#     click_count_pm.append(train_data.loc[(train_data.context_timestamp_day == i) & (train_data.context_timestamp >= timestamp_list[i]), \"is_trade\"].count())\n",
    "    \n",
    "# summary[\"y1_sample_count_am\"] = is_trade_am\n",
    "# summary[\"y1_sample_count_pm\"] = is_trade_pm\n",
    "# summary[\"sample_count_am\"] = click_count_am\n",
    "# summary[\"sample_count_pm\"] = click_count_pm\n",
    "# summary[\"y1_sample_am_rate\"] = summary[\"y1_sample_count_am\"] / summary[\"sample_count_am\"]\n",
    "# summary[\"y1_sample_pm_rate\"] = summary[\"y1_sample_count_pm\"] / summary[\"sample_count_pm\"]\n",
    "\n",
    "#历史数据定义为2天内\n",
    "history_dist_user = list()\n",
    "sample_dist_user = list()\n",
    "history_sample = list()\n",
    "is_trade_sample = list()\n",
    "for i in range(8):\n",
    "    sample_user_id = train_data.loc[train_data.context_timestamp_day == i,\"user_id\"].unique()\n",
    "    history_user_id = train_data.loc[(train_data.context_timestamp_day >= i-2)&(train_data.context_timestamp_day < i),\"user_id\"].unique()\n",
    "    sample_history_user_id = np.intersect1d(history_user_id, sample_user_id)\n",
    "    history_dist_user.append(sample_history_user_id.size)\n",
    "    sample_dist_user.append(sample_user_id.size)\n",
    "    \n",
    "    sample_history_user_sample = pd.DataFrame(sample_history_user_id, columns=[\"user_id\"])\n",
    "    print(sample_history_user_sample.shape[0])\n",
    "    sample_history_user_sample = pd.merge(sample_history_user_sample, train_data.loc[(train_data.context_timestamp_day == i)\n",
    "                                &(train_data.is_trade.isnull()==False),[\"user_id\", \"is_trade\"]], how=\"left\", on=\"user_id\")\n",
    "    is_trade_sample.append(sample_history_user_sample.loc[sample_history_user_sample.is_trade == 1.0,\"user_id\"].shape[0])\n",
    "    #print(sample_history_user_sample.shape[0])\n",
    "    history_sample.append(sample_history_user_sample.shape[0])\n",
    "summary[\"dist_user_history_count\"] = history_dist_user\n",
    "summary[\"dist_user_sample_count\"] = sample_dist_user\n",
    "summary[\"history_sample_count\"] = history_sample\n",
    "summary[\"y1_history_sample_count\"] = is_trade_sample\n",
    "\n",
    "summary[\"y1_history_sample_rate\"] = summary[\"y1_history_sample_count\"] / summary[\"history_sample_count\"]\n",
    "summary[\"dist_user_history_rate\"] = summary[\"dist_user_history_count\"] / summary[\"dist_user_sample_count\"]\n",
    "summary[\"history_sample_rate\"] = summary[\"history_sample_count\"] /summary[\"sample_count\"]\n",
    "\n",
    "summary[\"new_sample_count\"] =  summary[\"sample_count\"] - summary[\"history_sample_count\"]\n",
    "summary[\"y1_new_sample_count\"] =  summary[\"y1_sample_count\"] - summary[\"y1_history_sample_count\"]\n",
    "summary[\"y1_new_sample_rate\"] = summary[\"y1_new_sample_count\"] / summary[\"new_sample_count\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 698,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "79554\n",
      "122537\n",
      "123494\n",
      "115351\n",
      "114697\n",
      "157074\n",
      "195001\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "time_stamp_0700 = int(datetime.datetime.strptime(\"20180907_000000\",\"%Y%m%d_%H%M%S\").timestamp())\n",
    "time_stamp_0708 = int(datetime.datetime.strptime(\"20180907_100000\",\"%Y%m%d_%H%M%S\").timestamp())\n",
    "time_stamp_0712 = int(datetime.datetime.strptime(\"20180907_120000\",\"%Y%m%d_%H%M%S\").timestamp())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_data_7_0008am = train_data.loc[(train_data.context_timestamp >= time_stamp_0700) & (train_data.context_timestamp < time_stamp_0708), :]\n",
    "train_data_7_0812am = train_data.loc[(train_data.context_timestamp >= time_stamp_0708) & (train_data.context_timestamp < time_stamp_0712), :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 682,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "79554\n",
      "122537\n",
      "151973\n",
      "166167\n",
      "188817\n",
      "297221\n",
      "344204\n"
     ]
    }
   ],
   "source": [
    "history_dist_user = list()\n",
    "sample_dist_user = list()\n",
    "history_sample = list()\n",
    "is_trade_sample = list()\n",
    "for i in range(8):\n",
    "    sample_user_id = train_data.loc[train_data.context_timestamp_day == i,\"user_id\"].unique()\n",
    "    history_user_id = train_data.loc[train_data.context_timestamp_day < i,\"user_id\"].unique()\n",
    "    sample_history_user_id = np.intersect1d(history_user_id, sample_user_id)\n",
    "    history_dist_user.append(sample_history_user_id.size)\n",
    "    sample_dist_user.append(sample_user_id.size)\n",
    "    \n",
    "    sample_history_user_sample = pd.DataFrame(sample_history_user_id, columns=[\"user_id\"])\n",
    "    print(sample_history_user_sample.shape[0])\n",
    "    sample_history_user_sample = pd.merge(sample_history_user_sample, train_data.loc[(train_data.context_timestamp_day == i)\n",
    "                                &(train_data.is_trade.isnull()==False),[\"user_id\", \"is_trade\"]], how=\"left\", on=\"user_id\")\n",
    "    is_trade_sample.append(sample_history_user_sample.loc[sample_history_user_sample.is_trade == 1.0,\"user_id\"].shape[0])\n",
    "    #print(sample_history_user_sample.shape[0])\n",
    "    history_sample.append(sample_history_user_sample.shape[0])\n",
    "summary[\"dist_user_history_count\"] = history_dist_user\n",
    "summary[\"dist_user_sample_count\"] = sample_dist_user\n",
    "summary[\"history_sample_count\"] = history_sample\n",
    "summary[\"y1_history_sample_count\"] = is_trade_sample\n",
    "\n",
    "summary[\"y1_history_sample_rate\"] = summary[\"y1_history_sample_count\"] / summary[\"history_sample_count\"]\n",
    "summary[\"dist_user_history_rate\"] = summary[\"dist_user_history_count\"] / summary[\"dist_user_sample_count\"]\n",
    "summary[\"history_sample_rate\"] = summary[\"history_sample_count\"] /summary[\"sample_count\"]\n",
    "\n",
    "summary[\"new_sample_count\"] =  summary[\"sample_count\"] - summary[\"history_sample_count\"]\n",
    "summary[\"y1_new_sample_count\"] =  summary[\"y1_sample_count\"] - summary[\"y1_history_sample_count\"]\n",
    "summary[\"y1_new_sample_rate\"] = summary[\"y1_new_sample_count\"] / summary[\"new_sample_count\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "读取数据 ，将训练数据和测试数据合并"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "87"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#读取train和test的数据\n",
    "train_data = pd.read_csv(\"D:/project/o2o_atom73/atom72_0426/round2_train.txt\",sep=\" \")\n",
    "factor_data = [\"user_gender_id\",\"user_occupation_id\"]\n",
    "numeric_data =[\"item_price_level\",\"item_sales_level\",\"item_collected_level\",\n",
    "               \"item_pv_level\",\"user_age_level\",\"user_star_level\",\"context_page_id\",\"shop_review_num_level\",\n",
    "               \"shop_review_positive_rate\",\"shop_star_level\",\"shop_score_service\",\n",
    "               \"shop_score_delivery\",\"shop_score_description\"]\n",
    "#获取广告类型属性的前两个\n",
    "def get_cp_head_2(x):\n",
    "    cp_str_list = x.split(\";\")\n",
    "    if len(cp_str_list) >=2:\n",
    "        ret = cp_str_list[0] + \";\" + cp_str_list[1]\n",
    "    else:\n",
    "        ret = cp_str_list[0]\n",
    "    return ret\n",
    "train_data[\"context_timestamp_hour\"] = train_data[\"context_timestamp\"].apply(lambda x: datetime.datetime.fromtimestamp(x).hour)\n",
    "train_data[\"context_timestamp_day\"] = train_data[\"context_timestamp\"].apply(lambda x:datetime.datetime.fromtimestamp(x).day)\n",
    "train_data[\"cp_head_2\"] = train_data[\"predict_category_property\"].apply(get_cp_head_2)\n",
    "\n",
    "train_feature_drop = [\"instance_id\",\"is_trade\", \"shop_id\",\"user_id\",\"item_id\",\"context_timestamp_day\",\"context_timestamp\",\"cp_head_2\"]\n",
    "test_feature_drop = [\"instance_id\", \"shop_id\",\"user_id\",\"item_id\",\"context_timestamp_day\",\"context_timestamp\",\"cp_head_2\"]\n",
    "train_data.drop_duplicates(\"instance_id\", inplace=True)\n",
    "#31号替换成0，便于特征提取时区分历史特征区间和当前样本区间\n",
    "train_data.loc[train_data.context_timestamp_day==31,\"context_timestamp_day\"] = 0\n",
    "\n",
    "test_data = pd.read_csv(\"D:/project/o2o_atom73/atom72_0426/round2_test_a.txt\",sep=\" \")\n",
    "test_data[\"context_timestamp_hour\"] = test_data[\"context_timestamp\"].apply(lambda x: datetime.datetime.fromtimestamp(x).hour)\n",
    "test_data[\"context_timestamp_day\"] = test_data[\"context_timestamp\"].apply(lambda x:datetime.datetime.fromtimestamp(x).day)\n",
    "test_data[\"cp_head_2\"] = test_data[\"predict_category_property\"].apply(get_cp_head_2)\n",
    "\n",
    "#concat将train 和test行合并时（test缺少label列）会根据列名进行合并，label那一列在test会用NAN填充，\n",
    "#同时整个数据集的label列的数据类型变成float\n",
    "train_data = pd.concat([train_data, test_data], axis=0)\n",
    "#合并a榜和b榜后，需要重新生成索引，否则会出现重复索引 groupby后返回rank信息时会报错\n",
    "train_data.reset_index(inplace=True,drop=True)\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##############################特征提取############################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#训练集 2~6号上午\n",
    "#验证集 6号下午\n",
    "#最终没有使用这个方案，直接使用了7号上午作为训练集和验证集\n",
    "########################for循环提取特征并写入csv文件 #########################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     None\n",
       "1     None\n",
       "2     None\n",
       "3     None\n",
       "4     None\n",
       "5     None\n",
       "6     None\n",
       "7     None\n",
       "8     None\n",
       "9     None\n",
       "10    None\n",
       "11    None\n",
       "dtype: object"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "####################################################################1.获取采样周期\n",
    "timestamp_list = list()\n",
    "time_str_list = [\"20180902_000000\", \"20180902_120000\", \"20180903_000000\", \"20180903_120000\", \n",
    "             \"20180904_000000\", \"20180904_120000\", \"20180905_000000\", \"20180905_120000\",\n",
    "             \"20180906_000000\", \"20180906_120000\", \"20180907_000000\", \"20180907_120000\",]\n",
    "for timestr in time_str_list:\n",
    "    timestamp_list.append(int(datetime.datetime.strptime(timestr,\"%Y%m%d_%H%M%S\").timestamp())) \n",
    "\n",
    "time_2_day = timestamp_list[4] - timestamp_list[0]\n",
    "time_half_day = timestamp_list[1] - timestamp_list[0]\n",
    "ts_dataset = pd.DataFrame(data=timestamp_list, columns=[\"sample_begin\"])\n",
    "ts_dataset[\"sample_end\"] = ts_dataset[\"sample_begin\"] + time_half_day \n",
    "ts_dataset[\"history_begin\"] = ts_dataset[\"sample_begin\"] - time_2_day\n",
    "ts_dataset[\"half_day_before_sample_begin\"] = ts_dataset[\"sample_begin\"] - time_half_day\n",
    "ts_dataset.reset_index(inplace=True)\n",
    "####################################################################1.根据采样周期划分样本集\n",
    "sample_data_dict = dict()\n",
    "def get_sample_data(x):\n",
    "    sample_data_dict[x[\"index\"]] = train_data.loc[(train_data[\"context_timestamp\"] >= x[\"sample_begin\"]) & (train_data[\"context_timestamp\"] < x[\"sample_end\"]), :]\n",
    "ts_dataset.apply(get_sample_data, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(368138, 30)\n",
      "(921338, 30)\n",
      "(359044, 30)\n",
      "(877784, 30)\n",
      "(329331, 30)\n",
      "(828310, 30)\n",
      "(328562, 30)\n",
      "(871657, 30)\n",
      "(398229, 30)\n",
      "(1536214, 30)\n",
      "(1077175, 30)\n",
      "(519888, 30)\n",
      "(359044, 30)\n",
      "(877784, 30)\n",
      "(329331, 30)\n",
      "(828310, 30)\n",
      "(328562, 30)\n",
      "(871657, 30)\n",
      "(398229, 30)\n",
      "(1536214, 30)\n",
      "(1077175, 30)\n",
      "(519888, 30)\n",
      "(368138, 30)\n",
      "(921338, 30)\n",
      "(359044, 30)\n",
      "(877784, 30)\n",
      "(329331, 30)\n",
      "(828310, 30)\n",
      "(328562, 30)\n",
      "(871657, 30)\n",
      "(398229, 30)\n",
      "(1536214, 30)\n",
      "(1077175, 30)\n",
      "(519888, 30)\n",
      "(368138, 30)\n",
      "(921338, 30)\n",
      "(359044, 30)\n",
      "(877784, 30)\n",
      "(329331, 30)\n",
      "(828310, 30)\n",
      "(328562, 30)\n",
      "(871657, 30)\n",
      "(398229, 30)\n",
      "(1536214, 30)\n",
      "(1077175, 30)\n",
      "(519888, 30)\n",
      "(368138, 30)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\anaconda\\lib\\site-packages\\ipykernel_launcher.py:37: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "D:\\anaconda\\lib\\site-packages\\ipykernel_launcher.py:44: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "D:\\anaconda\\lib\\site-packages\\ipykernel_launcher.py:50: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "D:\\anaconda\\lib\\site-packages\\ipykernel_launcher.py:57: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(921338, 30)\n",
      "(359044, 30)\n",
      "(877784, 30)\n",
      "(329331, 30)\n",
      "(828310, 30)\n",
      "(328562, 30)\n",
      "(871657, 30)\n",
      "(398229, 30)\n",
      "(1536214, 30)\n",
      "(1077175, 30)\n",
      "(519888, 30)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "################################################### 1 基础特征\n",
    "base_feature = get_base_feature(train_data, factor_data, numeric_data)\n",
    "sample_base_feature = pd.concat([train_data[train_feature_drop], base_feature], axis=1, join=\"inner\")\n",
    "sample_base_feature = sample_base_feature.loc[sample_base_feature[\"context_timestamp_day\"] >=2,:]\n",
    "sample_base_feature.to_csv(\"./sample_base_feature.csv\", index=False)\n",
    "gc.collect()\n",
    "###################################################2.样本特征：广告类别属性\n",
    "item_cp_feature = get_item_cp_feature(train_data.loc[train_data[\"context_timestamp_day\"] >=2,:])\n",
    "item_cp_feature.to_csv(\"./item_cp_feature.csv\", index=False)\n",
    "gc.collect()\n",
    "#减小内存\n",
    "train_data.drop(\"predict_category_property\", axis=1, inplace=True)\n",
    "gc.collect()\n",
    "\n",
    "###################################################3.历史特征：用户\n",
    "user_feature_dict = dict()\n",
    "for i in sample_data_dict:\n",
    "    print(sample_data_dict[i].shape)\n",
    "    time_begin = ts_dataset.loc[i, \"history_begin\"]\n",
    "    time_end = ts_dataset.loc[i, \"sample_begin\"]\n",
    "    user_id = sample_data_dict[i][[\"user_id\"]].drop_duplicates()\n",
    "    user_history_data = pd.merge(user_id, train_data.loc[(train_data[\"context_timestamp\"] >= time_begin) \n",
    "    & (train_data[\"context_timestamp\"] < time_end),:],how=\"inner\", on=\"user_id\")\n",
    "    user_feature_dict[i] = get_user_feature(sample_data_dict[i],user_history_data)\n",
    "    del user_history_data\n",
    "    del user_id\n",
    "    gc.collect()\n",
    "user_feature = pd.concat([user_feature_dict[0], user_feature_dict[1],user_feature_dict[2],user_feature_dict[3],\n",
    "                          user_feature_dict[4],user_feature_dict[5],user_feature_dict[6], user_feature_dict[7],\n",
    "                          user_feature_dict[8],user_feature_dict[9], user_feature_dict[10],user_feature_dict[11]])\n",
    "user_feature.to_csv(\"./user_feature.csv\", index=False)\n",
    "del user_feature_dict\n",
    "gc.collect()\n",
    "\n",
    "####################################################历史特征：广告\n",
    "item_feature_dict = dict()\n",
    "for i in sample_data_dict:\n",
    "    print(sample_data_dict[i].shape)\n",
    "    time_begin = ts_dataset.loc[i, \"history_begin\"]\n",
    "    time_end = ts_dataset.loc[i, \"sample_begin\"]\n",
    "    item_id = sample_data_dict[i][[\"item_id\"]].drop_duplicates()\n",
    "    item_history_data = pd.merge(item_id, train_data.loc[(train_data[\"context_timestamp\"] >= time_begin) \n",
    "    & (train_data[\"context_timestamp\"] < time_end),:],how=\"inner\", on=\"item_id\")\n",
    "    item_feature_dict[i] = get_item_feature(sample_data_dict[i],item_history_data)\n",
    "    del item_history_data\n",
    "    del item_id\n",
    "    gc.collect()\n",
    "item_feature = pd.concat([item_feature_dict[0], item_feature_dict[1],item_feature_dict[2],item_feature_dict[3],\n",
    "                          item_feature_dict[4],item_feature_dict[5],item_feature_dict[6], item_feature_dict[7],\n",
    "                          item_feature_dict[8],item_feature_dict[9], item_feature_dict[10],item_feature_dict[11]])\n",
    "item_feature.to_csv(\"./item_feature.csv\", index=False)\n",
    "del item_feature_dict\n",
    "gc.collect()\n",
    "\n",
    "####################################################历史特征：商店\n",
    "shop_feature_dict = dict()\n",
    "for i in sample_data_dict:\n",
    "    print(sample_data_dict[i].shape)\n",
    "    time_begin = ts_dataset.loc[i, \"history_begin\"]\n",
    "    time_end = ts_dataset.loc[i, \"sample_begin\"]\n",
    "    shop_id = sample_data_dict[i][[\"shop_id\"]].drop_duplicates()\n",
    "    shop_history_data = pd.merge(shop_id, train_data.loc[(train_data[\"context_timestamp\"] >= time_begin) \n",
    "    & (train_data[\"context_timestamp\"] < time_end),:],how=\"inner\", on=\"shop_id\")\n",
    "    shop_feature_dict[i] = get_shop_feature(sample_data_dict[i],shop_history_data)\n",
    "    del shop_history_data\n",
    "    del shop_id\n",
    "    gc.collect()\n",
    "shop_feature = pd.concat([shop_feature_dict[0],shop_feature_dict[1],shop_feature_dict[2],shop_feature_dict[3],\n",
    "                         shop_feature_dict[4],shop_feature_dict[5],shop_feature_dict[6],shop_feature_dict[7],\n",
    "                         shop_feature_dict[8],shop_feature_dict[9],shop_feature_dict[10],shop_feature_dict[11]])\n",
    "shop_feature.to_csv(\"./shop_feature.csv\", index=False)\n",
    "del shop_feature_dict\n",
    "gc.collect()\n",
    "\n",
    "####################################################广告商店\n",
    "shopitem_feature_dict = dict()\n",
    "for i in sample_data_dict:\n",
    "    print(sample_data_dict[i].shape)\n",
    "    time_begin = ts_dataset.loc[i, \"history_begin\"]\n",
    "    time_end = ts_dataset.loc[i, \"sample_begin\"]\n",
    "    shopitem_id = sample_data_dict[i][[\"shop_id\", \"item_id\"]].drop_duplicates()\n",
    "    shopitem_history_data = pd.merge(shopitem_id, train_data.loc[(train_data[\"context_timestamp\"] >= time_begin) \n",
    "    & (train_data[\"context_timestamp\"] < time_end),:],how=\"inner\", on=[\"shop_id\", \"item_id\"])\n",
    "    shopitem_feature_dict[i] = get_shopitem_feature(sample_data_dict[i],shopitem_history_data)\n",
    "    del shopitem_history_data\n",
    "    del shopitem_id\n",
    "    gc.collect()\n",
    "shopitem_feature = pd.concat([shopitem_feature_dict[0], shopitem_feature_dict[1],shopitem_feature_dict[2],\n",
    "                              shopitem_feature_dict[3], shopitem_feature_dict[4],shopitem_feature_dict[5],\n",
    "                              shopitem_feature_dict[6], shopitem_feature_dict[7],shopitem_feature_dict[8],\n",
    "                              shopitem_feature_dict[9], shopitem_feature_dict[10],shopitem_feature_dict[11]])\n",
    "shopitem_feature.to_csv(\"./shopitem_feature.csv\", index=False)\n",
    "del shopitem_feature_dict\n",
    "gc.collect()\n",
    "\n",
    "####################################################样本行为特征：用户\n",
    "user_act_feature_dict = dict()\n",
    "for i in sample_data_dict:\n",
    "    print(sample_data_dict[i].shape)\n",
    "    user_act_feature_dict[i] = get_user_sample_action_feature(sample_data_dict[i])\n",
    "    gc.collect()\n",
    "user_sample_action_feature = pd.concat([user_act_feature_dict[0], user_act_feature_dict[1],user_act_feature_dict[2],\n",
    "                                        user_act_feature_dict[3], user_act_feature_dict[4],user_act_feature_dict[5],\n",
    "                                        user_act_feature_dict[6], user_act_feature_dict[7],user_act_feature_dict[8],\n",
    "                                        user_act_feature_dict[9], user_act_feature_dict[10],user_act_feature_dict[11]])\n",
    "user_sample_action_feature.to_csv(\"./user_sample_action_feature.csv\", index=False)\n",
    "del user_act_feature_dict\n",
    "gc.collect()\n",
    "\n",
    "####################################################样本行为特征V2：用户\n",
    "user_action_v2_feature_dict = dict()\n",
    "for i in sample_data_dict:\n",
    "    print(sample_data_dict[i].shape)\n",
    "    time_begin = ts_dataset.loc[i, \"half_day_before_sample_begin\"]\n",
    "    time_end = ts_dataset.loc[i, \"sample_begin\"]\n",
    "    #user_id = sample_data_dict[i][[\"user_id\"]].drop_duplicates()\n",
    "    user_history_data = train_data.loc[(train_data[\"context_timestamp\"] >= time_begin) \n",
    "    & (train_data[\"context_timestamp\"] < time_end),:]\n",
    "    user_action_v2_feature_dict[i] = get_user_action_v2_feature(sample_data_dict[i], user_history_data)\n",
    "    del user_history_data\n",
    "    #del user_id\n",
    "    gc.collect()\n",
    "user_action_v2_feature = pd.concat([user_action_v2_feature_dict[0], user_action_v2_feature_dict[1],user_action_v2_feature_dict[2],\n",
    "                                    user_action_v2_feature_dict[3], user_action_v2_feature_dict[4],user_action_v2_feature_dict[5],\n",
    "                                    user_action_v2_feature_dict[6], user_action_v2_feature_dict[7],user_action_v2_feature_dict[8],\n",
    "                                                                                    user_action_v2_feature_dict[9], user_action_v2_feature_dict[10],user_action_v2_feature_dict[11]])\n",
    "user_action_v2_feature.to_csv(\"./user_action_v2_feature.csv\", index=False)\n",
    "del user_action_v2_feature_dict\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#训练集 7号0点至8点\n",
    "#验证集 7号8点~12点"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "time_stamp_0612 = int(datetime.datetime.strptime(\"20180906_120000\",\"%Y%m%d_%H%M%S\").timestamp())\n",
    "time_stamp_0700 = int(datetime.datetime.strptime(\"20180907_000000\",\"%Y%m%d_%H%M%S\").timestamp())\n",
    "train_data_07 = train_data.loc[train_data[\"context_timestamp_day\"]==7, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "base ok\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\anaconda\\lib\\site-packages\\ipykernel_launcher.py:62: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "D:\\anaconda\\lib\\site-packages\\ipykernel_launcher.py:63: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "D:\\anaconda\\lib\\site-packages\\ipykernel_launcher.py:64: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "D:\\anaconda\\lib\\site-packages\\ipykernel_launcher.py:65: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "D:\\anaconda\\lib\\site-packages\\ipykernel_launcher.py:66: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "D:\\anaconda\\lib\\site-packages\\ipykernel_launcher.py:67: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "D:\\anaconda\\lib\\site-packages\\ipykernel_launcher.py:77: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "D:\\anaconda\\lib\\site-packages\\ipykernel_launcher.py:79: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "D:\\anaconda\\lib\\site-packages\\ipykernel_launcher.py:81: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "D:\\anaconda\\lib\\site-packages\\ipykernel_launcher.py:82: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "D:\\anaconda\\lib\\site-packages\\ipykernel_launcher.py:83: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "D:\\anaconda\\lib\\site-packages\\ipykernel_launcher.py:85: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "D:\\anaconda\\lib\\site-packages\\ipykernel_launcher.py:93: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "D:\\anaconda\\lib\\site-packages\\ipykernel_launcher.py:94: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "D:\\anaconda\\lib\\site-packages\\ipykernel_launcher.py:95: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "D:\\anaconda\\lib\\site-packages\\ipykernel_launcher.py:96: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "D:\\anaconda\\lib\\site-packages\\ipykernel_launcher.py:97: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "D:\\anaconda\\lib\\site-packages\\ipykernel_launcher.py:99: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "D:\\anaconda\\lib\\site-packages\\ipykernel_launcher.py:100: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "D:\\anaconda\\lib\\site-packages\\ipykernel_launcher.py:101: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "D:\\anaconda\\lib\\site-packages\\ipykernel_launcher.py:14: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cp ok\n",
      "user ok\n",
      "item ok\n",
      "shop ok\n",
      "shopitem ok\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\anaconda\\lib\\site-packages\\ipykernel_launcher.py:37: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "D:\\anaconda\\lib\\site-packages\\ipykernel_launcher.py:44: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "D:\\anaconda\\lib\\site-packages\\ipykernel_launcher.py:50: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "D:\\anaconda\\lib\\site-packages\\ipykernel_launcher.py:57: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "user action ok\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\anaconda\\lib\\site-packages\\ipykernel_launcher.py:15: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  from ipykernel import kernelapp as app\n",
      "D:\\anaconda\\lib\\site-packages\\ipykernel_launcher.py:16: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  app.launch_new_instance()\n",
      "D:\\anaconda\\lib\\site-packages\\ipykernel_launcher.py:29: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "D:\\anaconda\\lib\\site-packages\\ipykernel_launcher.py:30: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "D:\\anaconda\\lib\\site-packages\\ipykernel_launcher.py:31: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "D:\\anaconda\\lib\\site-packages\\ipykernel_launcher.py:44: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "D:\\anaconda\\lib\\site-packages\\ipykernel_launcher.py:45: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "D:\\anaconda\\lib\\site-packages\\ipykernel_launcher.py:46: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "user action v2 ok\n"
     ]
    }
   ],
   "source": [
    "################################################### 1 基础特征\n",
    "#将基础特征和id信息合并，这里需要确保样本顺序一致，或者直接使用instance_id进行merge\n",
    "base_feature = get_base_feature(train_data_07, factor_data, numeric_data)\n",
    "sample_base_feature = pd.concat([train_data_07[train_feature_drop], base_feature], axis=1, join=\"inner\")\n",
    "sample_base_feature.to_csv(\"./sample_base_feature.csv\", index=False)\n",
    "gc.collect()\n",
    "print(\"base ok\")\n",
    "\n",
    "###################################################2.样本特征：广告类别属性\n",
    "item_cp_feature = get_item_cp_feature(train_data_07)\n",
    "item_cp_feature.to_csv(\"./item_cp_feature.csv\", index=False)\n",
    "gc.collect()\n",
    "#predict_category_property在后续提取过程不在使用，删除用于减小内存\n",
    "train_data_07.drop(\"predict_category_property\", axis=1, inplace=True)\n",
    "gc.collect()\n",
    "print(\"cp ok\")\n",
    "\n",
    "###################################################3.历史特征：用户\n",
    "#历史特征区间为样本前所有天\n",
    "#优化：根据用户分组，则历史数据仅仅保留样本中存在的用户样本，\n",
    "#原理是获取历史数据和样本数据中user的交集，使用inner进行merge\n",
    "user_id = train_data_07[[\"user_id\"]].drop_duplicates()\n",
    "user_history_data = pd.merge(user_id, train_data.loc[train_data[\"context_timestamp_day\"] <7,:],how=\"inner\", on=\"user_id\")\n",
    "user_feature = get_user_feature(train_data_07, user_history_data)\n",
    "user_feature.to_csv(\"./user_feature.csv\", index=False)\n",
    "gc.collect()\n",
    "print(\"user ok\")\n",
    "####################################################历史特征：广告\n",
    "\n",
    "item_id = train_data_07[[\"item_id\"]].drop_duplicates()\n",
    "item_history_data = pd.merge(item_id, train_data.loc[train_data[\"context_timestamp_day\"] <7,:] ,how=\"inner\", on=\"item_id\")\n",
    "item_feature = get_item_feature(train_data_07, item_history_data)\n",
    "item_feature.to_csv(\"./item_feature.csv\", index=False)\n",
    "gc.collect()\n",
    "print(\"item ok\")\n",
    "\n",
    "####################################################历史特征：商店\n",
    "shop_id = train_data_07[[\"shop_id\"]].drop_duplicates()\n",
    "shop_history_data = pd.merge(shop_id, train_data.loc[train_data[\"context_timestamp_day\"] <7,:], how=\"inner\", on=\"shop_id\")\n",
    "shop_feature = get_shop_feature(train_data_07, shop_history_data)\n",
    "shop_feature.to_csv(\"./shop_feature.csv\", index=False)\n",
    "print(\"shop ok\")\n",
    "\n",
    "####################################################历史特征:广告品牌\n",
    "item_brand_id = train_data_07[[\"user_id\", \"item_brand_id\"]].drop_duplicates()\n",
    "item_brand_history_data = pd.merge(item_brand_id, train_data.loc[train_data[\"context_timestamp_day\"] <7,:], how=\"inner\", on=[\"user_id\", \"item_brand_id\"])\n",
    "item_brand_feature = get_item_brand_feature(train_data_07, item_brand_history_data)\n",
    "item_brand_feature.to_csv(\"./item_brand_feature.csv\", index=False)\n",
    "gc.collect()\n",
    "print(\"item_brand ok\")\n",
    "\n",
    "####################################################历史特征广告商店\n",
    "shopitem_id = train_data_07[[\"shop_id\", \"item_id\"]].drop_duplicates()\n",
    "shopitem_history_data = pd.merge(shopitem_id, train_data.loc[train_data[\"context_timestamp_day\"] <7,:], how=\"inner\", on=[\"shop_id\", \"item_id\"])\n",
    "shopitem_feature = get_shopitem_feature(train_data_07, shopitem_history_data)\n",
    "shopitem_feature.to_csv(\"./shopitem_feature.csv\", index=False)\n",
    "gc.collect()\n",
    "print(\"shopitem ok\")\n",
    "\n",
    "####################################################历史特征用户广告\n",
    "useritem_id = train_data_07[[\"user_id\", \"item_id\"]].drop_duplicates()\n",
    "useritem_history_data = pd.merge(useritem_id, train_data.loc[train_data[\"context_timestamp_day\"] <7,:], how=\"inner\", on=[\"user_id\", \"item_id\"])\n",
    "useritem_feature = get_useritem_feature(train_data_07, useritem_history_data)\n",
    "useritem_feature.to_csv(\"./useritem_feature.csv\", index=False)\n",
    "gc.collect()\n",
    "print(\"useritem ok\")\n",
    "\n",
    "####################################################历史特征用户商店\n",
    "usershop_id = train_data_07[[\"user_id\", \"shop_id\"]].drop_duplicates()\n",
    "usershop_history_data = pd.merge(usershop_id, train_data.loc[train_data[\"context_timestamp_day\"] <7,:], how=\"inner\", on=[\"user_id\", \"shop_id\"])\n",
    "usershop_feature = get_usershop_feature(train_data_07, usershop_history_data)\n",
    "usershop_feature.to_csv(\"./usershop_feature.csv\", index=False)\n",
    "gc.collect()\n",
    "print(\"usershop ok\")\n",
    "\n",
    "####################################################历史特征用户广告品牌\n",
    "useritem_brand_id = train_data_07[[\"user_id\", \"item_brand_id\"]].drop_duplicates()\n",
    "useritem_brand_history_data = pd.merge(useritem_brand_id, train_data.loc[train_data[\"context_timestamp_day\"] <7,:], how=\"inner\", on=[\"user_id\", \"item_brand_id\"])\n",
    "useritem_brand_feature = get_useritem_brand_feature(train_data_07, useritem_brand_history_data)\n",
    "useritem_brand_feature.to_csv(\"./useritem_brand_feature.csv\", index=False)\n",
    "gc.collect()\n",
    "print(\"useritem_brand ok\")\n",
    "\n",
    "####################################################历史样本差分特征：广告属性差异\n",
    "item_id = train_data_07[[\"item_id\"]].drop_duplicates()\n",
    "item_history_data = pd.merge(item_id, train_data.loc[train_data[\"context_timestamp_day\"] <7,:] ,how=\"inner\", on=\"item_id\")\n",
    "item_diff_feature = get_item_diff_feture(train_data_07, item_history_data)\n",
    "item_diff_feature.to_csv(\"./item_diff_feature.csv\", index=False)\n",
    "gc.collect()\n",
    "print(\"item_diff_feature ok\")\n",
    "####################################################样本行为特征：用户\n",
    "user_sample_action_feature = get_user_sample_action_feature(train_data_07)\n",
    "user_sample_action_feature.to_csv(\"./user_sample_action_feature.csv\", index=False)\n",
    "gc.collect()\n",
    "print(\"user action ok\")\n",
    "\n",
    "####################################################样本行为特征V2：用户 上一次，下一次的点击时间间隔\n",
    "#用户行为特征包括了样本前半天的用户数据，主要用于提取时间信息\n",
    "#不仅仅是对用户分组，还有对用户广告，用户商店分组，因此没有在函数外对历史用户进行merge\n",
    "user_history_data = train_data.loc[(train_data[\"context_timestamp\"] >= time_stamp_0612) \n",
    "& (train_data[\"context_timestamp\"] < time_stamp_0700),:]\n",
    "user_action_v2_feature = get_user_action_v2_feature(train_data_07, user_history_data)\n",
    "user_action_v2_feature.to_csv(\"./user_action_v2_feature.csv\", index=False)\n",
    "gc.collect()\n",
    "print(\"user action v2 ok\")\n",
    "####################################################样本行为特征V3：用户在上/下一个小时内的点击数\n",
    "#用户行为特征包括了样本前一小时的用户数据，主要用于提取时间信息\n",
    "#不仅仅是对用户分组，还有对用户广告，用户商店分组，因此没有在函数外对历史用户进行merge\n",
    "user_history_data = train_data.loc[(train_data[\"context_timestamp\"] >= (time_stamp_0700-3600)) \n",
    "& (train_data[\"context_timestamp\"] < time_stamp_0700),:]\n",
    "user_action_v3_feature = get_user_action_v3_feature(train_data_07, user_history_data)\n",
    "user_action_v3_feature.to_csv(\"./user_action_v3_feature.csv\", index=False)\n",
    "gc.collect()\n",
    "print(\"user action v3 ok\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#############################特征提取接口###############################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#######################################################样本基础特征\n",
    "def get_base_feature(input_data, factor_data, numeric_data):   \n",
    "    base_data = input_data.copy()\n",
    "    factor_feature_data = pd.get_dummies(base_data[factor_data], columns=factor_data)\n",
    "    numeric_feature_data = base_data[numeric_data]\n",
    "    base_feature = pd.concat([factor_feature_data, numeric_feature_data], axis=1)\n",
    "    return base_feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#######################################################样本广告类别属性和预测类别属性特征\n",
    "def get_pred_category_list(x):\n",
    "    category_list = list()\n",
    "    for categeory in x:\n",
    "        category_list.append(categeory.split(\":\")[0])\n",
    "    return category_list\n",
    "#获取真实类目在预测类目列表中的索引排序\n",
    "def get_pred_category_index(x):\n",
    "    #print(x[\"category\"])\n",
    "    #print(x[\"pred_cageg\"])\n",
    "    if x[\"item_category\"] in x[\"pred_category_list\"]:\n",
    "        #索引/数量 索引不为0 因此将索引+1 同时融入了没有找到该索引的信息\n",
    "        return x[\"pred_category_list\"].index(x[\"item_category\"]) + 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "def get_true_category_property_in_pred_list(x):\n",
    "    if x[\"category_index_in_pred_category_list\"] == 0:\n",
    "        return \"NULL\" #返回\"NULL\"\n",
    "    else:\n",
    "        return x[\"pred_category_property_list\"][x[\"category_index_in_pred_category_list\"] - 1]\n",
    "\n",
    "def get_all_property_list(x):\n",
    "    #print(x)\n",
    "    property_list = list()\n",
    "    for property_str in x:\n",
    "        #print(string.split(\":\")[0])\n",
    "        property_list.append(property_str.split(\":\")[-1])\n",
    "    #print(ret_list)\n",
    "    return property_list\n",
    "\n",
    "def get_pred_category_which_has_pro_list(x):\n",
    "    #print(x)\n",
    "    category_list = list()\n",
    "    for category in x:\n",
    "        #print(string.split(\":\")[0])\n",
    "        category_pro = category.split(\":\")\n",
    "        if category_pro[-1] != \"-1\":\n",
    "            category_list.append(category_pro[0])\n",
    "    return category_list\n",
    "\n",
    "def get_pred_category_which_has_pro_index(x):\n",
    "    #print(x[\"category\"])\n",
    "    #print(x[\"pred_cageg\"])\n",
    "    if x[\"item_category\"] in x[\"pred_category_which_has_pro_list\"]:\n",
    "        #索引/数量 索引不为0 因此将索引+1 同时融入了没有找到该索引的信息\n",
    "        return x[\"pred_category_which_has_pro_list\"].index(x[\"item_category\"]) + 1\n",
    "    else:\n",
    "        return 0\n",
    "#category_index_in_pred_category_list: 当前样本真实类目在预测得到的类目列表中的排序索引\n",
    "##category_index_div_pred_num: pred_category_num/category_index_in_pred_category_list\n",
    "##pred_pro_intersection_item_pro_num：当前样本的上下文预测得到的广告类目与真实广告类目相符的那条信息的属性个数和真实属性的交集\n",
    "##pred_pro_intersection_item_pro_num_div_pred_property_num  pred_pro_intersection_item_pro_num/pred_property_num\n",
    "##pred_pro_intersection_item_pro_num_div_item_property_num  pred_pro_intersection_item_pro_num/item_property_num\n",
    "##pred_category_which_property_is_-1_num:当前样本的所有预测属性中-1的个数\n",
    "##pred_category_which_property_is_-1_num_div_pred_category_num：当前样本中预测的属性中-1的个数与所有属性个数之比\n",
    "#pred_category_which_has_pro_num：当前样本中预测的广告类型中拥有属性的个数\n",
    "#pred_category_which_has_pro_num_div_pred_category_num:当前样本中预测的广告类型中拥有属性的个数与总类型数之比\n",
    "def get_item_cp_feature(input_data):\n",
    "    category_data = input_data\n",
    "    category_feature = category_data[[\"instance_id\",\"item_category_list\", \"predict_category_property\",\"item_property_list\"]]\n",
    "    #item_category:当前样本所属的真实广告类目\n",
    "    #pred_category_property_list:当前样本上下文预测得到的类目属性列表\n",
    "    #pred_category_list:当前样本上下文预测得到的类目列表\n",
    "    #pred_category_num: 当前样本上下文预测得到的类目个数\n",
    "    category_feature[\"item_category\"] = category_feature[\"item_category_list\"].apply(lambda x: x.split(\";\")[-1])\n",
    "    category_feature[\"pred_category_property_list\"] = category_feature[\"predict_category_property\"].apply(lambda x: x.split(\";\"))\n",
    "    category_feature[\"pred_category_list\"] = category_feature[\"pred_category_property_list\"].apply(get_pred_category_list)\n",
    "    category_feature[\"category_index_in_pred_category_list\"] = category_feature.apply(get_pred_category_index, axis=1)\n",
    "    category_feature[\"pred_category_num\"] = category_feature.apply(lambda x: len(x[\"pred_category_list\"]), axis=1)\n",
    "    category_feature[\"category_index_div_pred_num\"] = category_feature[\"category_index_in_pred_category_list\"] / category_feature[\"pred_category_num\"]\n",
    "    gc.collect()\n",
    "\n",
    "    #item_property_list: 当前样本的广告在某个类目下的真实属性列表\n",
    "    #item_property_num: 当前样本的广告在某个类目下的真实属性个数\n",
    "    #true_category_property_in_pred_list 当前样本的上下文预测得到的广告类目和真实广告类目相符的那条信息\n",
    "    #pred_property_list：当前样本的上下文预测得到的广告类目与真实广告类目相符的那条信息的属性列表\n",
    "    #pred_property_num：当前样本的上下文预测得到的广告类目与真实广告类目相符的那条信息的属性个数\n",
    "    \n",
    "    category_feature[\"item_property_list\"] = category_feature.item_property_list.apply(lambda x:x.split(\";\"))\n",
    "    #真实广告属于同一个类别，但会拥有不同的属性\n",
    "    category_feature[\"item_property_num\"] = category_feature.item_property_list.apply(lambda x:len(x))    \n",
    "    #上下文预测中和广告类别预测一致的信息\n",
    "    category_feature[\"true_category_property_in_pred_list\"] = category_feature.apply(get_true_category_property_in_pred_list, axis=1)\n",
    "    category_feature[\"pred_property_list\"] = category_feature[\"true_category_property_in_pred_list\"].apply(lambda x:x.split(\":\")[-1].split(\",\"))\n",
    "    category_feature[\"pred_property_num\"] = category_feature[\"pred_property_list\"].apply(lambda x:len(x))\n",
    "    #set的结果是元组\n",
    "    category_feature[\"pred_pro_intersection_item_pro_num\"] = category_feature.apply(lambda x: len(set(x[\"pred_property_list\"]).intersection(set(x[\"item_property_list\"]))), axis=1)\n",
    "    gc.collect()\n",
    "\n",
    "    #pred_pro_intersection_item_pro_num_div_pred_property_num  pred_pro_intersection_item_pro_num/pred_property_num\n",
    "    #pred_pro_intersection_item_pro_num_div_item_property_num  pred_pro_intersection_item_pro_num/item_property_num\n",
    "    #pred_all_property_list \n",
    "    #pred_category_which_property_is_\n",
    "    #pred_category_which_property_is_\n",
    "    category_feature[\"pred_pro_intersection_item_pro_num_div_pred_property_num\"] = category_feature.apply(lambda x:x[\"pred_pro_intersection_item_pro_num\"]/x[\"pred_property_num\"],axis=1)\n",
    "    category_feature[\"pred_pro_intersection_item_pro_num_div_item_property_num\"] = category_feature.apply(lambda x:x[\"pred_pro_intersection_item_pro_num\"]/x[\"item_property_num\"],axis=1)\n",
    "    category_feature[\"pred_all_property_list\"] = category_feature.pred_category_property_list.apply(get_all_property_list)\n",
    "    category_feature[\"pred_category_which_property_is_-1_num\"] = category_feature[\"pred_all_property_list\"].apply(lambda x:x.count('-1'))\n",
    "    category_feature[\"pred_category_which_property_is_-1_num_div_pred_category_num\"] = category_feature.apply(lambda x:x[\"pred_category_which_property_is_-1_num\"]/x[\"pred_category_num\"], axis=1)\n",
    "    gc.collect()\n",
    "    category_feature[\"pred_category_which_has_pro_list\"] =  category_feature.pred_category_property_list.apply(get_pred_category_which_has_pro_list)\n",
    "    category_feature[\"pred_category_which_has_pro_num\"] = category_feature.apply(get_pred_category_which_has_pro_index, axis=1)\n",
    "    category_feature[\"pred_category_which_has_pro_num_div_pred_category_num\"] = category_feature.apply(lambda x: x[\"pred_category_which_has_pro_num\"]/x[\"pred_category_num\"], axis=1)\n",
    "    gc.collect()\n",
    "    train_category_feature = category_feature[[\"instanpred_category_which_property_is_ce_id\",\"category_index_in_pred_category_list\",\"category_index_div_pred_num\",\"pred_pro_intersection_item_pro_num\",\n",
    "        \"pred_pro_intersection_item_pro_num_div_pred_property_num\",\"pred_pro_intersection_item_pro_num_div_item_property_num\",\n",
    "                    \"pred_category_which_property_is_-1_num\",\"pred_category_which_property_is_-1_num_div_pred_category_num\",\n",
    "                    \"pred_category_which_has_pro_num\",\"pred_category_which_has_pro_num_div_pred_category_num\"]]\n",
    "    return train_category_feature\n",
    "################################广告类目属性特征######################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "######################################################历史转化率：商店评分变化\n",
    "def get_shop_diff(input_data, history_day):\n",
    "    \n",
    "    shop_data = input_data.copy()\n",
    "    date_index = shop_data.context_timestamp_day.unique()\n",
    "    date_index.sort()\n",
    "    shop_data_current_day = shop_data.loc[shop_data.context_timestamp_day == date_index[-1],:]\n",
    "    shop_data_history_day = shop_data.loc[(shop_data.context_timestamp_day < date_index[-1]) & (shop_data.context_timestamp_day >= date_index[-1-history_day]),:]\n",
    "    #shop_data = shop_data_history_day\n",
    "    \n",
    "    shop_data = shop_data.loc[(shop_data.context_timestamp_day <= date_index[-1]) & (shop_data.context_timestamp_day >= date_index[-1-history_day]),:]\n",
    "    shop_data = shop_data.drop_duplicates([\"shop_id\", \"context_timestamp_day\"])\n",
    "    shop_id = shop_data[[\"shop_id\"]].drop_duplicates()\n",
    "    date_index = shop_data.context_timestamp_day.unique()\n",
    "    date_index.sort()\n",
    "\n",
    "    t1 = shop_data[[\"shop_id\", \"context_timestamp_day\", \"shop_review_num_level\"]]\n",
    "    t1 = t1.set_index([\"shop_id\", \"context_timestamp_day\"])\n",
    "    t1 = t1.unstack()\n",
    "    t1 = t1.reset_index()\n",
    "    t1[\"shop_shop_review_num_level_diff\"] = t1.shop_review_num_level[date_index[2]] *7000 - t1.shop_review_num_level[date_index[1]]*4000 -t1.shop_review_num_level[date_index[0]]*3000\n",
    "    t1[\"shop_shop_review_num_level_diff\"] = t1[\"shop_shop_review_num_level_diff\"]/t1.shop_review_num_level[date_index[0]]\n",
    "    t1 = t1[[\"shop_shop_review_num_level_diff\", \"shop_id\"]]\n",
    "    t1.columns = t1.columns.droplevel(1)\n",
    "\n",
    "    t2 = shop_data[[\"shop_id\", \"context_timestamp_day\", \"shop_review_positive_rate\"]]\n",
    "    t2 = t2.set_index([\"shop_id\", \"context_timestamp_day\"])\n",
    "    t2 = t2.unstack()\n",
    "    t2 = t2.reset_index()\n",
    "    t2[\"shop_shop_review_positive_rate_diff\"] = t2.shop_review_positive_rate[date_index[2]] *7000 - t2.shop_review_positive_rate[date_index[1]]*4000 -t2.shop_review_positive_rate[date_index[0]]*3000\n",
    "    t2[\"shop_shop_review_positive_rate_diff\"] = t2[\"shop_shop_review_positive_rate_diff\"]/t2.shop_review_positive_rate[date_index[0]]\n",
    "    t2 = t2[[\"shop_shop_review_positive_rate_diff\", \"shop_id\"]]\n",
    "    t2.columns = t2.columns.droplevel(1)\n",
    "\n",
    "    t3 = shop_data[[\"shop_id\", \"context_timestamp_day\", \"shop_star_level\"]]\n",
    "    t3 = t3.set_index([\"shop_id\", \"context_timestamp_day\"])\n",
    "    t3 = t3.unstack()\n",
    "    t3 = t3.reset_index()\n",
    "    t3[\"shop_shop_star_level_diff\"] = t3.shop_star_level[date_index[2]] *7000 - t3.shop_star_level[date_index[1]]*4000 -t3.shop_star_level[date_index[0]]*3000\n",
    "    t3[\"shop_shop_star_level_diff\"] = t3[\"shop_shop_star_level_diff\"]/t3.shop_star_level[date_index[0]]\n",
    "    t3 = t3[[\"shop_shop_star_level_diff\", \"shop_id\"]]\n",
    "    t3.columns = t3.columns.droplevel(1)\n",
    "\n",
    "    t4 = shop_data[[\"shop_id\", \"context_timestamp_day\", \"shop_score_service\"]]\n",
    "    t4 = t4.set_index([\"shop_id\", \"context_timestamp_day\"])\n",
    "    t4 = t4.unstack()\n",
    "    t4 = t4.reset_index()\n",
    "    t4[\"shop_shop_score_service_diff\"] = t4.shop_score_service[date_index[2]] *7000 - t4.shop_score_service[date_index[1]]*4000 -t4.shop_score_service[date_index[0]]*3000\n",
    "    t4[\"shop_shop_score_service_diff\"] = t4[\"shop_shop_score_service_diff\"]/t4.shop_score_service[date_index[0]]\n",
    "    t4 = t4[[\"shop_shop_score_service_diff\", \"shop_id\"]]\n",
    "    t4.columns = t4.columns.droplevel(1)\n",
    "\n",
    "    t5 = shop_data[[\"shop_id\", \"context_timestamp_day\", \"shop_score_delivery\"]]\n",
    "    t5 = t5.set_index([\"shop_id\", \"context_timestamp_day\"])\n",
    "    t5 = t5.unstack()\n",
    "    t5 = t5.reset_index()\n",
    "    t5[\"shop_shop_score_delivery_diff\"] = t5.shop_score_delivery[date_index[2]] *7000 - t5.shop_score_delivery[date_index[1]]*4000 -t5.shop_score_delivery[date_index[0]]*3000\n",
    "    t5[\"shop_shop_score_delivery_diff\"] = t5[\"shop_shop_score_delivery_diff\"]/t5.shop_score_delivery[date_index[0]]\n",
    "    t5 = t5[[\"shop_shop_score_delivery_diff\", \"shop_id\"]]\n",
    "    t5.columns = t5.columns.droplevel(1)\n",
    "\n",
    "\n",
    "    t6 = shop_data[[\"shop_id\", \"context_timestamp_day\", \"shop_score_description\"]]\n",
    "    t6 = t6.set_index([\"shop_id\", \"context_timestamp_day\"])\n",
    "    t6 = t6.unstack()\n",
    "    t6 = t6.reset_index()\n",
    "\n",
    "    t6[\"shop_shop_score_description_diff\"] = t6.shop_score_description[date_index[2]] *7000 - t6.shop_score_description[date_index[1]]*4000 -t6.shop_score_description[date_index[0]]*3000\n",
    "    t6[\"shop_shop_score_description_diff\"] = t6[\"shop_shop_score_description_diff\"]/t6.shop_score_description[date_index[0]]\n",
    "    t6 = t6[[\"shop_shop_score_description_diff\", \"shop_id\"]]\n",
    "    t6.columns = t6.columns.droplevel(1)\n",
    "\n",
    "    shop_diff_feature = pd.merge(shop_id, t1, how=\"left\", on=\"shop_id\")\n",
    "    shop_diff_feature = pd.merge(shop_diff_feature, t2, how=\"left\", on=\"shop_id\")\n",
    "    shop_diff_feature = pd.merge(shop_diff_feature, t3, how=\"left\", on=\"shop_id\")\n",
    "    shop_diff_feature = pd.merge(shop_diff_feature, t4, how=\"left\", on=\"shop_id\")\n",
    "    shop_diff_feature = pd.merge(shop_diff_feature, t5, how=\"left\", on=\"shop_id\")\n",
    "    shop_diff_feature = pd.merge(shop_diff_feature, t6, how=\"left\", on=\"shop_id\")\n",
    "    shop_diff_feature.fillna(value=0, inplace=True)\n",
    "    \n",
    "    shop_sample_feture = shop_data_current_day[[\"instance_id\", \"shop_id\"]]\n",
    "    shop_diff_feature = pd.merge(shop_sample_feture, shop_diff_feature, how=\"left\", on=\"shop_id\")\n",
    "    shop_diff_feature.drop(\"shop_id\", inplace=True, axis=1)\n",
    "    return shop_diff_feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shop_item_dist_count                          #商店中的不同广告个数\n",
    "shop_y1_item_dist_count                       #商店中购买的不同广告个数\n",
    "shop_y1_item_count_div_item_count             #商店中广告的购买率\n",
    "shop_y1_item_count_div_item_dist_count        #商店中购买的广告个数比不同的广告个数\n",
    "shop_y1_item_dist_count_div_item_dist_coun    #商店中购买的不同广告个数比不同的广告个数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "######################################################历史转化率：商店\n",
    "def get_shop_feature(sample_data, history_data):\n",
    "    shop_data_current_day = sample_data\n",
    "    shop_data = history_data\n",
    "\n",
    "    shop_id = shop_data.loc[:,[\"shop_id\"]].drop_duplicates()\n",
    "    shop_data_is_trade = shop_data.loc[shop_data[\"is_trade\"] == 1, :]\n",
    "    shop_group = shop_data.groupby(\"shop_id\")\n",
    "    shop_y1_group = shop_data_is_trade.groupby(\"shop_id\")\n",
    "\n",
    "    tf0 = shop_group[[\"item_id\"]].count()\n",
    "    tf0.rename(columns={\"item_id\":\"shop_count\"},inplace=True)\n",
    "    #广告统计\n",
    "    tf1_1 = tf0.copy()\n",
    "    tf1_1[\"shop_item_dist_count\"] = shop_group[\"item_id\"].agg(lambda x:x.unique().size)\n",
    "    tf1_2 = shop_y1_group[[\"item_id\"]].count()\n",
    "    tf1_2.rename(columns={\"item_id\": \"shop_y1_item_count\"},inplace=True)\n",
    "    tf1_2[\"shop_y1_item_dist_count\"] = shop_y1_group[\"item_id\"].agg(lambda x:x.unique().size)\n",
    "    tf1_1.reset_index(inplace=True)\n",
    "    tf1_2.reset_index(inplace=True)\n",
    "    tf1_tmp = pd.merge(left=tf1_1, right=tf1_2, how=\"left\", on=\"shop_id\")\n",
    "    tf1_tmp[\"shop_y1_item_count_div_item_count\"] = tf1_tmp[\"shop_y1_item_count\"] / tf1_tmp[\"shop_count\"]\n",
    "    tf1_tmp[\"shop_y1_item_count_div_item_dist_count\"] = tf1_tmp[\"shop_y1_item_count\"] / tf1_tmp[\"shop_item_dist_count\"]\n",
    "    tf1_tmp[\"shop_y1_item_dist_count_div_item_dist_count\"] = tf1_tmp[\"shop_y1_item_dist_count\"] / tf1_tmp[\"shop_item_dist_count\"]\n",
    "    tf1_tmp.fillna(value=0, inplace=True)\n",
    "    tf1 = tf1_tmp.drop(labels=\"shop_count\", axis=1)\n",
    "    #用户统计\n",
    "    tf2_1 = tf0.copy()\n",
    "    tf2_1[\"shop_user_dist_count\"] = shop_group[\"user_id\"].agg(lambda x:x.unique().size)\n",
    "    tf2_2 = shop_y1_group[[\"user_id\"]].count()\n",
    "    tf2_2.rename(columns={\"user_id\": \"shop_y1_user_count\"},inplace=True)\n",
    "    tf2_2[\"shop_y1_user_dist_count\"] = shop_y1_group[\"user_id\"].agg(lambda x:x.unique().size)\n",
    "    tf2_1.reset_index(inplace=True)\n",
    "    tf2_2.reset_index(inplace=True)\n",
    "    tf2_tmp = pd.merge(left=tf2_1, right=tf2_2, how=\"left\", on=\"shop_id\")\n",
    "    tf2_tmp[\"shop_y1_user_count_div_user_count\"] = tf2_tmp[\"shop_y1_user_count\"] / tf2_tmp[\"shop_count\"]\n",
    "    tf2_tmp[\"shop_y1_user_count_div_user_dist_count\"] = tf2_tmp[\"shop_y1_user_count\"] / tf2_tmp[\"shop_user_dist_count\"]\n",
    "    tf2_tmp[\"shop_y1_user_dist_count_div_user_dist_count\"] = tf2_tmp[\"shop_y1_user_dist_count\"] / tf2_tmp[\"shop_user_dist_count\"]\n",
    "    tf2_tmp.fillna(value=0, inplace=True)\n",
    "    tf2 = tf2_tmp.drop(labels=\"shop_count\", axis=1)\n",
    "    del tf2_1, tf2_2\n",
    "    gc.collect()\n",
    "    #广告品牌统计\n",
    "    tf3_1 = tf0.copy()\n",
    "    tf3_1[\"shop_item_brand_dist_count\"] = shop_group[\"item_brand_id\"].agg(lambda x:x[x!=-1].unique().size)\n",
    "    tf3_2 = shop_y1_group[[\"item_brand_id\"]].count()\n",
    "    tf3_2.rename(columns={\"item_brand_id\": \"shop_y1_item_brand_count\"},inplace=True)\n",
    "    tf3_2[\"shop_y1_item_brand_dist_count\"] = shop_y1_group[\"item_brand_id\"].agg(lambda x:x[x!=-1].unique().size)\n",
    "    tf3_1.reset_index(inplace=True)\n",
    "    tf3_2.reset_index(inplace=True)\n",
    "    tf3_tmp = pd.merge(left=tf3_1, right=tf3_2, how=\"left\", on=\"shop_id\")\n",
    "    tf3_tmp[\"shop_y1_item_brand_count_div_item_brand_count\"] = tf3_tmp[\"shop_y1_item_brand_count\"] / tf3_tmp[\"shop_count\"]\n",
    "    tf3_tmp[\"shop_y1_item_brand_count_div_item_brand_dist_count\"] = tf3_tmp[\"shop_y1_item_brand_count\"] / tf3_tmp[\"shop_item_brand_dist_count\"]\n",
    "    tf3_tmp[\"shop_y1_item_brand_dist_count_div_item_brand_dist_count\"] = tf3_tmp[\"shop_y1_item_brand_dist_count\"] / tf3_tmp[\"shop_item_brand_dist_count\"]\n",
    "    tf1_tmp.fillna(value=0, inplace=True)\n",
    "    tf3 = tf3_tmp.drop(labels=\"shop_count\", axis=1)\n",
    "    del tf3_1, tf3_2\n",
    "    gc.collect()\n",
    "    #城市统计\n",
    "    tf4_1 = tf0.copy()\n",
    "    tf4_group = shop_data.loc[shop_data[\"item_city_id\"] != -1,:].groupby(\"shop_id\")\n",
    "    tf4_1[\"shop_item_city_dist_count\"] = tf4_group[\"item_city_id\"].agg(lambda x:x[x!=-1].unique().size)\n",
    "    tf4_2 = shop_y1_group[[\"item_city_id\"]].count()\n",
    "    tf4_2.rename(columns={\"item_city_id\": \"shop_y1_item_city_count\"},inplace=True)\n",
    "    tf4_2[\"shop_y1_item_city_dist_count\"] = shop_y1_group[\"item_city_id\"].agg(lambda x:x[x!=-1].unique().size)\n",
    "    tf4_1.reset_index(inplace=True)\n",
    "    tf4_2.reset_index(inplace=True)\n",
    "    tf4_tmp = pd.merge(left=tf4_1, right=tf4_2, how=\"left\", on=\"shop_id\")\n",
    "    tf4_tmp[\"shop_y1_item_city_count_div_item_city_count\"] = tf4_tmp[\"shop_y1_item_city_count\"] / tf4_tmp[\"shop_count\"]\n",
    "    tf4_tmp[\"shop_y1_item_city_count_div_item_city_dist_count\"] = tf4_tmp[\"shop_y1_item_city_count\"] / tf4_tmp[\"shop_item_city_dist_count\"]\n",
    "    tf4_tmp[\"shop_y1_item_city_dist_count_div_item_city_dist_count\"] = tf4_tmp[\"shop_y1_item_city_dist_count\"] / tf4_tmp[\"shop_item_city_dist_count\"]\n",
    "    tf4_tmp.fillna(value=0, inplace=True)\n",
    "    tf4 = tf4_tmp.drop(labels=\"shop_count\", axis=1)\n",
    "    #tf4 = tf4_tmp.loc[:,[\"shop_id\", \"shop_y1_item_city_count_div_item_city_count\", \"shop_y1_item_city_count_div_item_city_dist_count\", \"shop_y1_item_city_dist_count_div_item_city_dist_count\"]]\n",
    "    del tf4_1, tf4_2\n",
    "    gc.collect()\n",
    "    #搜索属性统计\n",
    "    tf5_1 = tf0.copy()\n",
    "    tf5_group = shop_data.loc[shop_data[\"cp_head_2\"] != -1,:].groupby(\"shop_id\")\n",
    "    tf5_1[\"shop_predict_cp_dist_count\"] = tf5_group[\"cp_head_2\"].agg(lambda x:x[x!=-1].unique().size)\n",
    "    tf5_2 = shop_y1_group[[\"cp_head_2\"]].count()\n",
    "    tf5_2.rename(columns={\"cp_head_2\": \"shop_y1_predict_cp_count\"},inplace=True)\n",
    "    tf5_2[\"shop_y1_predict_cp_dist_count\"] = shop_y1_group[\"cp_head_2\"].agg(lambda x:x[x!=-1].unique().size)\n",
    "    tf5_1.reset_index(inplace=True)\n",
    "    tf5_2.reset_index(inplace=True)\n",
    "    tf5_tmp = pd.merge(left=tf5_1, right=tf5_2, how=\"left\", on=\"shop_id\")\n",
    "    tf5_tmp[\"shop_y1_predict_cp_count_div_predict_cp_count\"] = tf5_tmp[\"shop_y1_predict_cp_count\"] / tf5_tmp[\"shop_count\"]\n",
    "    tf5_tmp[\"shop_y1_predict_cp_count_div_predict_cp_dist_count\"] = tf5_tmp[\"shop_y1_predict_cp_count\"] / tf5_tmp[\"shop_predict_cp_dist_count\"]\n",
    "    tf5_tmp[\"shop_y1_predict_cp_dist_count_div_predict_cp_dist_count\"] = tf5_tmp[\"shop_y1_predict_cp_dist_count\"] / tf5_tmp[\"shop_predict_cp_dist_count\"]\n",
    "    tf5_tmp.fillna(value=0, inplace=True)\n",
    "    tf5 = tf5_tmp.drop(labels=\"shop_count\", axis=1)\n",
    "    del tf5_1, tf5_2\n",
    "    gc.collect()\n",
    "    #店铺中交易成功的广告商品的价格等级,销量等级，被收藏次数的等级，展示次数的等级的平均值\n",
    "    \n",
    "    tmp_data = shop_data_is_trade.loc[shop_data_is_trade[\"item_sales_level\"] !=-1, [\"shop_id\", \"item_id\", \"item_sales_level\"]].drop_duplicates()\n",
    "    tn1_2_group = tmp_data.groupby(\"shop_id\")\n",
    "    tn1_2 = tn1_2_group[[\"item_sales_level\"]].mean()\n",
    "    tn1_2.rename(columns={\"item_sales_level\":\"shop_y1_item_sales_level_mean\"}, inplace=True)\n",
    "    tn1_2.reset_index(inplace=True)\n",
    "    tn1_tmp = pd.merge(left=shop_id, right=tn1_2, how=\"left\", on=\"shop_id\")\n",
    "    tn1 = tn1_tmp\n",
    "\n",
    "    tmp_data = shop_data_is_trade.loc[shop_data_is_trade[\"item_price_level\"] !=-1, [\"shop_id\", \"item_id\", \"item_price_level\"]].drop_duplicates()\n",
    "    tn2_2_group = tmp_data.groupby(\"shop_id\")\n",
    "    tn2_2 = tn2_2_group[[\"item_price_level\"]].mean()\n",
    "    tn2_2.rename(columns={\"item_price_level\":\"shop_y1_item_price_level_mean\"}, inplace=True)\n",
    "    tn2_2.reset_index(inplace=True)\n",
    "    tn2 = tn2_tmp\n",
    "\n",
    "    tmp_data = shop_data_is_trade.loc[shop_data_is_trade[\"item_collected_level\"] !=-1, [\"shop_id\", \"item_id\", \"item_collected_level\"]].drop_duplicates()\n",
    "    tn3_2_group = tmp_data.groupby(\"shop_id\")\n",
    "    tn3_2 = tn3_2_group[[\"item_collected_level\"]].mean()\n",
    "    tn3_2.rename(columns={\"item_collected_level\":\"shop_y1_item_collected_level_mean\"}, inplace=True)\n",
    "    tn3_2.reset_index(inplace=True)\n",
    "    tn3_tmp = pd.merge(left=shop_id, right=tn3_2, how=\"left\", on=\"shop_id\")\n",
    "    tn3 = tn3_tmp\n",
    "    \n",
    "    tmp_data = shop_data_is_trade.loc[shop_data_is_trade[\"item_pv_level\"] !=-1, [\"shop_id\", \"item_id\", \"item_pv_level\"]].drop_duplicates()\n",
    "    tn4_2_group = tmp_data.groupby(\"shop_id\")\n",
    "    tn4_2 = tn4_2_group[[\"item_pv_level\"]].mean()\n",
    "    tn4_2.rename(columns={\"item_pv_level\":\"shop_y1_item_pv_level_mean\"}, inplace=True)\n",
    "    tn4_2.reset_index(inplace=True)\n",
    "    tn4_tmp = pd.merge(left=shop_id, right=tn4_2, how=\"left\", on=\"shop_id\")\n",
    "    tn4 = tn4_tmp\n",
    "\n",
    "    tmp_data = shop_data_is_trade.loc[shop_data_is_trade[\"user_age_level\"] !=-1,:]\n",
    "    tn5_2_group = tmp_data.groupby(\"shop_id\")\n",
    "    tn5_2 = tn5_2_group[[\"user_age_level\"]].mean()\n",
    "    tn5_2.rename(columns={\"user_age_level\":\"shop_y1_user_age_level_mean\"}, inplace=True)\n",
    "    tn5_2.reset_index(inplace=True)\n",
    "    tn5_tmp = pd.merge(left=shop_id, right=tn5_2, how=\"left\", on=\"shop_id\")\n",
    "    tn5 = tn5_tmp\n",
    "\n",
    "    tmp_data = shop_data_is_trade.loc[shop_data_is_trade[\"user_star_level\"] !=-1,:]\n",
    "    tn6_2_group = tmp_data.groupby(\"shop_id\")\n",
    "    tn6_2 = tn6_2_group[[\"user_star_level\"]].mean()\n",
    "    tn6_2.rename(columns={\"user_star_level\":\"shop_y1_user_star_level_mean\"}, inplace=True)\n",
    "    tn6_2.reset_index(inplace=True)\n",
    "    tn6_tmp = pd.merge(left=shop_id, right=tn6_2, how=\"left\", on=\"shop_id\")\n",
    "    tn6 = tn6_tmp\n",
    "    \n",
    "    tmp_data = shop_data_is_trade.loc[shop_data_is_trade[\"context_page_id\"] !=-1,:]\n",
    "    tn7_2_group = tmp_data.groupby(\"shop_id\")\n",
    "    tn7_2 = tn7_2_group[[\"context_page_id\"]].mean()\n",
    "    tn7_2.rename(columns={\"context_page_id\":\"shop_y1_context_page_id_mean\"}, inplace=True)\n",
    "    tn7_2.reset_index(inplace=True)\n",
    "    tn7_tmp = pd.merge(left=shop_id, right=tn7_2, how=\"left\", on=\"shop_id\")\n",
    "    tn7 = tn7_tmp\n",
    "    \n",
    "    tmp_data = shop_data.loc[shop_data[\"shop_review_num_level\"] !=-1, :]\n",
    "    tn8_1 = tmp_data.groupby(\"shop_id\")[[\"shop_review_num_level\"]].mean().reset_index()\n",
    "    tn8_1.rename(columns={\"shop_review_num_level\":\"shop_shop_review_num_level_mean\"}, inplace=True)\n",
    "    tmp_data = shop_data_is_trade.loc[shop_data_is_trade[\"shop_review_num_level\"] !=-1,:]\n",
    "    tn8_2_group = tmp_data.groupby(\"shop_id\")\n",
    "    tn8_2 = tn8_2_group[[\"shop_review_num_level\"]].mean()\n",
    "    tn8_2.rename(columns={\"shop_review_num_level\":\"shop_y1_shop_review_num_level_mean\"}, inplace=True)\n",
    "    tn8_2.reset_index(inplace=True)\n",
    "    tn8_tmp = pd.merge(left=shop_id, right=tn8_1, how=\"left\", on=\"shop_id\")\n",
    "    tn8_tmp = pd.merge(left=tn8_tmp, right=tn8_2, how=\"left\", on=\"shop_id\")\n",
    "    tn8 = tn8_tmp\n",
    "\n",
    "    tmp_data = shop_data.loc[shop_data[\"shop_review_positive_rate\"] !=-1, :]\n",
    "    tn9_1 = tmp_data.groupby(\"shop_id\")[[\"shop_review_positive_rate\"]].mean().reset_index()\n",
    "    tn9_1.rename(columns={\"shop_review_positive_rate\":\"shop_shop_review_positive_rate_mean\"}, inplace=True)\n",
    "    tmp_data = shop_data_is_trade.loc[shop_data_is_trade[\"shop_review_positive_rate\"] !=-1,:]\n",
    "    tn9_2_group = tmp_data.groupby(\"shop_id\")\n",
    "    tn9_2 = tn9_2_group[[\"shop_review_positive_rate\"]].mean()\n",
    "    tn9_2.rename(columns={\"shop_review_positive_rate\":\"shop_y1_shop_review_positive_rate_mean\"}, inplace=True)\n",
    "    tn9_2.reset_index(inplace=True)\n",
    "    tn9_tmp = pd.merge(left=shop_id, right=tn9_1, how=\"left\", on=\"shop_id\")\n",
    "    tn9_tmp = pd.merge(left=tn9_tmp, right=tn9_2, how=\"left\", on=\"shop_id\")\n",
    "    tn9 = tn9_tmp\n",
    "\n",
    "    tmp_data = shop_data.loc[shop_data[\"shop_star_level\"] !=-1, :]\n",
    "    tn10_1 = tmp_data.groupby(\"shop_id\")[[\"shop_star_level\"]].mean().reset_index()\n",
    "    tn10_1.rename(columns={\"shop_star_level\":\"shop_shop_star_level_mean\"}, inplace=True)\n",
    "    tmp_data = shop_data_is_trade.loc[shop_data_is_trade[\"shop_star_level\"] !=-1,:]\n",
    "    tn10_2_group = tmp_data.groupby(\"shop_id\")\n",
    "    tn10_2 = tn10_2_group[[\"shop_star_level\"]].mean()\n",
    "    tn10_2.rename(columns={\"shop_star_level\":\"shop_y1_shop_star_level_mean\"}, inplace=True)\n",
    "    tn10_2.reset_index(inplace=True)\n",
    "    tn10_tmp = pd.merge(left=shop_id, right=tn10_1, how=\"left\", on=\"shop_id\")\n",
    "    tn10_tmp = pd.merge(left=tn10_tmp, right=tn10_2, how=\"left\", on=\"shop_id\")\n",
    "    tn10 = tn10_tmp\n",
    "\n",
    "    tmp_data = shop_data.loc[shop_data[\"shop_score_delivery\"] !=-1, :]\n",
    "    tn11_1 = tmp_data.groupby(\"shop_id\")[[\"shop_score_delivery\"]].mean().reset_index()\n",
    "    tn11_1.rename(columns={\"shop_score_delivery\":\"shop_shop_score_delivery_mean\"}, inplace=True)\n",
    "    tmp_data = shop_data_is_trade.loc[shop_data_is_trade[\"shop_score_delivery\"] !=-1,:]\n",
    "    tn11_2_group = tmp_data.groupby(\"shop_id\")\n",
    "    tn11_2 = tn11_2_group[[\"shop_score_delivery\"]].mean()\n",
    "    tn11_2.rename(columns={\"shop_score_delivery\":\"shop_y1_shop_score_delivery_mean\"}, inplace=True)\n",
    "    tn11_2.reset_index(inplace=True)\n",
    "    tn11_tmp = pd.merge(left=shop_id, right=tn11_1, how=\"left\", on=\"shop_id\")\n",
    "    tn11_tmp = pd.merge(left=tn11_tmp, right=tn11_2, how=\"left\", on=\"shop_id\")\n",
    "    tn11 = tn11_tmp\n",
    "\n",
    "    tmp_data = shop_data.loc[shop_data[\"shop_score_description\"] !=-1, :]\n",
    "    tn12_1 = tmp_data.groupby(\"shop_id\")[[\"shop_score_description\"]].mean().reset_index()\n",
    "    tn12_1.rename(columns={\"shop_score_description\":\"shop_shop_score_description_mean\"}, inplace=True)\n",
    "    tmp_data = shop_data_is_trade.loc[shop_data_is_trade[\"shop_score_description\"] !=-1,:]\n",
    "    tn12_2_group = tmp_data.groupby(\"shop_id\")\n",
    "    tn12_2 = tn12_2_group[[\"shop_score_description\"]].mean()\n",
    "    tn12_2.rename(columns={\"shop_score_description\":\"shop_y1_shop_score_description_mean\"}, inplace=True)\n",
    "    tn12_2.reset_index(inplace=True)\n",
    "    tn12_tmp = pd.merge(left=shop_id, right=tn12_1, how=\"left\", on=\"shop_id\")\n",
    "    tn12_tmp = pd.merge(left=tn12_tmp, right=tn12_2, how=\"left\", on=\"shop_id\")\n",
    "    tn12 = tn12_tmp\n",
    "    \n",
    "    shop_id = shop_data.loc[:,[\"shop_id\"]].drop_duplicates()\n",
    "\n",
    "    shop_feature = tf0.reset_index()\n",
    "    shop_feature = pd.merge(left=shop_feature, right=tf1, how=\"left\", on=\"shop_id\")\n",
    "    shop_feature = pd.merge(left=shop_feature, right=tf2, how=\"left\", on=\"shop_id\")\n",
    "    shop_feature = pd.merge(left=shop_feature, right=tf3, how=\"left\", on=\"shop_id\")\n",
    "    shop_feature = pd.merge(left=shop_feature, right=tf4, how=\"left\", on=\"shop_id\")\n",
    "    shop_feature = pd.merge(left=shop_feature, right=tf5, how=\"left\", on=\"shop_id\")\n",
    "    shop_feature = pd.merge(left=shop_feature, right=tn1, how=\"left\", on=\"shop_id\")\n",
    "    shop_feature = pd.merge(left=shop_feature, right=tn2, how=\"left\", on=\"shop_id\")\n",
    "    shop_feature = pd.merge(left=shop_feature, right=tn3, how=\"left\", on=\"shop_id\")\n",
    "    shop_feature = pd.merge(left=shop_feature, right=tn4, how=\"left\", on=\"shop_id\")\n",
    "    shop_feature = pd.merge(left=shop_feature, right=tn5, how=\"left\", on=\"shop_id\")\n",
    "    shop_feature = pd.merge(left=shop_feature, right=tn6, how=\"left\", on=\"shop_id\")\n",
    "    shop_feature = pd.merge(left=shop_feature, right=tn7, how=\"left\", on=\"shop_id\")\n",
    "    shop_feature = pd.merge(left=shop_feature, right=tn8, how=\"left\", on=\"shop_id\")\n",
    "    shop_feature = pd.merge(left=shop_feature, right=tn9, how=\"left\", on=\"shop_id\")\n",
    "    shop_feature = pd.merge(left=shop_feature, right=tn10, how=\"left\", on=\"shop_id\")\n",
    "    shop_feature = pd.merge(left=shop_feature, right=tn11, how=\"left\", on=\"shop_id\")\n",
    "    shop_feature = pd.merge(left=shop_feature, right=tn12, how=\"left\", on=\"shop_id\")\n",
    "    #shop_feature = pd.merge(left=shop_feature, right=to1, how=\"left\", on=\"shop_id\")\n",
    "    \n",
    "    shop_sample_feture = shop_data_current_day[[\"instance_id\", \"shop_id\"]]\n",
    "    shop_feature = pd.merge(shop_sample_feture, shop_feature, how=\"left\", on=\"shop_id\")\n",
    "    shop_feature.drop(\"shop_id\", inplace=True, axis=1)\n",
    "    return shop_feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "######################################################历史转化率：广告\n",
    "def get_item_feature(sample_data, history_data):\n",
    "    item_data_current_day = sample_data\n",
    "    item_data = history_data\n",
    "    \n",
    "    item_id = item_data.loc[:,[\"item_id\"]].drop_duplicates()\n",
    "    item_data_is_trade = item_data.loc[item_data[\"is_trade\"] == 1, :]\n",
    "    item_group = item_data.groupby(\"item_id\")\n",
    "    item_y1_group = item_data_is_trade.groupby(\"item_id\")\n",
    "\n",
    "    tf0 = item_group[[\"user_id\"]].count()\n",
    "    tf0.rename(columns={\"user_id\":\"item_count\"},inplace=True)\n",
    "    #用户统计\n",
    "    tf1_1 = tf0.copy()\n",
    "    tf1_1[\"item_user_dist_count\"] = item_group[\"user_id\"].agg(lambda x:x.unique().size)\n",
    "    tf1_2 = item_y1_group[[\"user_id\"]].count()\n",
    "    tf1_2.rename(columns={\"user_id\": \"item_y1_user_count\"},inplace=True)\n",
    "    tf1_2[\"item_y1_user_dist_count\"] = item_y1_group[\"user_id\"].agg(lambda x:x.unique().size)\n",
    "    tf1_1.reset_index(inplace=True)\n",
    "    tf1_2.reset_index(inplace=True)\n",
    "    tf1_tmp = pd.merge(left=tf1_1, right=tf1_2, how=\"left\", on=\"item_id\")\n",
    "    tf1_tmp[\"item_y1_user_count_div_user_count\"] = tf1_tmp[\"item_y1_user_count\"] / tf1_tmp[\"item_count\"]\n",
    "    tf1_tmp[\"item_y1_user_count_div_user_dist_count\"] = tf1_tmp[\"item_y1_user_count\"] / tf1_tmp[\"item_user_dist_count\"]\n",
    "    tf1_tmp[\"item_y1_user_dist_count_div_user_dist_count\"] = tf1_tmp[\"item_y1_user_dist_count\"] / tf1_tmp[\"item_user_dist_count\"]\n",
    "    tf1_tmp.fillna(value=0, inplace=True)\n",
    "    tf1 = tf1_tmp.drop(labels=\"item_count\", axis=1)\n",
    "\n",
    "    #商店统计\n",
    "    tf2_1 = tf0.copy()\n",
    "    tf2_1[\"item_shop_dist_count\"] = item_group[\"shop_id\"].agg(lambda x:x.unique().size)\n",
    "    tf2_2 = item_y1_group[[\"shop_id\"]].count()\n",
    "    tf2_2.rename(columns={\"shop_id\": \"item_y1_shop_count\"},inplace=True)\n",
    "    tf2_2[\"item_y1_shop_dist_count\"] = item_y1_group[\"shop_id\"].agg(lambda x:x.unique().size)\n",
    "    tf2_1.reset_index(inplace=True)\n",
    "    tf2_2.reset_index(inplace=True)\n",
    "    tf2_tmp = pd.merge(left=tf2_1, right=tf2_2, how=\"left\", on=\"item_id\")\n",
    "    tf2_tmp[\"item_y1_shop_count_div_shop_count\"] = tf2_tmp[\"item_y1_shop_count\"] / tf2_tmp[\"item_count\"]\n",
    "    tf2_tmp[\"item_y1_shop_count_div_shop_dist_count\"] = tf2_tmp[\"item_y1_shop_count\"] / tf2_tmp[\"item_shop_dist_count\"]\n",
    "    tf2_tmp[\"item_y1_shop_dist_count_div_shop_dist_count\"] = tf2_tmp[\"item_y1_shop_dist_count\"] / tf2_tmp[\"item_shop_dist_count\"]\n",
    "    tf2_tmp.fillna(value=0, inplace=True)\n",
    "    tf2 = tf2_tmp.drop(labels=\"item_count\", axis=1)\n",
    "\n",
    "    #店铺中交易成功的广告商品的价格等级,销量等级，被收藏次数的等级，展示次数的等级的平均值\n",
    "    tmp_data = item_data_is_trade.loc[item_data_is_trade[\"user_age_level\"] !=-1,:]\n",
    "    tn5_2_group = tmp_data.groupby(\"item_id\")\n",
    "    tn5_2 = tn5_2_group[[\"user_age_level\"]].mean()\n",
    "    tn5_2.rename(columns={\"user_age_level\":\"item_y1_user_age_level_mean\"}, inplace=True)\n",
    "    tn5_2.reset_index(inplace=True)\n",
    "    tn5_tmp = pd.merge(left=item_id, right=tn5_2, how=\"left\", on=\"item_id\")\n",
    "    tn5 = tn5_tmp\n",
    "\n",
    "    tmp_data = item_data_is_trade.loc[item_data_is_trade[\"user_star_level\"] !=-1,:]\n",
    "    tn6_2_group = tmp_data.groupby(\"item_id\")\n",
    "    tn6_2 = tn6_2_group[[\"user_star_level\"]].mean()\n",
    "    tn6_2.rename(columns={\"user_star_level\":\"item_y1_user_star_level_mean\"}, inplace=True)\n",
    "    tn6_2.reset_index(inplace=True)\n",
    "    tn6_tmp = pd.merge(left=item_id, right=tn6_2, how=\"left\", on=\"item_id\")\n",
    "    tn6 = tn6_tmp\n",
    "\n",
    "    tmp_data = item_data_is_trade.loc[item_data_is_trade[\"shop_review_num_level\"] !=-1,:]\n",
    "    tn8_2_group = tmp_data.groupby(\"item_id\")\n",
    "    tn8_2 = tn8_2_group[[\"shop_review_num_level\"]].mean()\n",
    "    tn8_2.rename(columns={\"shop_review_num_level\":\"item_y1_shop_review_num_level_mean\"}, inplace=True)\n",
    "    tn8_2.reset_index(inplace=True)\n",
    "    tn8_tmp = pd.merge(left=item_id, right=tn8_2, how=\"left\", on=\"item_id\")\n",
    "    tn8 = tn8_tmp\n",
    "\n",
    "    tmp_data = item_data_is_trade.loc[item_data_is_trade[\"shop_review_positive_rate\"] !=-1,:]\n",
    "    tn9_2_group = tmp_data.groupby(\"item_id\")\n",
    "    tn9_2 = tn9_2_group[[\"shop_review_positive_rate\"]].mean()\n",
    "    tn9_2.rename(columns={\"shop_review_positive_rate\":\"item_y1_shop_review_positive_rate_mean\"}, inplace=True)\n",
    "    tn9_2.reset_index(inplace=True)\n",
    "    tn9_tmp = pd.merge(left=item_id, right=tn9_2, how=\"left\", on=\"item_id\")\n",
    "    tn9 = tn9_tmp\n",
    "\n",
    "    tmp_data = item_data_is_trade.loc[item_data_is_trade[\"shop_star_level\"] !=-1,:]\n",
    "    tn10_2_group = tmp_data.groupby(\"item_id\")\n",
    "    tn10_2 = tn10_2_group[[\"shop_star_level\"]].mean()\n",
    "    tn10_2.rename(columns={\"shop_star_level\":\"item_y1_shop_star_level_mean\"}, inplace=True)\n",
    "    tn10_2.reset_index(inplace=True)\n",
    "    tn10_tmp = pd.merge(left=item_id, right=tn10_2, how=\"left\", on=\"item_id\")\n",
    "    tn10 = tn10_tmp\n",
    "\n",
    "    tmp_data = item_data_is_trade.loc[item_data_is_trade[\"shop_score_delivery\"] !=-1,:]\n",
    "    tn11_2_group = tmp_data.groupby(\"item_id\")\n",
    "    tn11_2 = tn11_2_group[[\"shop_score_delivery\"]].mean()\n",
    "    tn11_2.rename(columns={\"shop_score_delivery\":\"item_y1_shop_score_delivery_mean\"}, inplace=True)\n",
    "    tn11_2.reset_index(inplace=True)\n",
    "    tn11_tmp = pd.merge(left=item_id, right=tn11_2, how=\"left\", on=\"item_id\")\n",
    "    tn11 = tn11_tmp\n",
    "\n",
    "    tmp_data = item_data_is_trade.loc[item_data_is_trade[\"shop_score_description\"] !=-1,:]\n",
    "    tn12_2_group = tmp_data.groupby(\"item_id\")\n",
    "    tn12_2 = tn12_2_group[[\"shop_score_description\"]].mean()\n",
    "    tn12_2.rename(columns={\"shop_score_description\":\"item_y1_shop_score_description_mean\"}, inplace=True)\n",
    "    tn12_2.reset_index(inplace=True)\n",
    "    tn12_tmp = pd.merge(left=item_id, right=tn12_2, how=\"left\", on=\"item_id\")\n",
    "    tn12 = tn12_tmp\n",
    "\n",
    "    item_feature = tf0.reset_index()\n",
    "    item_feature = pd.merge(left=item_feature, right=tf1, how=\"left\", on=\"item_id\")\n",
    "    item_feature = pd.merge(left=item_feature, right=tf2, how=\"left\", on=\"item_id\")\n",
    "    item_feature = pd.merge(left=item_feature, right=tn5, how=\"left\", on=\"item_id\")\n",
    "    item_feature = pd.merge(left=item_feature, right=tn8, how=\"left\", on=\"item_id\")\n",
    "    item_feature = pd.merge(left=item_feature, right=tn9, how=\"left\", on=\"item_id\")\n",
    "    item_feature = pd.merge(left=item_feature, right=tn10, how=\"left\", on=\"item_id\")\n",
    "    item_feature = pd.merge(left=item_feature, right=tn11, how=\"left\", on=\"item_id\")\n",
    "    item_feature = pd.merge(left=item_feature, right=tn12, how=\"left\", on=\"item_id\")\n",
    "    #item_feature = pd.merge(left=item_feature, right=to1, how=\"left\", on=\"item_id\")\n",
    "    \n",
    "    item_sample_feture = item_data_current_day[[\"instance_id\", \"item_id\"]]\n",
    "    item_feature = pd.merge(item_sample_feture, item_feature, how=\"left\", on=\"item_id\")\n",
    "    item_feature.drop(\"item_id\", inplace=True, axis=1)\n",
    "    return item_feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################################历史转化率：广告品牌ID\n",
    "def get_item_brand_feature(sample_data, history_data):\n",
    "    #item_brand_data_current_day=train_data_07\n",
    "    #item_brand_data = item_brand_history_data\n",
    "    item_brand_data_current_day = sample_data\n",
    "    item_brand_data = history_data\n",
    "\n",
    "    item_brand_id = item_brand_data.loc[:,[\"item_brand_id\"]].drop_duplicates()\n",
    "    item_brand_data_is_trade = item_brand_data.loc[item_brand_data[\"is_trade\"] == 1, :]\n",
    "    item_brand_group = item_brand_data.groupby(\"item_brand_id\")\n",
    "    item_brand_y1_group = item_brand_data_is_trade.groupby(\"item_brand_id\")\n",
    "\n",
    "    tf0 = item_brand_group[[\"instance_id\"]].count()\n",
    "    tf0.rename(columns={\"instance_id\":\"item_brand_count\"},inplace=True)\n",
    "\n",
    "    tf1_1 = tf0.copy()\n",
    "    tf1_1[\"item_brand_item_dist_count\"] = item_brand_group[\"item_id\"].agg(lambda x:x.unique().size)\n",
    "    tf1_2 = item_brand_y1_group[[\"item_id\"]].count()\n",
    "    tf1_2.rename(columns={\"item_id\": \"item_brand_y1_item_count\"},inplace=True)\n",
    "    tf1_2[\"item_brand_y1_item_dist_count\"] = item_brand_y1_group[\"item_id\"].agg(lambda x:x.unique().size)\n",
    "    tf1_1.reset_index(inplace=True)\n",
    "    tf1_2.reset_index(inplace=True)\n",
    "    tf1_tmp = pd.merge(left=tf1_1, right=tf1_2, how=\"left\", on=\"item_brand_id\")\n",
    "    tf1_tmp[\"item_brand_y1_item_count_div_item_count\"] = tf1_tmp[\"item_brand_y1_item_count\"] / tf1_tmp[\"item_brand_count\"]\n",
    "    tf1_tmp[\"item_brand_y1_item_count_div_item_dist_count\"] = tf1_tmp[\"item_brand_y1_item_count\"] / tf1_tmp[\"item_brand_item_dist_count\"]\n",
    "    tf1_tmp[\"item_brand_y1_item_dist_count_div_item_dist_count\"] = tf1_tmp[\"item_brand_y1_item_dist_count\"] / tf1_tmp[\"item_brand_item_dist_count\"]\n",
    "    tf1_tmp.fillna(value=0, inplace=True)\n",
    "    tf1 = tf1_tmp.drop(labels=\"item_brand_count\", axis=1)\n",
    "    del tf1_1, tf1_2\n",
    "    gc.collect()\n",
    "    tf2_1 = tf0.copy()\n",
    "    tf2_1[\"item_brand_user_dist_count\"] = item_brand_group[\"user_id\"].agg(lambda x:x.unique().size)\n",
    "    tf2_2 = item_brand_y1_group[[\"user_id\"]].count()\n",
    "    tf2_2.rename(columns={\"user_id\": \"item_brand_y1_user_count\"},inplace=True)\n",
    "    tf2_2[\"item_brand_y1_user_dist_count\"] = item_brand_y1_group[\"user_id\"].agg(lambda x:x.unique().size)\n",
    "    tf2_1.reset_index(inplace=True)\n",
    "    tf2_2.reset_index(inplace=True)\n",
    "    tf2_tmp = pd.merge(left=tf2_1, right=tf2_2, how=\"left\", on=\"item_brand_id\")\n",
    "    tf2_tmp[\"item_brand_y1_user_count_div_user_count\"] = tf2_tmp[\"item_brand_y1_user_count\"] / tf2_tmp[\"item_brand_count\"]\n",
    "    tf2_tmp[\"item_brand_y1_user_count_div_user_dist_count\"] = tf2_tmp[\"item_brand_y1_user_count\"] / tf2_tmp[\"item_brand_user_dist_count\"]\n",
    "    tf2_tmp[\"item_brand_y1_user_dist_count_div_user_dist_count\"] = tf2_tmp[\"item_brand_y1_user_dist_count\"] / tf2_tmp[\"item_brand_user_dist_count\"]\n",
    "    tf2_tmp.fillna(value=0, inplace=True)\n",
    "    tf2 = tf2_tmp.drop(labels=\"item_brand_count\", axis=1)\n",
    "    #tf2 = tf2_tmp.loc[:,[\"item_brand_id\", \"item_brand_y1_user_count_div_user_count\", \"item_brand_y1_user_count_div_user_dist_count\", \"item_brand_y1_user_dist_count_div_user_dist_count\"]]\n",
    "    del tf2_1, tf2_2\n",
    "    gc.collect()\n",
    "\n",
    "    tf3_1 = tf0.copy()\n",
    "    tf3_1[\"item_brand_shop_dist_count\"] = item_brand_group[\"shop_id\"].agg(lambda x:x[x!=-1].unique().size)\n",
    "    tf3_2 = item_brand_y1_group[[\"shop_id\"]].count()\n",
    "    tf3_2.rename(columns={\"shop_id\": \"item_brand_y1_shop_count\"},inplace=True)\n",
    "    tf3_2[\"item_brand_y1_shop_dist_count\"] = item_brand_y1_group[\"shop_id\"].agg(lambda x:x[x!=-1].unique().size)\n",
    "    tf3_1.reset_index(inplace=True)\n",
    "    tf3_2.reset_index(inplace=True)\n",
    "    tf3_tmp = pd.merge(left=tf3_1, right=tf3_2, how=\"left\", on=\"item_brand_id\")\n",
    "    tf3_tmp[\"item_brand_y1_shop_count_div_shop_count\"] = tf3_tmp[\"item_brand_y1_shop_count\"] / tf3_tmp[\"item_brand_count\"]\n",
    "    tf3_tmp[\"item_brand_y1_shop_count_div_shop_dist_count\"] = tf3_tmp[\"item_brand_y1_shop_count\"] / tf3_tmp[\"item_brand_shop_dist_count\"]\n",
    "    tf3_tmp[\"item_brand_y1_shop_dist_count_div_shop_dist_count\"] = tf3_tmp[\"item_brand_y1_shop_dist_count\"] / tf3_tmp[\"item_brand_shop_dist_count\"]\n",
    "    tf1_tmp.fillna(value=0, inplace=True)\n",
    "    tf3 = tf3_tmp.drop(labels=\"item_brand_count\", axis=1)\n",
    "    del tf3_1, tf3_2\n",
    "    gc.collect()\n",
    "\n",
    "    tf4_1 = tf0.copy()\n",
    "    tf4_group = item_brand_data.loc[item_brand_data[\"item_city_id\"] != -1,:].groupby(\"item_brand_id\")\n",
    "    tf4_1[\"item_brand_item_city_dist_count\"] = tf4_group[\"item_city_id\"].agg(lambda x:x[x!=-1].unique().size)\n",
    "    tf4_2 = item_brand_y1_group[[\"item_city_id\"]].count()\n",
    "    tf4_2.rename(columns={\"item_city_id\": \"item_brand_y1_item_city_count\"},inplace=True)\n",
    "    tf4_2[\"item_brand_y1_item_city_dist_count\"] = item_brand_y1_group[\"item_city_id\"].agg(lambda x:x[x!=-1].unique().size)\n",
    "    tf4_1.reset_index(inplace=True)\n",
    "    tf4_2.reset_index(inplace=True)\n",
    "    tf4_tmp = pd.merge(left=tf4_1, right=tf4_2, how=\"left\", on=\"item_brand_id\")\n",
    "    tf4_tmp[\"item_brand_y1_item_city_count_div_item_city_count\"] = tf4_tmp[\"item_brand_y1_item_city_count\"] / tf4_tmp[\"item_brand_count\"]\n",
    "    tf4_tmp[\"item_brand_y1_item_city_count_div_item_city_dist_count\"] = tf4_tmp[\"item_brand_y1_item_city_count\"] / tf4_tmp[\"item_brand_item_city_dist_count\"]\n",
    "    tf4_tmp[\"item_brand_y1_item_city_dist_count_div_item_city_dist_count\"] = tf4_tmp[\"item_brand_y1_item_city_dist_count\"] / tf4_tmp[\"item_brand_item_city_dist_count\"]\n",
    "    tf4_tmp.fillna(value=0, inplace=True)\n",
    "    tf4 = tf4_tmp.drop(labels=\"item_brand_count\", axis=1)\n",
    "    del tf4_1, tf4_2\n",
    "    gc.collect()\n",
    "\n",
    "    tf5_1 = tf0.copy()\n",
    "    tf5_group = item_brand_data.loc[item_brand_data[\"cp_head_2\"] != -1,:].groupby(\"item_brand_id\")\n",
    "    tf5_1[\"item_brand_predict_cp_dist_count\"] = tf5_group[\"cp_head_2\"].agg(lambda x:x[x!=-1].unique().size)\n",
    "    tf5_2 = item_brand_y1_group[[\"cp_head_2\"]].count()\n",
    "    tf5_2.rename(columns={\"cp_head_2\": \"item_brand_y1_predict_cp_count\"},inplace=True)\n",
    "    tf5_2[\"item_brand_y1_predict_cp_dist_count\"] = item_brand_y1_group[\"cp_head_2\"].agg(lambda x:x[x!=-1].unique().size)\n",
    "    tf5_1.reset_index(inplace=True)\n",
    "    tf5_2.reset_index(inplace=True)\n",
    "    tf5_tmp = pd.merge(left=tf5_1, right=tf5_2, how=\"left\", on=\"item_brand_id\")\n",
    "    tf5_tmp[\"item_brand_y1_predict_cp_count_div_predict_cp_count\"] = tf5_tmp[\"item_brand_y1_predict_cp_count\"] / tf5_tmp[\"item_brand_count\"]\n",
    "    tf5_tmp[\"item_brand_y1_predict_cp_count_div_predict_cp_dist_count\"] = tf5_tmp[\"item_brand_y1_predict_cp_count\"] / tf5_tmp[\"item_brand_predict_cp_dist_count\"]\n",
    "    tf5_tmp[\"item_brand_y1_predict_cp_dist_count_div_predict_cp_dist_count\"] = tf5_tmp[\"item_brand_y1_predict_cp_dist_count\"] / tf5_tmp[\"item_brand_predict_cp_dist_count\"]\n",
    "    tf5_tmp.fillna(value=0, inplace=True)\n",
    "    tf5 = tf5_tmp.drop(labels=\"item_brand_count\", axis=1)\n",
    "    del tf5_1, tf5_2\n",
    "    gc.collect()\n",
    "    #店铺中交易成功的广告商品的价格等级,销量等级，被收藏次数的等级，展示次数的等级的平均值\n",
    "    tmp_data = item_brand_data_is_trade.loc[item_brand_data_is_trade[\"item_sales_level\"] !=-1, [\"item_brand_id\", \"item_id\", \"item_sales_level\"]].drop_duplicates()\n",
    "    tn1_2_group = tmp_data.groupby(\"item_brand_id\")\n",
    "    tn1_2 = tn1_2_group[[\"item_sales_level\"]].mean()\n",
    "    tn1_2.rename(columns={\"item_sales_level\":\"item_brand_y1_item_sales_level_mean\"}, inplace=True)\n",
    "    tn1_2.reset_index(inplace=True)\n",
    "    tn1_tmp = pd.merge(left=item_brand_id, right=tn1_2, how=\"left\", on=\"item_brand_id\")\n",
    "    tn1 = tn1_tmp\n",
    "\n",
    "    tmp_data = item_brand_data_is_trade.loc[item_brand_data_is_trade[\"item_price_level\"] !=-1, [\"item_brand_id\", \"item_id\", \"item_price_level\"]].drop_duplicates()\n",
    "    tn2_2_group = tmp_data.groupby(\"item_brand_id\")\n",
    "    tn2_2 = tn2_2_group[[\"item_price_level\"]].mean()\n",
    "    tn2_2.rename(columns={\"item_price_level\":\"item_brand_y1_item_price_level_mean\"}, inplace=True)\n",
    "    tn2_2.reset_index(inplace=True)\n",
    "    tn2_tmp = pd.merge(left=item_brand_id, right=tn2_2, how=\"left\", on=\"item_brand_id\")\n",
    "    tn2 = tn2_tmp\n",
    "\n",
    "    tmp_data = item_brand_data_is_trade.loc[item_brand_data_is_trade[\"item_collected_level\"] !=-1, [\"item_brand_id\", \"item_id\", \"item_collected_level\"]].drop_duplicates()\n",
    "    tn3_2_group = tmp_data.groupby(\"item_brand_id\")\n",
    "    tn3_2 = tn3_2_group[[\"item_collected_level\"]].mean()\n",
    "    tn3_2.rename(columns={\"item_collected_level\":\"item_brand_y1_item_collected_level_mean\"}, inplace=True)\n",
    "    tn3_2.reset_index(inplace=True)\n",
    "    tn3_tmp = pd.merge(left=item_brand_id, right=tn3_2, how=\"left\", on=\"item_brand_id\")\n",
    "    tn3 = tn3_tmp\n",
    "\n",
    "    tmp_data = item_brand_data_is_trade.loc[item_brand_data_is_trade[\"item_pv_level\"] !=-1, [\"item_brand_id\", \"item_id\", \"item_pv_level\"]].drop_duplicates()\n",
    "    tn4_2_group = tmp_data.groupby(\"item_brand_id\")\n",
    "    tn4_2 = tn4_2_group[[\"item_pv_level\"]].mean()\n",
    "    tn4_2.rename(columns={\"item_pv_level\":\"item_brand_y1_item_pv_level_mean\"}, inplace=True)\n",
    "    tn4_2.reset_index(inplace=True)\n",
    "    tn4_tmp = pd.merge(left=item_brand_id, right=tn4_2, how=\"left\", on=\"item_brand_id\")\n",
    "    tn4 = tn4_tmp\n",
    "\n",
    "    tmp_data = item_brand_data_is_trade.loc[item_brand_data_is_trade[\"user_age_level\"] !=-1,:]\n",
    "    tn5_2_group = tmp_data.groupby(\"item_brand_id\")\n",
    "    tn5_2 = tn5_2_group[[\"user_age_level\"]].mean()\n",
    "    tn5_2.rename(columns={\"user_age_level\":\"item_brand_y1_user_age_level_mean\"}, inplace=True)\n",
    "    tn5_2.reset_index(inplace=True)\n",
    "    tn5_tmp = pd.merge(left=item_brand_id, right=tn5_2, how=\"left\", on=\"item_brand_id\")\n",
    "    tn5 = tn5_tmp\n",
    "\n",
    "    tmp_data = item_brand_data_is_trade.loc[item_brand_data_is_trade[\"user_star_level\"] !=-1,:]\n",
    "    tn6_2_group = tmp_data.groupby(\"item_brand_id\")\n",
    "    tn6_2 = tn6_2_group[[\"user_star_level\"]].mean()\n",
    "    tn6_2.rename(columns={\"user_star_level\":\"item_brand_y1_user_star_level_mean\"}, inplace=True)\n",
    "    tn6_2.reset_index(inplace=True)\n",
    "    tn6_tmp = pd.merge(left=item_brand_id, right=tn6_2, how=\"left\", on=\"item_brand_id\")\n",
    "    tn6 = tn6_tmp\n",
    "\n",
    "    tmp_data = item_brand_data_is_trade.loc[item_brand_data_is_trade[\"context_page_id\"] !=-1,:]\n",
    "    tn7_2_group = tmp_data.groupby(\"item_brand_id\")\n",
    "    tn7_2 = tn7_2_group[[\"context_page_id\"]].mean()\n",
    "    tn7_2.rename(columns={\"context_page_id\":\"item_brand_y1_context_page_id_mean\"}, inplace=True)\n",
    "    tn7_2.reset_index(inplace=True)\n",
    "    tn7_tmp = pd.merge(left=item_brand_id, right=tn7_2, how=\"left\", on=\"item_brand_id\")\n",
    "     tn7 = tn7_tmp\n",
    "\n",
    "    tmp_data = item_brand_data.loc[item_brand_data[\"shop_review_num_level\"] !=-1, :]\n",
    "    tn8_1 = tmp_data.groupby(\"item_brand_id\")[[\"shop_review_num_level\"]].mean().reset_index()\n",
    "    tn8_1.rename(columns={\"shop_review_num_level\":\"item_brand_shop_review_num_level_mean\"}, inplace=True)\n",
    "    tmp_data = item_brand_data_is_trade.loc[item_brand_data_is_trade[\"shop_review_num_level\"] !=-1,:]\n",
    "    tn8_2_group = tmp_data.groupby(\"item_brand_id\")\n",
    "    tn8_2 = tn8_2_group[[\"shop_review_num_level\"]].mean()\n",
    "    tn8_2.rename(columns={\"shop_review_num_level\":\"item_brand_y1_shop_review_num_level_mean\"}, inplace=True)\n",
    "    tn8_2.reset_index(inplace=True)\n",
    "    tn8_tmp = pd.merge(left=item_brand_id, right=tn8_1, how=\"left\", on=\"item_brand_id\")\n",
    "    tn8_tmp = pd.merge(left=tn8_tmp, right=tn8_2, how=\"left\", on=\"item_brand_id\")\n",
    "    tn8 = tn8_tmp\n",
    "\n",
    "    tmp_data = item_brand_data.loc[item_brand_data[\"shop_review_positive_rate\"] !=-1, :]\n",
    "    tn9_1 = tmp_data.groupby(\"item_brand_id\")[[\"shop_review_positive_rate\"]].mean().reset_index()\n",
    "    tn9_1.rename(columns={\"shop_review_positive_rate\":\"item_brand_shop_review_positive_rate_mean\"}, inplace=True)\n",
    "    tmp_data = item_brand_data_is_trade.loc[item_brand_data_is_trade[\"shop_review_positive_rate\"] !=-1,:]\n",
    "    tn9_2_group = tmp_data.groupby(\"item_brand_id\")\n",
    "    tn9_2 = tn9_2_group[[\"shop_review_positive_rate\"]].mean()\n",
    "    tn9_2.rename(columns={\"shop_review_positive_rate\":\"item_brand_y1_shop_review_positive_rate_mean\"}, inplace=True)\n",
    "    tn9_2.reset_index(inplace=True)\n",
    "    tn9_tmp = pd.merge(left=item_brand_id, right=tn9_1, how=\"left\", on=\"item_brand_id\")\n",
    "    tn9_tmp = pd.merge(left=tn9_tmp, right=tn9_2, how=\"left\", on=\"item_brand_id\")\n",
    "    tn9 = tn9_tmp\n",
    "\n",
    "    tmp_data = item_brand_data.loc[item_brand_data[\"shop_star_level\"] !=-1, :]\n",
    "    tn10_1 = tmp_data.groupby(\"item_brand_id\")[[\"shop_star_level\"]].mean().reset_index()\n",
    "    tn10_1.rename(columns={\"shop_star_level\":\"item_brand_shop_star_level_mean\"}, inplace=True)\n",
    "    tmp_data = item_brand_data_is_trade.loc[item_brand_data_is_trade[\"shop_star_level\"] !=-1,:]\n",
    "    tn10_2_group = tmp_data.groupby(\"item_brand_id\")\n",
    "    tn10_2 = tn10_2_group[[\"shop_star_level\"]].mean()\n",
    "    tn10_2.rename(columns={\"shop_star_level\":\"item_brand_y1_shop_star_level_mean\"}, inplace=True)\n",
    "    tn10_2.reset_index(inplace=True)\n",
    "    tn10_tmp = pd.merge(left=item_brand_id, right=tn10_1, how=\"left\", on=\"item_brand_id\")\n",
    "    tn10_tmp = pd.merge(left=tn10_tmp, right=tn10_2, how=\"left\", on=\"item_brand_id\")\n",
    "    tn10 = tn10_tmp\n",
    "\n",
    "    tmp_data = item_brand_data.loc[item_brand_data[\"shop_score_delivery\"] !=-1, :]\n",
    "    tn11_1 = tmp_data.groupby(\"item_brand_id\")[[\"shop_score_delivery\"]].mean().reset_index()\n",
    "    tn11_1.rename(columns={\"shop_score_delivery\":\"item_brand_shop_score_delivery_mean\"}, inplace=True)\n",
    "    tmp_data = item_brand_data_is_trade.loc[item_brand_data_is_trade[\"shop_score_delivery\"] !=-1,:]\n",
    "    tn11_2_group = tmp_data.groupby(\"item_brand_id\")\n",
    "    tn11_2 = tn11_2_group[[\"shop_score_delivery\"]].mean()\n",
    "    tn11_2.rename(columns={\"shop_score_delivery\":\"item_brand_y1_shop_score_delivery_mean\"}, inplace=True)\n",
    "    tn11_2.reset_index(inplace=True)\n",
    "    tn11_tmp = pd.merge(left=item_brand_id, right=tn11_1, how=\"left\", on=\"item_brand_id\")\n",
    "    tn11_tmp = pd.merge(left=tn11_tmp, right=tn11_2, how=\"left\", on=\"item_brand_id\")\n",
    "    tn11 = tn11_tmp\n",
    "\n",
    "    tmp_data = item_brand_data.loc[item_brand_data[\"shop_score_description\"] !=-1, :]\n",
    "    tn12_1 = tmp_data.groupby(\"item_brand_id\")[[\"shop_score_description\"]].mean().reset_index()\n",
    "    tn12_1.rename(columns={\"shop_score_description\":\"item_brand_shop_score_description_mean\"}, inplace=True)\n",
    "    tmp_data = item_brand_data_is_trade.loc[item_brand_data_is_trade[\"shop_score_description\"] !=-1,:]\n",
    "    tn12_2_group = tmp_data.groupby(\"item_brand_id\")\n",
    "    tn12_2 = tn12_2_group[[\"shop_score_description\"]].mean()\n",
    "    tn12_2.rename(columns={\"shop_score_description\":\"item_brand_y1_shop_score_description_mean\"}, inplace=True)\n",
    "    tn12_2.reset_index(inplace=True)\n",
    "    tn12_tmp = pd.merge(left=item_brand_id, right=tn12_1, how=\"left\", on=\"item_brand_id\")\n",
    "    tn12_tmp = pd.merge(left=tn12_tmp, right=tn12_2, how=\"left\", on=\"item_brand_id\")\n",
    "    tn12 = tn12_tmp\n",
    "\n",
    "    item_brand_id = item_brand_data.loc[:,[\"item_brand_id\"]].drop_duplicates()\n",
    "\n",
    "    item_brand_feature = tf0.reset_index()\n",
    "    item_brand_feature = pd.merge(left=item_brand_feature, right=tf1, how=\"left\", on=\"item_brand_id\")\n",
    "    item_brand_feature = pd.merge(left=item_brand_feature, right=tf2, how=\"left\", on=\"item_brand_id\")\n",
    "    item_brand_feature = pd.merge(left=item_brand_feature, right=tf3, how=\"left\", on=\"item_brand_id\")\n",
    "    item_brand_feature = pd.merge(left=item_brand_feature, right=tf4, how=\"left\", on=\"item_brand_id\")\n",
    "    item_brand_feature = pd.merge(left=item_brand_feature, right=tf5, how=\"left\", on=\"item_brand_id\")\n",
    "    item_brand_feature = pd.merge(left=item_brand_feature, right=tn1, how=\"left\", on=\"item_brand_id\")\n",
    "    item_brand_feature = pd.merge(left=item_brand_feature, right=tn2, how=\"left\", on=\"item_brand_id\")\n",
    "    item_brand_feature = pd.merge(left=item_brand_feature, right=tn3, how=\"left\", on=\"item_brand_id\")\n",
    "    item_brand_feature = pd.merge(left=item_brand_feature, right=tn4, how=\"left\", on=\"item_brand_id\")\n",
    "    item_brand_feature = pd.merge(left=item_brand_feature, right=tn5, how=\"left\", on=\"item_brand_id\")\n",
    "    item_brand_feature = pd.merge(left=item_brand_feature, right=tn6, how=\"left\", on=\"item_brand_id\")\n",
    "    item_brand_feature = pd.merge(left=item_brand_feature, right=tn7, how=\"left\", on=\"item_brand_id\")\n",
    "    item_brand_feature = pd.merge(left=item_brand_feature, right=tn8, how=\"left\", on=\"item_brand_id\")\n",
    "    item_brand_feature = pd.merge(left=item_brand_feature, right=tn9, how=\"left\", on=\"item_brand_id\")\n",
    "    item_brand_feature = pd.merge(left=item_brand_feature, right=tn10, how=\"left\", on=\"item_brand_id\")\n",
    "    item_brand_feature = pd.merge(left=item_brand_feature, right=tn11, how=\"left\", on=\"item_brand_id\")\n",
    "    item_brand_feature = pd.merge(left=item_brand_feature, right=tn12, how=\"left\", on=\"item_brand_id\")\n",
    "\n",
    "    item_brand_sample_feture = item_brand_data_current_day[[\"instance_id\", \"item_brand_id\"]]\n",
    "    item_brand_feature = pd.merge(item_brand_sample_feture, item_brand_feature, how=\"left\", on=\"item_brand_id\")\n",
    "    item_brand_feature.drop(\"item_brand_id\", inplace=True, axis=1)\n",
    "    return item_brand_feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################################历史转化率：用户\n",
    "def get_user_feature(sample_data, history_data):\n",
    "    user_data_current_day = sample_data\n",
    "    user_data_history_day = history_data\n",
    "    user_data_y1_history_day = user_data_history_day.loc[user_data_history_day.is_trade == 1,:]\n",
    "    user_id_current_day = user_data_current_day.loc[:,[\"user_id\"]].drop_duplicates()\n",
    "###################################用户历史点击次数，历史点击天数\n",
    "    th_1 = user_data_history_day.groupby(\"user_id\")[[\"instance_id\"]].count()\n",
    "    th_1[\"user_day_count\"] = user_data_history_day.groupby(\"user_id\")[\"context_timestamp_day\"].agg(lambda x:x.unique().size)\n",
    "    th_1.reset_index(inplace=True)\n",
    "    th_1.rename(columns={\"instance_id\":\"user_count\"},inplace=True)\n",
    "    \n",
    "    th_2 = user_data_y1_history_day.groupby(\"user_id\")[[\"instance_id\"]].count().reset_index()\n",
    "    th_2.rename(columns={\"instance_id\":\"user_y1_count\"},inplace=True)\n",
    "    t_tmp = pd.merge(th_1, th_2, how=\"left\", on=\"user_id\")\n",
    "    t_tmp[\"user_y1_count_div_user_count\"] = t_tmp[\"user_y1_count\"] / t_tmp[\"user_count\"]\n",
    "    user_id_history_feature = pd.merge(user_id_current_day, t_tmp, how=\"left\", on=\"user_id\")\n",
    "    user_id_history_feature.fillna(0, inplace=True)\n",
    "    \n",
    "#     t1 = user_data_current_day.groupby(\"user_id\")[[\"item_id\"]].count()\n",
    "#     t1.rename(columns={\"item_id\":\"user_count_today\"},inplace=True)\n",
    "#     t1[\"user_item_dist_count_today\"] = user_data_current_day.groupby(\"user_id\")[\"item_id\"].agg(lambda x:x.unique().size)\n",
    "#     t1[\"user_shop_dist_count_doday\"] = user_data_current_day.groupby(\"user_id\")[\"shop_id\"].agg(lambda x:x.unique().size)\n",
    "#     t1.reset_index(inplace=True)\n",
    "#     t1.fillna(0, inplace=True)\n",
    "#     user_feature_tmp = pd.merge(user_id_history_feature, t1, how=\"left\", on=\"user_id\")\n",
    "\n",
    "#     user_data_current_day[\"user_click_rank_today\"] = user_data_current_day.loc[:, [\"user_id\", \"context_timestamp\"]].groupby(\"user_id\")[\"context_timestamp\"].rank(ascending=1,method='min')\n",
    "\n",
    "    user_sample_feture = user_data_current_day[[\"instance_id\", \"user_id\"]]\n",
    "    user_feature = pd.merge(user_sample_feture, user_id_history_feature, how=\"left\", on=\"user_id\")\n",
    "    user_feature.drop(\"user_id\", inplace=True, axis=1)\n",
    "    return user_feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "######################################################历史转化率：商店与广告\n",
    "def get_shopitem_feature(sample_data, history_data):\n",
    "    shopitem_data_current_day = sample_data\n",
    "    shopitem_data = history_data \n",
    "    \n",
    "    shopitem_id = shopitem_data.loc[:,[\"shop_id\", \"item_id\"]].drop_duplicates()\n",
    "    shopitem_data_is_trade = shopitem_data.loc[shopitem_data[\"is_trade\"] == 1, :]\n",
    "    shopitem_group = shopitem_data.groupby([\"shop_id\", \"item_id\"])\n",
    "    shopitem_y1_group = shopitem_data_is_trade.groupby([\"shop_id\", \"item_id\"])\n",
    "\n",
    "    tf0 = shopitem_group[[\"user_id\"]].count()\n",
    "    tf0.rename(columns={\"user_id\":\"shopitem_count\"},inplace=True)\n",
    "    #用户统计\n",
    "    tf1_1 = tf0.copy()\n",
    "    tf1_1[\"shopitem_user_dist_count\"] = shopitem_group[\"user_id\"].agg(lambda x:x.unique().size)\n",
    "    tf1_2 = shopitem_y1_group[[\"user_id\"]].count()\n",
    "    tf1_2.rename(columns={\"user_id\": \"shopitem_y1_user_count\"},inplace=True)\n",
    "    tf1_2[\"shopitem_y1_user_dist_count\"] = shopitem_y1_group[\"user_id\"].agg(lambda x:x.unique().size)\n",
    "    tf1_1.reset_index(inplace=True)\n",
    "    tf1_2.reset_index(inplace=True)\n",
    "    tf1_tmp = pd.merge(left=tf1_1, right=tf1_2, how=\"left\", on=[\"shop_id\", \"item_id\"])\n",
    "    tf1_tmp[\"shopitem_y1_user_count_div_user_count\"] = tf1_tmp[\"shopitem_y1_user_count\"] / tf1_tmp[\"shopitem_count\"]\n",
    "    tf1_tmp[\"shopitem_y1_user_count_div_user_dist_count\"] = tf1_tmp[\"shopitem_y1_user_count\"] / tf1_tmp[\"shopitem_user_dist_count\"]\n",
    "    tf1_tmp[\"shopitem_y1_user_dist_count_div_user_dist_count\"] = tf1_tmp[\"shopitem_y1_user_dist_count\"] / tf1_tmp[\"shopitem_user_dist_count\"]\n",
    "    tf1_tmp.fillna(value=0, inplace=True)\n",
    "    tf1 = tf1_tmp\n",
    "    shopitem_feature = pd.merge(left=shopitem_id, right=tf1, how=\"left\", on=[\"shop_id\", \"item_id\"])\n",
    "    \n",
    "    shopitem_sample_feture = shopitem_data_current_day[[\"instance_id\", \"shop_id\", \"item_id\"]]\n",
    "    shopitem_feature = pd.merge(shopitem_sample_feture, shopitem_feature, how=\"left\", on=[\"shop_id\",\"item_id\"])\n",
    "    shopitem_feature.drop([\"shop_id\",\"item_id\"], inplace=True, axis=1)\n",
    "\n",
    "    return shopitem_feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################################历史转化率：用户与广告\n",
    "def get_useritem_feature(sample_data, history_data):\n",
    "\n",
    "    #useritem_id = train_data_07[[\"user_id\", \"item_id\"]].drop_duplicates()\n",
    "    #useritem_history_data = pd.merge(useritem_id, train_data.loc[train_data[\"context_timestamp_day\"] <7,:], how=\"inner\", on=[\"user_id\", \"item_id\"])\n",
    "    #useritem_data = pd.merge(useritem_id, train_data.loc[train_data[\"context_timestamp_day\"] <7,:], how=\"inner\", on=[\"user_id\", \"item_id\"])\n",
    "    useritem_data_current_day = sample_data\n",
    "    useritem_data = history_data \n",
    "\n",
    "    useritem_id = useritem_data.loc[:,[\"user_id\", \"item_id\"]].drop_duplicates()\n",
    "    useritem_data_is_trade = useritem_data.loc[useritem_data[\"is_trade\"] == 1, :]\n",
    "    useritem_group = useritem_data.groupby([\"user_id\", \"item_id\"])\n",
    "    useritem_y1_group = useritem_data_is_trade.groupby([\"user_id\", \"item_id\"])\n",
    "\n",
    "    tf0 = useritem_group[[\"instance_id\"]].count()\n",
    "    tf0.rename(columns={\"instance_id\":\"useritem_count\"},inplace=True)\n",
    "\n",
    "    tf1_1 = tf0.copy()\n",
    "    tf1_1[\"useritem_shop_dist_count\"] = useritem_group[\"shop_id\"].agg(lambda x:x.unique().size)\n",
    "    tf1_2 = useritem_y1_group[[\"instance_id\"]].count()\n",
    "    tf1_2.rename(columns={\"instance_id\": \"useritem_y1_shop_count\"},inplace=True)\n",
    "    tf1_2[\"useritem_y1_shop_dist_count\"] = useritem_y1_group[\"user_id\"].agg(lambda x:x.unique().size)\n",
    "    tf1_1.reset_index(inplace=True)\n",
    "    tf1_2.reset_index(inplace=True)\n",
    "    tf1_tmp = pd.merge(left=tf1_1, right=tf1_2, how=\"left\", on=[\"user_id\", \"item_id\"])\n",
    "    tf1_tmp[\"useritem_y1_shop_count_div_shop_count\"] = tf1_tmp[\"useritem_y1_shop_count\"] / tf1_tmp[\"useritem_count\"]\n",
    "    tf1_tmp[\"useritem_y1_shop_count_div_shop_dist_count\"] = tf1_tmp[\"useritem_y1_shop_count\"] / tf1_tmp[\"useritem_shop_dist_count\"]\n",
    "    tf1_tmp[\"useritem_y1_shop_dist_count_div_shop_dist_count\"] = tf1_tmp[\"useritem_y1_shop_dist_count\"] / tf1_tmp[\"useritem_shop_dist_count\"]\n",
    "    tf1_tmp.fillna(value=0, inplace=True)\n",
    "    tf1 = tf1_tmp\n",
    "    #tf1 = tf1_tmp.drop(labels=\"useritem_count\", axis=1)\n",
    "    useritem_feature = pd.merge(left=useritem_id, right=tf1, how=\"left\", on=[\"user_id\", \"item_id\"])\n",
    "\n",
    "    useritem_sample_feture = useritem_data_current_day[[\"instance_id\", \"user_id\", \"item_id\"]]\n",
    "    useritem_feature = pd.merge(useritem_sample_feture, useritem_feature, how=\"left\", on=[\"user_id\",\"item_id\"])\n",
    "    useritem_feature.drop([\"user_id\",\"item_id\"], inplace=True, axis=1)\n",
    "    useritem_feature.fillna(value=0, inplace=True)\n",
    "    return useritem_feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################################历史转化率：用户与商店\n",
    "def get_usershop_feature(sample_data, history_data):\n",
    "\n",
    "    #usershop_id = train_data_07[[\"user_id\", \"shop_id\"]].drop_duplicates()\n",
    "    #usershop_history_data = pd.merge(usershop_id, train_data.loc[train_data[\"context_timestamp_day\"] <7,:], how=\"inner\", on=[\"user_id\", \"shop_id\"])\n",
    "    #usershop_data = pd.merge(usershop_id, train_data.loc[train_data[\"context_timestamp_day\"] <7,:], how=\"inner\", on=[\"user_id\", \"shop_id\"])\n",
    "    usershop_data_current_day = sample_data\n",
    "    usershop_data = history_data \n",
    "\n",
    "    usershop_id = usershop_data.loc[:,[\"user_id\", \"shop_id\"]].drop_duplicates()\n",
    "    usershop_data_is_trade = usershop_data.loc[usershop_data[\"is_trade\"] == 1, :]\n",
    "    usershop_group = usershop_data.groupby([\"user_id\", \"shop_id\"])\n",
    "    usershop_y1_group = usershop_data_is_trade.groupby([\"user_id\", \"shop_id\"])\n",
    "\n",
    "    tf0 = usershop_group[[\"instance_id\"]].count()\n",
    "    tf0.rename(columns={\"instance_id\":\"usershop_count\"},inplace=True)\n",
    "\n",
    "    tf1_1 = tf0.copy()\n",
    "    tf1_1[\"usershop_item_dist_count\"] = usershop_group[\"item_id\"].agg(lambda x:x.unique().size)\n",
    "    tf1_2 = usershop_y1_group[[\"instance_id\"]].count()\n",
    "    tf1_2.rename(columns={\"instance_id\": \"usershop_y1_item_count\"},inplace=True)\n",
    "    tf1_2[\"usershop_y1_item_dist_count\"] = usershop_y1_group[\"user_id\"].agg(lambda x:x.unique().size)\n",
    "    tf1_1.reset_index(inplace=True)\n",
    "    tf1_2.reset_index(inplace=True)\n",
    "    tf1_tmp = pd.merge(left=tf1_1, right=tf1_2, how=\"left\", on=[\"user_id\", \"shop_id\"])\n",
    "    tf1_tmp[\"usershop_y1_item_count_div_item_count\"] = tf1_tmp[\"usershop_y1_item_count\"] / tf1_tmp[\"usershop_count\"]\n",
    "    tf1_tmp[\"usershop_y1_item_count_div_item_dist_count\"] = tf1_tmp[\"usershop_y1_item_count\"] / tf1_tmp[\"usershop_item_dist_count\"]\n",
    "    tf1_tmp[\"usershop_y1_item_dist_count_div_item_dist_count\"] = tf1_tmp[\"usershop_y1_item_dist_count\"] / tf1_tmp[\"usershop_item_dist_count\"]\n",
    "    tf1_tmp.fillna(value=0, inplace=True)\n",
    "    tf1 = tf1_tmp\n",
    "    #tf1 = tf1_tmp.drop(labels=\"usershop_count\", axis=1)\n",
    "    usershop_feature = pd.merge(left=usershop_id, right=tf1, how=\"left\", on=[\"user_id\", \"shop_id\"])\n",
    "\n",
    "    usershop_sample_feture = usershop_data_current_day[[\"instance_id\", \"user_id\", \"shop_id\"]]\n",
    "    usershop_feature = pd.merge(usershop_sample_feture, usershop_feature, how=\"left\", on=[\"user_id\",\"shop_id\"])\n",
    "    usershop_feature.drop([\"user_id\",\"shop_id\"], inplace=True, axis=1)\n",
    "    usershop_feature.fillna(value=0, inplace=True)\n",
    "    return usershop_feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################################历史转化率：用户与广告品牌\n",
    "def get_useritem_brand_feature(sample_data, history_data):\n",
    "\n",
    "    #useritem_brand_id = train_data_07[[\"user_id\", \"item_brand_id\"]].drop_duplicates()\n",
    "    #useritem_brand_history_data = pd.merge(useritem_brand_id, train_data.loc[train_data[\"context_timestamp_day\"] <7,:], how=\"inner\", on=[\"user_id\", \"item_brand_id\"])\n",
    "    #useritem_brand_data = pd.merge(useritem_brand_id, train_data.loc[train_data[\"context_timestamp_day\"] <7,:], how=\"inner\", on=[\"user_id\", \"item_brand_id\"])\n",
    "    useritem_brand_data_current_day = sample_data\n",
    "    useritem_brand_data = history_data \n",
    "\n",
    "    useritem_brand_id = useritem_brand_data.loc[:,[\"user_id\", \"item_brand_id\"]].drop_duplicates()\n",
    "    useritem_brand_data_is_trade = useritem_brand_data.loc[useritem_brand_data[\"is_trade\"] == 1, :]\n",
    "    useritem_brand_group = useritem_brand_data.groupby([\"user_id\", \"item_brand_id\"])\n",
    "    useritem_brand_y1_group = useritem_brand_data_is_trade.groupby([\"user_id\", \"item_brand_id\"])\n",
    "\n",
    "    tf0 = useritem_brand_group[[\"instance_id\"]].count()\n",
    "    tf0.rename(columns={\"instance_id\":\"useritem_brand_count\"},inplace=True)\n",
    "\n",
    "    tf1_1 = tf0.copy()\n",
    "    tf1_1[\"useritem_brand_shop_dist_count\"] = useritem_brand_group[\"shop_id\"].agg(lambda x:x.unique().size)\n",
    "    tf1_2 = useritem_brand_y1_group[[\"instance_id\"]].count()\n",
    "    tf1_2.rename(columns={\"instance_id\": \"useritem_brand_y1_shop_count\"},inplace=True)\n",
    "    tf1_2[\"useritem_brand_y1_shop_dist_count\"] = useritem_brand_y1_group[\"user_id\"].agg(lambda x:x.unique().size)\n",
    "    tf1_1.reset_index(inplace=True)\n",
    "    tf1_2.reset_index(inplace=True)\n",
    "    tf1_tmp = pd.merge(left=tf1_1, right=tf1_2, how=\"left\", on=[\"user_id\", \"item_brand_id\"])\n",
    "    tf1_tmp[\"useritem_brand_y1_shop_count_div_shop_count\"] = tf1_tmp[\"useritem_brand_y1_shop_count\"] / tf1_tmp[\"useritem_brand_count\"]\n",
    "    tf1_tmp[\"useritem_brand_y1_shop_count_div_shop_dist_count\"] = tf1_tmp[\"useritem_brand_y1_shop_count\"] / tf1_tmp[\"useritem_brand_shop_dist_count\"]\n",
    "    tf1_tmp[\"useritem_brand_y1_shop_dist_count_div_shop_dist_count\"] = tf1_tmp[\"useritem_brand_y1_shop_dist_count\"] / tf1_tmp[\"useritem_brand_shop_dist_count\"]\n",
    "    tf1_tmp.fillna(value=0, inplace=True)\n",
    "    tf1 = tf1_tmp\n",
    "    #tf1 = tf1_tmp.drop(labels=\"useritem_brand_count\", axis=1)\n",
    "    useritem_brand_feature = pd.merge(left=useritem_brand_id, right=tf1, how=\"left\", on=[\"user_id\", \"item_brand_id\"])\n",
    "\n",
    "    useritem_brand_sample_feture = useritem_brand_data_current_day[[\"instance_id\", \"user_id\", \"item_brand_id\"]]\n",
    "    useritem_brand_feature = pd.merge(useritem_brand_sample_feture, useritem_brand_feature, how=\"left\", on=[\"user_id\",\"item_brand_id\"])\n",
    "    useritem_brand_feature.drop([\"user_id\",\"item_brand_id\"], inplace=True, axis=1)\n",
    "    useritem_brand_feature.fillna(value=0, inplace=True)\n",
    "    return useritem_brand_feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_item_diff_feture(sample_data, history_data):\n",
    "    #去重\n",
    "    #sample_data = train_data_07\n",
    "    item_id = sample_data.loc[:,[\"item_id\"]].drop_duplicates()\n",
    "    #history_data = pd.merge(item_id, train_data.loc[train_data[\"context_timestamp_day\"] <7,:] ,how=\"inner\", on=\"item_id\")\n",
    "    #去除缺失值影响\n",
    "    item_sales_level_drop = history_data.loc[history_data.item_sales_level == -1,:].index\n",
    "    history_data_item_sales_level = history_data.drop(item_sales_level_drop, axis=0)\n",
    "    t1 = history_data_item_sales_level.groupby(\"item_id\")[[\"context_timestamp\"]].agg(lambda x:x.min()).reset_index() \n",
    "    t1 = pd.merge(t1, history_data[[\"item_sales_level\", \"context_timestamp\", \"item_id\"]], how = \"left\", on=[\"item_id\", \"context_timestamp\"])\n",
    "    t1.rename(columns={\"item_sales_level\":\"item_sales_level_history\"},inplace=True)\n",
    "    t1.drop_duplicates(\"item_id\",inplace=True)\n",
    "    t1 = pd.merge(sample_data[[\"instance_id\", \"item_sales_level\",\"item_id\"]], t1, how = \"left\", on=\"item_id\")\n",
    "    t1[\"item_sales_level_diff\"] = t1[\"item_sales_level\"] - t1[\"item_sales_level_history\"]\n",
    "\n",
    "    t2 = history_data.groupby(\"item_id\")[[\"context_timestamp\"]].agg(lambda x:x.min()).reset_index() \n",
    "    t2 = pd.merge(t2, history_data[[\"item_price_level\",\"item_collected_level\",\"item_pv_level\", \"context_timestamp\", \"item_id\"]], how = \"left\", on=[\"item_id\", \"context_timestamp\"])\n",
    "    t2.rename(columns={\"item_price_level\":\"item_price_level_history\",\n",
    "                       \"item_collected_level\":\"item_collected_level_history\",\n",
    "                       \"item_pv_level\":\"item_pv_level_history\"},inplace=True)\n",
    "    t2.drop_duplicates(\"item_id\",inplace=True)\n",
    "    t2 = pd.merge(sample_data[[\"instance_id\", \"item_price_level\",\"item_collected_level\",\"item_pv_level\",\"item_id\"]], t2, how = \"left\", on=\"item_id\")\n",
    "    t2[\"item_price_level_diff\"] = t2[\"item_price_level\"] - t2[\"item_price_level_history\"]\n",
    "    t2[\"item_collected_level_diff\"] = t2[\"item_collected_level\"] - t2[\"item_collected_level_history\"]\n",
    "    t2[\"item_pv_level_diff\"] = t2[\"item_pv_level\"] - t2[\"item_pv_level_history\"]\n",
    "\n",
    "    t1 = t1[[\"instance_id\",\"item_sales_level_diff\"]]\n",
    "    t2 = t2[[\"instance_id\",\"item_price_level_diff\",\"item_collected_level_diff\",\"item_pv_level_diff\"]]\n",
    "    item_diff_feature = pd.merge(t1,t2, how=\"left\", on=\"instance_id\")\n",
    "    item_diff_feature.fillna(value=0, inplace=True)\n",
    "    return item_diff_feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#####################################################样本中用户行为特征提取\n",
    "def get_user_sample_action_feature(input_data):\n",
    "    sample_data = input_data.copy()\n",
    "    user_id = sample_data.loc[:,[\"user_id\"]].drop_duplicates()\n",
    "    sample_id = sample_data[[\"instance_id\", \"user_id\",\"shop_id\", \"item_id\",\"cp_head_2\", \"item_brand_id\"]]\n",
    "    #############################################################用户当前点击的不同 广告，商店，搜索词，广告品牌的个数\n",
    "    t1_group = sample_data.groupby(\"user_id\")\n",
    "    t1 = t1_group[[\"item_id\"]].count()\n",
    "    t1.rename(columns={\"item_id\":\"user_count_today\"},inplace=True)\n",
    "    t1[\"user_item_dist_count_today\"] = t1_group[\"item_id\"].agg(lambda x:x.unique().size)\n",
    "    t1[\"user_shop_dist_count_today\"] = t1_group[\"shop_id\"].agg(lambda x:x.unique().size)\n",
    "    t1[\"user_cp_head_2_dist_count_today\"] = t1_group[\"cp_head_2\"].agg(lambda x:x.unique().size)\n",
    "    t1[\"user_item_brand_dist_count_today\"] = t1_group[\"item_brand_id\"].agg(lambda x:x.unique().size)\n",
    "    t1.reset_index(inplace=True)\n",
    "    #t1.fillna(0, inplace=True)\n",
    "    #############################################################用户当前点击特定广告的次数\n",
    "    t2 = sample_data.groupby([\"user_id\",\"item_id\"])[[\"instance_id\"]].count()\n",
    "    t2.rename(columns={\"instance_id\":\"useritem_count_today\"},inplace=True)\n",
    "    t2.reset_index(inplace=True)               \n",
    "    #############################################################用户当前点击特定商店的次数\n",
    "    t3 = sample_data.groupby([\"user_id\",\"shop_id\"])[[\"instance_id\"]].count()\n",
    "    t3.rename(columns={\"instance_id\":\"usershop_count_today\"},inplace=True)\n",
    "    t3.reset_index(inplace=True)\n",
    "    #############################################################用户当前使用特定搜索词的次数\n",
    "    t4 = sample_data.groupby([\"user_id\",\"cp_head_2\"])[[\"instance_id\"]].count()\n",
    "    t4.rename(columns={\"instance_id\":\"usercp_head_2_count_today\"},inplace=True)\n",
    "    t4.reset_index(inplace=True)\n",
    "    #############################################################用户当前点击特定广告品牌的次数\n",
    "    t5 = sample_data.groupby([\"user_id\",\"item_brand_id\"])[[\"instance_id\"]].count()\n",
    "    t5.rename(columns={\"instance_id\":\"useritem_brand_count_today\"},inplace=True)\n",
    "    t5.reset_index(inplace=True)   \n",
    "    \n",
    "    t10 = sample_id[[\"instance_id\"]]\n",
    "    #sample_data[\"user_click_rank_today\"] = sample_data.loc[:, [\"user_id\", \"context_timestamp\"]].groupby(\"user_id\")[\"context_timestamp\"].rank(ascending=1,method='min')\n",
    "    #sample_data[\"usershop_click_rank_today\"] = sample_data.loc[:, [\"user_id\", \"shop_id\", \"context_timestamp\"]].groupby([\"user_id\", \"shop_id\"])[\"context_timestamp\"].rank(ascending=1,method='min')\n",
    "    #优化sample_data[\"usershop_click_rank_today\"] = sample_data.loc[:, [\"user_id\", \"shop_id\", \"context_timestamp\"]].groupby([\"user_id\", \"shop_id\"])[\"context_timestamp\"].rank(ascending=1,method='min')\n",
    "    ##############################################################用户当前是第几次点击广告\n",
    "    #获取重复点击的用户\n",
    "    tmp_dulicated = sample_data.duplicated(subset=\"user_id\", keep=False)\n",
    "    tmp = sample_data.loc[tmp_dulicated, :]\n",
    "    #仅仅对重复点击的用户进行时间排序并编号\n",
    "    tmp[\"user_click_rank_today\"] = tmp.groupby(\"user_id\")[\"context_timestamp\"].rank(ascending=1,method='min')\n",
    "    t11 = tmp.groupby([\"user_id\",\"context_timestamp\"])[\"instance_id\"].count().reset_index()\n",
    "    t11.rename(columns={\"instance_id\":\"user_click_same_time_count\"},inplace=True)\n",
    "    tmp = pd.merge(tmp, t11, how=\"left\",on=[\"user_id\",\"context_timestamp\"])\n",
    "    t10 = pd.merge(t10, tmp[[\"instance_id\", \"user_click_rank_today\",\"user_click_same_time_count\"]], how=\"left\", on=\"instance_id\")\n",
    "\n",
    "    ############################################################用户当前在特定商店是第几次点击\n",
    "    tmp_dulicated = sample_data.duplicated(subset=[\"user_id\", \"shop_id\"], keep=False)\n",
    "    tmp = sample_data.loc[tmp_dulicated, :]\n",
    "    tmp[\"usershop_click_rank_today\"] = tmp.groupby([\"user_id\", \"shop_id\"])[\"context_timestamp\"].rank(ascending=1,method='min')\n",
    "    #t10 = pd.merge(t10, tmp[[\"instance_id\", \"usershop_click_rank_today\"]], how=\"left\", on=\"instance_id\")\n",
    "    t11 = tmp.groupby([\"user_id\",\"shop_id\",\"context_timestamp\"])[\"instance_id\"].count().reset_index()\n",
    "    t11.rename(columns={\"instance_id\":\"usershop_click_same_time_count\"},inplace=True)\n",
    "    tmp = pd.merge(tmp, t11, how=\"left\",on=[\"user_id\",\"shop_id\",\"context_timestamp\"])\n",
    "    t10 = pd.merge(t10, tmp[[\"instance_id\", \"usershop_click_rank_today\",\"usershop_click_same_time_count\"]], how=\"left\", on=\"instance_id\")\n",
    "\n",
    "    ############################################################用户当前在特定广告是第几次点击\n",
    "    tmp_dulicated = sample_data.duplicated(subset=[\"user_id\", \"item_id\"], keep=False)\n",
    "    tmp = sample_data.loc[tmp_dulicated, :]\n",
    "    tmp[\"useritem_click_rank_today\"] = tmp.groupby([\"user_id\", \"item_id\"])[\"context_timestamp\"].rank(ascending=1,method='min')\n",
    "    #t10 = pd.merge(t10, tmp[[\"instance_id\", \"useritem_click_rank_today\"]], how=\"left\", on=\"instance_id\")\n",
    "    t11 = tmp.groupby([\"user_id\",\"item_id\",\"context_timestamp\"])[\"instance_id\"].count().reset_index()\n",
    "    t11.rename(columns={\"instance_id\":\"useritem_click_same_time_count\"},inplace=True)\n",
    "    tmp = pd.merge(tmp, t11, how=\"left\",on=[\"user_id\",\"item_id\",\"context_timestamp\"])\n",
    "    t10 = pd.merge(t10, tmp[[\"instance_id\", \"useritem_click_rank_today\",\"useritem_click_same_time_count\"]], how=\"left\", on=\"instance_id\")\n",
    "\n",
    "    ############################################################用户当前在特定搜索是第几次点击\n",
    "    tmp_dulicated = sample_data.duplicated(subset=[\"user_id\", \"cp_head_2\"], keep=False)\n",
    "    tmp = sample_data.loc[tmp_dulicated, :]\n",
    "    tmp[\"usercp_head_2_click_rank_today\"] = tmp.groupby([\"user_id\", \"cp_head_2\"])[\"context_timestamp\"].rank(ascending=1,method='min')\n",
    "    #t10 = pd.merge(t10, tmp[[\"instance_id\", \"usercp_head_2_click_rank_today\"]], how=\"left\", on=\"instance_id\")\n",
    "    t11 = tmp.groupby([\"user_id\",\"cp_head_2\",\"context_timestamp\"])[\"instance_id\"].count().reset_index()\n",
    "    t11.rename(columns={\"instance_id\":\"usercp_head_2_click_same_time_count\"},inplace=True)\n",
    "    tmp = pd.merge(tmp, t11, how=\"left\",on=[\"user_id\",\"cp_head_2\",\"context_timestamp\"])\n",
    "    t10 = pd.merge(t10, tmp[[\"instance_id\", \"usercp_head_2_click_rank_today\",\"usercp_head_2_click_same_time_count\"]], how=\"left\", on=\"instance_id\")\n",
    "    \n",
    "    tmp_dulicated = sample_data.duplicated(subset=[\"user_id\", \"item_brand_id\"], keep=False)\n",
    "    tmp = sample_data.loc[tmp_dulicated, :]\n",
    "    tmp[\"useritem_brand_click_rank_today\"] = tmp.groupby([\"user_id\", \"item_brand_id\"])[\"context_timestamp\"].rank(ascending=1,method='min')\n",
    "    #t10 = pd.merge(t10, tmp[[\"instance_id\", \"useritem_brand_click_rank_today\"]], how=\"left\", on=\"instance_id\")\n",
    "    t11 = tmp.groupby([\"user_id\",\"item_brand_id\",\"context_timestamp\"])[\"instance_id\"].count().reset_index()\n",
    "    t11.rename(columns={\"instance_id\":\"useritem_brand_click_same_time_count\"},inplace=True)\n",
    "    tmp = pd.merge(tmp, t11, how=\"left\",on=[\"user_id\",\"item_brand_id\",\"context_timestamp\"])\n",
    "    t10 = pd.merge(t10, tmp[[\"instance_id\", \"useritem_brand_click_rank_today\",\"useritem_brand_click_same_time_count\"]], how=\"left\", on=\"instance_id\")\n",
    "\n",
    "    \n",
    "    t10.fillna(value=1.0,inplace=True)\n",
    "    ############################################################用户当前点击的商店是第几个不同的商店\n",
    "    #按时间对样本进行排序，删除重复的user_id和shop_id\n",
    "\n",
    "    tmp_1 = sample_data.copy()\n",
    "    tmp_1.sort_values(by=[\"context_timestamp\", \"instance_id\"],ascending=[1,1],inplace=True)\n",
    "\n",
    "    tmp = tmp_1.copy()\n",
    "    tmp.drop_duplicates([\"user_id\", \"shop_id\"], keep=\"first\",inplace=True)\n",
    "    #根据时间对user_id进行组内排序并标号，然后与原始的sample_data合并\n",
    "    tmp[\"user_dist_shop_click_rank_today\"] = tmp.groupby(\"user_id\")[\"context_timestamp\"].rank(ascending=1,method='min')\n",
    "    tmp = pd.merge(tmp_1, tmp[[\"instance_id\", \"user_dist_shop_click_rank_today\"]], how=\"left\", on=\"instance_id\")\n",
    "    #对user_id和时间进行增序排序，模拟groupby user_id rank timestamp\n",
    "    tmp.sort_values(by=[\"user_id\",\"context_timestamp\",\"instance_id\"],ascending=[1,1,1],inplace=True)\n",
    "    #生成自然序列索引并放到列中\n",
    "    tmp.reset_index(inplace=True,drop=True)\n",
    "#    tmp.reset_index(inplace=True)\n",
    "    #将合并后的空值是user_id中的重复shop_id,将其序号赋值为上一个索引的值\n",
    "#     def fill_same_shop_rank_with_previous(x):\n",
    "#         if np.isnan(x[\"user_dist_shop_click_rank_today\"]) == True:\n",
    "#             #print(x[\"index\"])\n",
    "#             previous = tmp.loc[x[\"index\"] - 1, \"user_dist_shop_click_rank_today\"]\n",
    "#             tmp.loc[x[\"index\"], \"user_dist_shop_click_rank_today\"] = previous\n",
    "#             return previous\n",
    "#         else:\n",
    "#             return x.user_dist_shop_click_rank_today\n",
    "#     tmp[\"user_dist_shop_click_rank_today\"] = tmp.apply(fill_same_shop_rank_with_previous,axis=1)\n",
    "    tmp_2 = tmp.loc[tmp[\"user_dist_shop_click_rank_today\"].isnull()!=True, [\"user_dist_shop_click_rank_today\"]]\n",
    "    tmp[\"user_dist_shop_click_rank_today\"] = tmp_2[\"user_dist_shop_click_rank_today\"].reindex(index=range(tmp.shape[0]), method=\"ffill\")\n",
    "    t10 = pd.merge(t10, tmp[[\"instance_id\", \"user_dist_shop_click_rank_today\"]], how=\"left\", on=\"instance_id\")\n",
    "\n",
    "    ############################################################用户当前点击的广告是第几个不同的广告\n",
    "    #按时间对样本进行排序，删除重复的user_id和item_id\n",
    "    tmp = tmp_1.copy()\n",
    "    tmp.drop_duplicates([\"user_id\", \"item_id\"], keep=\"first\",inplace=True)\n",
    "    #根据时间对user_id进行组内排序并标号，然后与原始的sample_data合并\n",
    "    tmp[\"user_dist_item_click_rank_today\"] = tmp.groupby(\"user_id\")[\"context_timestamp\"].rank(ascending=1,method='min')\n",
    "    tmp = pd.merge(tmp_1, tmp[[\"instance_id\", \"user_dist_item_click_rank_today\"]], how=\"left\", on=\"instance_id\")\n",
    "    #对user_id和时间进行增序排序，模拟groupby user_id rank timestamp\n",
    "    tmp.sort_values(by=[\"user_id\",\"context_timestamp\",\"instance_id\"],ascending=[1,1,1],inplace=True)\n",
    "    #生成自然序列索引并放到列中\n",
    "    tmp.reset_index(inplace=True,drop=True)\n",
    "    #tmp.reset_index(inplace=True)\n",
    "    #合并后的空值是user_id中的重复item_id,将其序号赋值为上一个索引的值,(同时将原始数据中的当前数据也赋值成上一个索引值，否则连续两个nan会出错)\n",
    "#     def fill_same_item_rank_with_previous(x):\n",
    "#         if np.isnan(x[\"user_dist_item_click_rank_today\"]) == True:\n",
    "#             #print(x[\"index\"])\n",
    "#             previous = tmp.loc[x[\"index\"] - 1, \"user_dist_item_click_rank_today\"]\n",
    "#             tmp.loc[x[\"index\"], \"user_dist_item_click_rank_today\"] = previous\n",
    "#             return previous\n",
    "#         else:\n",
    "#             return x.user_dist_item_click_rank_today\n",
    "#     tmp[\"user_dist_item_click_rank_today\"] = tmp.apply(fill_same_item_rank_with_previous,axis=1)\n",
    "    tmp_2 = tmp.loc[tmp[\"user_dist_item_click_rank_today\"].isnull()!=True, [\"user_dist_item_click_rank_today\"]]\n",
    "    tmp[\"user_dist_item_click_rank_today\"] = tmp_2[\"user_dist_item_click_rank_today\"].reindex(index=range(tmp.shape[0]), method=\"ffill\")\n",
    "    t10 = pd.merge(t10, tmp[[\"instance_id\", \"user_dist_item_click_rank_today\"]], how=\"left\", on=\"instance_id\")\n",
    "\n",
    "    t_tmp = pd.merge(sample_id, t1, how=\"left\", on=\"user_id\")\n",
    "    t_tmp = pd.merge(t_tmp, t2, how=\"left\", on=[\"user_id\", \"item_id\"])\n",
    "    t_tmp = pd.merge(t_tmp, t3, how=\"left\", on=[\"user_id\", \"shop_id\"])\n",
    "    t_tmp = pd.merge(t_tmp, t4, how=\"left\", on=[\"user_id\", \"cp_head_2\"])\n",
    "    t_tmp = pd.merge(t_tmp, t5, how=\"left\", on=[\"user_id\", \"item_brand_id\"])\n",
    "    t_tmp = pd.merge(t_tmp, t10, how=\"left\", on=\"instance_id\")\n",
    "    t_tmp[\"user_click_rank_today\"] = t_tmp[\"user_click_rank_today\"] - 1\n",
    "    t_tmp[\"user_click_next_today_count\"] = t_tmp[\"user_count_today\"] - t_tmp[\"user_click_same_time_count\"] - t_tmp[\"user_click_rank_today\"]\n",
    "    t_tmp[\"usershop_click_rank_today\"] = t_tmp[\"usershop_click_rank_today\"] - 1\n",
    "    t_tmp[\"usershop_click_next_today_count\"] = t_tmp[\"usershop_count_today\"] - t_tmp[\"usershop_click_same_time_count\"] - t_tmp[\"usershop_click_rank_today\"]\n",
    "    t_tmp[\"useritem_click_rank_today\"] = t_tmp[\"useritem_click_rank_today\"] - 1\n",
    "    t_tmp[\"useritem_click_next_today_count\"] = t_tmp[\"useritem_count_today\"] - t_tmp[\"useritem_click_same_time_count\"] - t_tmp[\"useritem_click_rank_today\"]\n",
    "    t_tmp[\"usercp_head_2_click_rank_today\"] = t_tmp[\"usercp_head_2_click_rank_today\"] - 1\n",
    "    t_tmp[\"usercp_head_2_click_next_today_count\"] = t_tmp[\"usercp_head_2_count_today\"] - t_tmp[\"usercp_head_2_click_same_time_count\"] - t_tmp[\"usercp_head_2_click_rank_today\"]\n",
    "    t_tmp[\"useritem_brand_click_rank_today\"] = t_tmp[\"useritem_brand_click_rank_today\"] - 1\n",
    "    t_tmp[\"useritem_brand_click_next_today_count\"] = t_tmp[\"useritem_brand_count_today\"] - t_tmp[\"useritem_brand_click_same_time_count\"] - t_tmp[\"useritem_brand_click_rank_today\"]\n",
    "\n",
    "    t_tmp.drop([\"user_id\",\"shop_id\", \"item_id\",\"cp_head_2\", \"item_brand_id\"], inplace=True, axis=1)\n",
    "    sample_action_feature = t_tmp\n",
    "    return sample_action_feature\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#####################################################样本中用户行为特征提取(用户多次点击时间间隔)\n",
    "def get_user_action_v2_feature(sample_data, history_data):\n",
    "    #去重\n",
    "    user_id = sample_data.loc[:,[\"user_id\"]].drop_duplicates()\n",
    "    half_history_data = pd.merge(user_id, history_data,how=\"inner\", on=\"user_id\")\n",
    "\n",
    "    sample_id = sample_data[[\"instance_id\", \"user_id\",\"shop_id\", \"item_id\",\"cp_head_2\",\"item_brand_id\"]]\n",
    "    #tmp_dulicated = sample_data.duplicated(subset=\"user_id\", keep=False)\n",
    "    tmp_data = pd.concat([sample_data, half_history_data])\n",
    "    #仅仅对重复点击的用户进行时间排序并编号\n",
    "    #######################################用户上一次/下一次点击的时间间隔  \n",
    "    tmp_dulicated = tmp_data.duplicated(subset=\"user_id\", keep=False)\n",
    "    tmp_dulicated_data = tmp_data.loc[tmp_dulicated, :]\n",
    "    tmp_drop_usertime = tmp_dulicated_data.drop_duplicates(subset=[\"user_id\", \"context_timestamp\"])\n",
    "    tmp_drop_usertime.sort_values([\"user_id\", \"context_timestamp\"], ascending=[1,1], inplace=True)\n",
    "    tmp_drop_usertime[\"user_last_click_time_interval\"] = tmp_drop_usertime.groupby(\"user_id\")[\"context_timestamp\"].diff(periods=1)\n",
    "    tmp_drop_usertime[\"user_next_click_time_interval\"] = -tmp_drop_usertime.groupby(\"user_id\")[\"context_timestamp\"].diff(periods=-1)\n",
    "    tmp_dulicated_data = pd.merge(tmp_dulicated_data, tmp_drop_usertime[[\"user_last_click_time_interval\", \"user_next_click_time_interval\",\"user_id\", \"context_timestamp\"]], how=\"left\", on=[\"user_id\", \"context_timestamp\"])\n",
    "    action_v2_feature = pd.merge(sample_id[[\"instance_id\"]], tmp_dulicated_data[[\"instance_id\", \"user_last_click_time_interval\", \"user_next_click_time_interval\"]], how=\"left\", on=\"instance_id\")\n",
    "\n",
    "    #######################################用户上一次/下一次点击特定商店的时间间隔    \n",
    "    usershop_id = sample_data.loc[:,[\"user_id\",\"shop_id\"]].drop_duplicates()\n",
    "    half_history_data = pd.merge(usershop_id, history_data,how=\"inner\", on=[\"user_id\", \"shop_id\"])\n",
    "\n",
    "    #sample_id = sample_data[[\"instance_id\", \"user_id\",\"shop_id\", \"item_id\",\"cp_head_2\"]]\n",
    "    \n",
    "    tmp_data = pd.concat([sample_data, half_history_data])\n",
    "    tmp_dulicated = tmp_data.duplicated(subset=[\"user_id\",\"shop_id\"], keep=False)\n",
    "    tmp_dulicated_data = tmp_data.loc[tmp_dulicated, :]\n",
    "    #仅仅对重复点击的用户进行时间排序并编号\n",
    "    tmp_dulicated_data.sort_values([\"user_id\",\"shop_id\", \"context_timestamp\", \"instance_id\"], ascending=[1,1,1,1], inplace=True)\n",
    "    tmp_dulicated_data[\"usershop_last_click_time_interval\"] = tmp_dulicated_data.groupby([\"user_id\",\"shop_id\"])[\"context_timestamp\"].diff(periods=1)\n",
    "    tmp_dulicated_data[\"usershop_next_click_time_interval\"] = -tmp_dulicated_data.groupby([\"user_id\",\"shop_id\"])[\"context_timestamp\"].diff(periods=-1)\n",
    "    action_v2_feature = pd.merge(action_v2_feature, tmp_dulicated_data[[\"instance_id\", \"usershop_last_click_time_interval\", \"usershop_next_click_time_interval\"]], how=\"left\", on=\"instance_id\")\n",
    "\n",
    "    #######################################用户上一次/下一次点击特定广告的时间间隔\n",
    "    useritem_id = sample_data.loc[:,[\"user_id\",\"item_id\"]].drop_duplicates()\n",
    "    half_history_data = pd.merge(useritem_id, history_data,how=\"inner\", on=[\"user_id\", \"item_id\"])\n",
    "\n",
    "    tmp_data = pd.concat([sample_data, half_history_data])\n",
    "    tmp_dulicated = tmp_data.duplicated(subset=[\"user_id\",\"item_id\"], keep=False)\n",
    "    tmp_dulicated_data = tmp_data.loc[tmp_dulicated, :]\n",
    "\n",
    "    tmp_dulicated_data.sort_values([\"user_id\",\"item_id\", \"context_timestamp\", \"instance_id\"], ascending=[1,1,1,1], inplace=True)\n",
    "    tmp_dulicated_data[\"useritem_last_click_time_interval\"] = tmp_dulicated_data.groupby([\"user_id\",\"item_id\"])[\"context_timestamp\"].diff(periods=1)\n",
    "    tmp_dulicated_data[\"useritem_next_click_time_interval\"] = -tmp_dulicated_data.groupby([\"user_id\",\"item_id\"])[\"context_timestamp\"].diff(periods=-1)\n",
    "    action_v2_feature = pd.merge(action_v2_feature, tmp_dulicated_data[[\"instance_id\", \"useritem_last_click_time_interval\", \"useritem_next_click_time_interval\"]], how=\"left\", on=\"instance_id\")\n",
    "    \n",
    "    #######################################用户上一次/下一次点击特定搜索的时间间隔\n",
    "    usercp_head_2 = sample_data.loc[:,[\"user_id\",\"cp_head_2\"]].drop_duplicates()\n",
    "    half_history_data = pd.merge(usercp_head_2, history_data,how=\"inner\", on=[\"user_id\", \"cp_head_2\"])\n",
    "\n",
    "    tmp_data = pd.concat([sample_data, half_history_data])\n",
    "    tmp_dulicated = tmp_data.duplicated(subset=[\"user_id\",\"cp_head_2\"], keep=False)\n",
    "    tmp_dulicated_data = tmp_data.loc[tmp_dulicated, :]\n",
    "\n",
    "    tmp_dulicated_data.sort_values([\"user_id\",\"cp_head_2\", \"context_timestamp\", \"instance_id\"], ascending=[1,1,1,1], inplace=True)\n",
    "    tmp_dulicated_data[\"usercp_head_2_last_click_time_interval\"] = tmp_dulicated_data.groupby([\"user_id\",\"cp_head_2\"])[\"context_timestamp\"].diff(periods=1)\n",
    "    tmp_dulicated_data[\"usercp_head_2_next_click_time_interval\"] = -tmp_dulicated_data.groupby([\"user_id\",\"cp_head_2\"])[\"context_timestamp\"].diff(periods=-1)\n",
    "    action_v2_feature = pd.merge(action_v2_feature, tmp_dulicated_data[[\"instance_id\", \"usercp_head_2_last_click_time_interval\", \"usercp_head_2_next_click_time_interval\"]], how=\"left\", on=\"instance_id\")\n",
    "\n",
    "    #######################################用户上一次/下一次点击特定广告品牌的时间间隔\n",
    "    useritem_brand_id = sample_data.loc[:,[\"user_id\",\"item_brand_id\"]].drop_duplicates()\n",
    "    half_history_data = pd.merge(useritem_brand_id, history_data,how=\"inner\", on=[\"user_id\", \"item_brand_id\"])\n",
    "\n",
    "    tmp_data = pd.concat([sample_data, half_history_data])\n",
    "    tmp_dulicated = tmp_data.duplicated(subset=[\"user_id\",\"item_brand_id\"], keep=False)\n",
    "    tmp_dulicated_data = tmp_data.loc[tmp_dulicated, :]\n",
    "\n",
    "    tmp_dulicated_data.sort_values([\"user_id\",\"item_brand_id\", \"context_timestamp\", \"instance_id\"], ascending=[1,1,1,1], inplace=True)\n",
    "    tmp_dulicated_data[\"useritem_brand_last_click_time_interval\"] = tmp_dulicated_data.groupby([\"user_id\",\"item_brand_id\"])[\"context_timestamp\"].diff(periods=1)\n",
    "    tmp_dulicated_data[\"useritem_brand_next_click_time_interval\"] = -tmp_dulicated_data.groupby([\"user_id\",\"item_brand_id\"])[\"context_timestamp\"].diff(periods=-1)\n",
    "    action_v2_feature = pd.merge(action_v2_feature, tmp_dulicated_data[[\"instance_id\", \"useritem_brand_last_click_time_interval\", \"useritem_brand_next_click_time_interval\"]], how=\"left\", on=\"instance_id\")\n",
    "    \n",
    "    \n",
    "    #time_half_day = 43200\n",
    "    time_half_day = 86400\n",
    "    action_v2_feature.fillna(value=time_half_day,inplace=True)\n",
    "    action_v2_feature.loc[action_v2_feature[\"user_last_click_time_interval\"]>time_half_day, \"user_last_click_time_interval\"] = time_half_day\n",
    "    action_v2_feature.loc[action_v2_feature[\"user_next_click_time_interval\"]>time_half_day, \"user_next_click_time_interval\"] = time_half_day\n",
    "    action_v2_feature.loc[action_v2_feature[\"usershop_last_click_time_interval\"]>time_half_day, \"usershop_last_click_time_interval\"] = time_half_day\n",
    "    action_v2_feature.loc[action_v2_feature[\"usershop_next_click_time_interval\"]>time_half_day, \"usershop_next_click_time_interval\"] = time_half_day\n",
    "    action_v2_feature.loc[action_v2_feature[\"useritem_last_click_time_interval\"]>time_half_day, \"useritem_last_click_time_interval\"] = time_half_day\n",
    "    action_v2_feature.loc[action_v2_feature[\"useritem_next_click_time_interval\"]>time_half_day, \"useritem_next_click_time_interval\"] = time_half_day\n",
    "    action_v2_feature.loc[action_v2_feature[\"usercp_head_2_last_click_time_interval\"]>time_half_day, \"usercp_head_2_last_click_time_interval\"] = time_half_day\n",
    "    action_v2_feature.loc[action_v2_feature[\"usercp_head_2_next_click_time_interval\"]>time_half_day, \"usercp_head_2_next_click_time_interval\"] = time_half_day\n",
    "    action_v2_feature.loc[action_v2_feature[\"useritem_brand_last_click_time_interval\"]>time_half_day, \"useritem_brand_last_click_time_interval\"] = time_half_day\n",
    "    action_v2_feature.loc[action_v2_feature[\"useritem_brand_next_click_time_interval\"]>time_half_day, \"useritem_brand_next_click_time_interval\"] = time_half_day\n",
    "\n",
    "    return action_v2_feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_user_action_v3_feature(sample_data, history_data):\n",
    "    time_1_hours = 3600\n",
    "    user_id = sample_data.loc[:,[\"user_id\"]].drop_duplicates()\n",
    "    #获取需要的历史用户\n",
    "    history_data_used = pd.merge(user_id, history_data, how=\"inner\", on=\"user_id\")\n",
    "\n",
    "    sample_id = sample_data[[\"instance_id\", \"user_id\",\"shop_id\", \"item_id\",\"cp_head_2\"]]\n",
    "    #合并当前样本和历史数据\n",
    "    tmp_data = pd.concat([sample_data, history_data_used])\n",
    "    #获取重复出现的用户\n",
    "    tmp_dulicated = tmp_data.duplicated(subset=\"user_id\", keep=False)\n",
    "    tmp_dulicated_data = tmp_data.loc[tmp_dulicated, :]\n",
    "    ####################################################用户在当前样本时间之前的两小时内点击次数\n",
    "    tmp_dulicated_data.sort_values([\"user_id\", \"context_timestamp\", \"instance_id\"], ascending=[1,1,1], inplace=True)\n",
    "    def get_time_before_count_in_group(x):\n",
    "        global tmp_group\n",
    "        c = len(np.where((tmp_group.context_timestamp >(x-time_1_hours)) & (tmp_group.context_timestamp <x))[0])\n",
    "        return c\n",
    "    def get_time_before_count(x):\n",
    "        #print(x)\n",
    "        global tmp_group\n",
    "        tmp_group = x[[\"context_timestamp\"]]\n",
    "        #在tmp_group上再创建一列消耗约5秒的时间，因此在外面reset_index即可\n",
    "        #tmp_group[\"user_click_count_in_last_2_hours\"] = tmp_group.context_timestamp.apply(get_time_before_count)\n",
    "        #return tmp_group[[\"user_click_count_in_last_2_hours\"]]\n",
    "        return tmp_group.context_timestamp.apply(get_time_before_count_in_group)\n",
    "\n",
    "    tmp_out = tmp_dulicated_data.groupby(\"user_id\").apply(get_time_before_count)\n",
    "    tmp_out = tmp_out.reset_index()\n",
    "    tmp_out.rename(columns={\"context_timestamp\":\"user_click_count_in_last_2_hours\"}, inplace=True)\n",
    "    tmp_out.set_index(\"level_1\",inplace=True)\n",
    "    tmp_dulicated_data[\"user_click_count_in_last_2_hours\"] = tmp_out[\"user_click_count_in_last_2_hours\"]\n",
    "    action_v3_feature = pd.merge(sample_id[[\"instance_id\"]], tmp_dulicated_data[[\"instance_id\", \"user_click_count_in_last_2_hours\"]], how=\"left\", on=\"instance_id\")\n",
    "    print(\"last ok\")\n",
    "    ####################################################用户在当前样本时间之后的两小时内点击次数\n",
    "    def get_time_later_count_in_group(x):\n",
    "        global tmp_group\n",
    "        c = len(np.where((tmp_group.context_timestamp <(x+time_1_hours)) & (tmp_group.context_timestamp >x))[0])\n",
    "        return c\n",
    "    def get_time_later_count(x):\n",
    "        #print(x)\n",
    "        global tmp_group\n",
    "        tmp_group = x[[\"context_timestamp\"]]\n",
    "        #在tmp_group上再创建一列消耗约5秒的时间，因此在外面reset_index即可\n",
    "        #tmp_group[\"user_click_count_in_last_2_hours\"] = tmp_group.context_timestamp.apply(get_time_before_count)\n",
    "        #return tmp_group[[\"user_click_count_in_last_2_hours\"]]\n",
    "        return tmp_group.context_timestamp.apply(get_time_later_count_in_group)\n",
    "\n",
    "    tmp_dulicated = sample_data.duplicated(subset=\"user_id\", keep=False)\n",
    "    tmp_dulicated_data = sample_data.loc[tmp_dulicated, :]\n",
    "    tmp_dulicated_data.sort_values([\"user_id\", \"context_timestamp\", \"instance_id\"], ascending=[1,1,1], inplace=True)\n",
    "\n",
    "    tmp_out = tmp_dulicated_data.groupby(\"user_id\").apply(get_time_later_count)\n",
    "    tmp_out = tmp_out.reset_index()\n",
    "    tmp_out.rename(columns={\"context_timestamp\":\"user_click_count_in_later_2_hours\"}, inplace=True)\n",
    "    tmp_out.set_index(\"level_1\",inplace=True)\n",
    "    tmp_dulicated_data[\"user_click_count_in_later_2_hours\"] = tmp_out[\"user_click_count_in_later_2_hours\"]\n",
    "    print(\"later ok\")\n",
    "\n",
    "\n",
    "    action_v3_feature = pd.merge(action_v3_feature, tmp_dulicated_data[[\"instance_id\",\"user_click_count_in_later_2_hours\"]], how=\"left\", on=\"instance_id\")\n",
    "    action_v3_feature.fillna(value=0,inplace=True)\n",
    "    return action_v3_feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "############################################唯一个数和缺失个数\n",
    "def get_summary_data(data, miss_symbol):\n",
    "    input_data = data.replace(miss_symbol, np.NaN)\n",
    "    distinct_count = input_data.apply(lambda x: x.unique().size)\n",
    "    dismiss_count = input_data.apply(lambda x: x.count())\n",
    "    distinct_count.rename(\"distinct_count\", inplace=True)\n",
    "    dismiss_count.rename(\"dismiss_count\", inplace=True)\n",
    "    summary_data = pd.concat([distinct_count, dismiss_count],axis=1)\n",
    "    summary_data.insert(loc=0, column=\"row_index\",value=range(summary_data.index.size))\n",
    "    summary_data[\"miss_count\"] = input_data.index.size - summary_data.dismiss_count\n",
    "    summary_data[\"variable_type\"] = input_data.dtypes\n",
    "    summary_data[\"min_value\"] = input_data.min()\n",
    "    summary_data[\"max_value\"] = input_data.max()\n",
    "    #summary_data[\"row_index\"] = range(summary_data.index.size)\n",
    "    #不能先重建索引，否则后面就无法根据索引新增数据了\n",
    "    #summary_data.reset_index(inplace=True)\n",
    "    #summary_data.rename(columns={\"index\":\"column_name\"}, inplace=True)\n",
    "    return summary_data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
