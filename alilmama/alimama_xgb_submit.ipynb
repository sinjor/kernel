{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \"\"\"\n",
      "E:\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttrain-logloss:0.605053\n",
      "[1]\ttrain-logloss:0.532955\n",
      "[2]\ttrain-logloss:0.472973\n",
      "[3]\ttrain-logloss:0.422419\n",
      "[4]\ttrain-logloss:0.379397\n",
      "[5]\ttrain-logloss:0.342513\n",
      "[6]\ttrain-logloss:0.310683\n",
      "[7]\ttrain-logloss:0.283088\n",
      "[8]\ttrain-logloss:0.259096\n",
      "[9]\ttrain-logloss:0.238125\n",
      "[10]\ttrain-logloss:0.219814\n",
      "[11]\ttrain-logloss:0.203755\n",
      "[12]\ttrain-logloss:0.189656\n",
      "[13]\ttrain-logloss:0.177226\n",
      "[14]\ttrain-logloss:0.166292\n",
      "[15]\ttrain-logloss:0.156664\n",
      "[16]\ttrain-logloss:0.148191\n",
      "[17]\ttrain-logloss:0.140722\n",
      "[18]\ttrain-logloss:0.134154\n",
      "[19]\ttrain-logloss:0.128381\n",
      "[20]\ttrain-logloss:0.123267\n",
      "[21]\ttrain-logloss:0.118773\n",
      "[22]\ttrain-logloss:0.114811\n",
      "[23]\ttrain-logloss:0.111344\n",
      "[24]\ttrain-logloss:0.108298\n",
      "[25]\ttrain-logloss:0.10564\n",
      "[26]\ttrain-logloss:0.103304\n",
      "[27]\ttrain-logloss:0.10126\n",
      "[28]\ttrain-logloss:0.099464\n",
      "[29]\ttrain-logloss:0.0979\n",
      "[30]\ttrain-logloss:0.096526\n",
      "[31]\ttrain-logloss:0.095339\n",
      "[32]\ttrain-logloss:0.094305\n",
      "[33]\ttrain-logloss:0.093414\n",
      "[34]\ttrain-logloss:0.092634\n",
      "[35]\ttrain-logloss:0.09195\n",
      "[36]\ttrain-logloss:0.091357\n",
      "[37]\ttrain-logloss:0.090831\n",
      "[38]\ttrain-logloss:0.090368\n",
      "[39]\ttrain-logloss:0.089956\n",
      "[40]\ttrain-logloss:0.089596\n",
      "[41]\ttrain-logloss:0.08929\n",
      "[42]\ttrain-logloss:0.08902\n",
      "[43]\ttrain-logloss:0.088781\n",
      "[44]\ttrain-logloss:0.08856\n",
      "[45]\ttrain-logloss:0.08836\n",
      "[46]\ttrain-logloss:0.088182\n",
      "[47]\ttrain-logloss:0.088009\n",
      "[48]\ttrain-logloss:0.087849\n",
      "[49]\ttrain-logloss:0.087708\n",
      "[50]\ttrain-logloss:0.087586\n",
      "[51]\ttrain-logloss:0.087481\n",
      "[52]\ttrain-logloss:0.087361\n",
      "[53]\ttrain-logloss:0.087245\n",
      "[54]\ttrain-logloss:0.087144\n",
      "[55]\ttrain-logloss:0.087039\n",
      "[56]\ttrain-logloss:0.086943\n",
      "[57]\ttrain-logloss:0.086862\n",
      "[58]\ttrain-logloss:0.086785\n",
      "[59]\ttrain-logloss:0.086721\n",
      "[60]\ttrain-logloss:0.086652\n",
      "[61]\ttrain-logloss:0.08658\n",
      "[62]\ttrain-logloss:0.086524\n",
      "[63]\ttrain-logloss:0.086431\n",
      "[64]\ttrain-logloss:0.086345\n",
      "[65]\ttrain-logloss:0.086268\n",
      "[66]\ttrain-logloss:0.086229\n",
      "[67]\ttrain-logloss:0.086164\n",
      "[68]\ttrain-logloss:0.086112\n",
      "[69]\ttrain-logloss:0.086036\n",
      "[70]\ttrain-logloss:0.085982\n",
      "[71]\ttrain-logloss:0.085912\n",
      "[72]\ttrain-logloss:0.085854\n",
      "[73]\ttrain-logloss:0.085803\n",
      "[74]\ttrain-logloss:0.085761\n",
      "[75]\ttrain-logloss:0.085703\n",
      "[76]\ttrain-logloss:0.085669\n",
      "[77]\ttrain-logloss:0.085629\n",
      "[78]\ttrain-logloss:0.085591\n",
      "[79]\ttrain-logloss:0.085548\n",
      "[80]\ttrain-logloss:0.085501\n",
      "[81]\ttrain-logloss:0.085426\n",
      "[82]\ttrain-logloss:0.085378\n",
      "[83]\ttrain-logloss:0.085354\n",
      "[84]\ttrain-logloss:0.085318\n",
      "[85]\ttrain-logloss:0.08526\n",
      "[86]\ttrain-logloss:0.08519\n",
      "[87]\ttrain-logloss:0.085159\n",
      "[88]\ttrain-logloss:0.085106\n",
      "[89]\ttrain-logloss:0.085066\n",
      "logloss_prob: 0.0822071254078881\n",
      "xgb build success\n"
     ]
    }
   ],
   "source": [
    "params={'booster':'gbtree',\n",
    "    'eta': 0.1,\n",
    "    'n_estimators':103,\n",
    "    'max_depth':5, #3 10\n",
    "    'min_child_weight':3,#1 6\n",
    "    'gamma':0.3,\n",
    "    'subsample':0.8,\n",
    "    'colsample_bytree':0.8,\n",
    "    'objective': 'binary:logistic',\n",
    "    'nthread':8,\n",
    "    'scale_pos_weight':1,\n",
    "    'eval_metric': 'logloss',\n",
    "    'lambda':0,\n",
    "    'seed':0,\n",
    "    'silent':0}\n",
    "xgb_bst2 = xgb_build(params, train_feature_data, num_boost_round=90)\n",
    "#do_xgb_cv(params=params, train_feature_data=train_feature_data)\n",
    "#线上预测\n",
    "# test_feature_data = get_feature(test_data, data_type=\"test\", csv_read=True, csv_write=False)\n",
    "# test_feature_data = pd.concat([test_data[[\"shop_id\"]],test_feature_data], axis=1)\n",
    "\n",
    "# train_data_23_24 = train_data.loc[(train_data[\"context_timestamp_day\"] >= 23) & (train_data[\"context_timestamp_day\"] < 25), :]\n",
    "# shop_feature_25 = get_shop_feature(train_data_23_24)\n",
    "# test_feature_data = pd.merge(left=test_feature_data, right=shop_feature_25, how=\"left\", on=\"shop_id\")\n",
    "\n",
    "# test_feature_data = test_feature_data.iloc[:, 1:]\n",
    "\n",
    "# #重新建模预测\n",
    "# online_feature = train_feature_data.ix[train_feature_data[\"context_timestamp_day\"] > 19, feature_begin:]\n",
    "# online_label = train_feature_data[\"is_trade\"]\n",
    "# online_rebuild_pred(online_feature,online_label,test_feature_data, params=params, num_boost_round=90, result_file=\"result_v31_0414.csv\")\n",
    "\n",
    "\n",
    "#使用训练模型进行预测\n",
    "#online_pred(xgb_bst, test_feature_data, result_file=\"result_test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('shop_score_delivery', 119),\n",
       " ('shop_user_star_level_mean', 110),\n",
       " ('item_sales_level', 107),\n",
       " ('shop_context_page_id_mean', 101),\n",
       " ('shop_item_price_level_mean', 99),\n",
       " ('user_star_level', 99),\n",
       " ('user_age_level', 93),\n",
       " ('shop_score_description', 78),\n",
       " ('shop_user_age_level_mean', 77),\n",
       " ('shop_score_service', 73),\n",
       " ('category_index_div_pred_num', 72),\n",
       " ('shop_item_sales_level_mean', 61),\n",
       " ('shop_y1_user_count_div_user_dist_count', 58),\n",
       " ('shop_review_positive_rate', 56),\n",
       " ('shop_y1_item_count_div_item_count', 50),\n",
       " ('shop_y1_predict_cp_dist_count_div_predict_cp_dist_count', 49),\n",
       " ('shop_y1_predict_cp_count_div_predict_cp_dist_count', 44),\n",
       " ('pred_category_which_property_is_-1_num_div_pred_category_num', 43),\n",
       " ('shop_item_collected_level_mean', 41),\n",
       " ('item_price_level', 40),\n",
       " ('shop_y1_user_dist_count_div_user_dist_count', 38),\n",
       " ('item_category_list_7908382889764677758;7258015885215914736', 38),\n",
       " ('shop_count', 37),\n",
       " ('pred_category_which_property_is_-1_num', 34),\n",
       " ('shop_predict_cp_dist_count', 33),\n",
       " ('shop_y1_item_sales_level_mean', 32),\n",
       " ('shop_y1_item_count_div_item_dist_count', 32),\n",
       " ('pred_pro_intersection_item_pro_num_div_item_property_num', 31),\n",
       " ('user_gender_id_1', 30),\n",
       " ('shop_y1_user_star_level_mean', 29),\n",
       " ('shop_item_dist_count', 28),\n",
       " ('shop_item_pv_level_mean', 27),\n",
       " ('shop_user_dist_count', 27),\n",
       " ('item_collected_level', 27),\n",
       " ('item_pv_level', 26),\n",
       " ('category_index_in_pred_category_list', 25),\n",
       " ('pred_category_which_has_pro_num_div_pred_category_num', 25),\n",
       " ('shop_y1_user_age_level_mean', 24),\n",
       " ('shop_y1_user_star_level_min', 20),\n",
       " ('shop_y1_item_collected_level_mean', 18),\n",
       " ('shop_review_num_level', 18),\n",
       " ('shop_y1_item_pv_level_mean', 16),\n",
       " ('item_category_list_7908382889764677758;2642175453151805566;8868887661186419229',\n",
       "  15),\n",
       " ('shop_y1_item_dist_count_div_item_dist_count', 15),\n",
       " ('item_category_list_7908382889764677758;8277336076276184272', 15),\n",
       " ('shop_y1_item_price_level_mean', 14),\n",
       " ('shop_y1_user_count_div_user_count', 14),\n",
       " ('shop_y1_item_sales_level_min', 13),\n",
       " ('shop_y1_context_page_id_max', 13),\n",
       " ('user_occupation_id_2005', 13),\n",
       " ('shop_y1_user_star_level_max', 12),\n",
       " ('pred_pro_intersection_item_pro_num_div_pred_property_num', 11),\n",
       " ('user_occupation_id_2002', 11),\n",
       " ('shop_star_level', 11),\n",
       " ('item_category_list_7908382889764677758;2436715285093487584', 10),\n",
       " ('shop_y1_item_sales_level_max', 9),\n",
       " ('shop_y1_user_age_level_max', 9),\n",
       " ('item_category_list_7908382889764677758;2011981573061447208', 8),\n",
       " ('shop_y1_item_brand_count_div_item_brand_dist_count', 8),\n",
       " ('shop_y1_context_page_id_mean', 8),\n",
       " ('shop_y1_item_collected_level_max', 8),\n",
       " ('item_category_list_7908382889764677758;4879721024980945592', 7),\n",
       " ('shop_y1_user_age_level_min', 6),\n",
       " ('shop_y1_item_pv_level_min', 6),\n",
       " ('item_category_list_7908382889764677758;509660095530134768', 6),\n",
       " ('item_category_list_7908382889764677758;8710739180200009128', 5),\n",
       " ('pred_pro_intersection_item_pro_num', 5),\n",
       " ('pred_category_which_has_pro_num', 5),\n",
       " ('shop_y1_item_price_level_max', 4),\n",
       " ('user_gender_id_2', 4),\n",
       " ('shop_y1_item_brand_count_div_item_brand_count', 4),\n",
       " ('shop_y1_item_count', 4),\n",
       " ('item_category_list_7908382889764677758;3203673979138763595', 4),\n",
       " ('shop_y1_context_page_id_min', 4),\n",
       " ('user_gender_id_0', 3),\n",
       " ('shop_y1_item_price_level_min', 3),\n",
       " ('shop_y1_item_dist_count', 3),\n",
       " ('shop_item_brand_dist_count', 3),\n",
       " ('shop_y1_item_brand_dist_count', 3),\n",
       " ('shop_y1_user_count', 3),\n",
       " ('user_gender_id_-1', 3),\n",
       " ('user_occupation_id_2004', 2),\n",
       " ('item_category_list_7908382889764677758;5755694407684602296', 2),\n",
       " ('shop_y1_predict_cp_dist_count', 2),\n",
       " ('shop_y1_item_city_dist_count', 2),\n",
       " ('shop_y1_user_dist_count', 1),\n",
       " ('shop_y1_item_collected_level_min', 1),\n",
       " ('item_category_list_7908382889764677758;5799347067982556520', 1),\n",
       " ('shop_item_city_dist_count', 1)]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "feature_score = xgb_bst2.get_fscore()\n",
    "feature_score = sorted(feature_score.items(), key=lambda x:x[1],reverse=True)\n",
    "display(feature_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_feature_data = get_feature(train_data, data_type=\"train\", csv_read=True, csv_write=False)\n",
    "train_feature_data = pd.concat([train_data[[timestamp,\"is_trade\", \"shop_id\", \"context_timestamp_day\"]],train_feature_data], axis=1)\n",
    "\n",
    "train_data_18_19 = train_data.loc[(train_data[\"context_timestamp_day\"] >= 18) & (train_data[\"context_timestamp_day\"] < 20), :]\n",
    "train_data_19_20 = train_data.loc[(train_data[\"context_timestamp_day\"] >= 19) & (train_data[\"context_timestamp_day\"] < 21), :]\n",
    "train_data_20_21 = train_data.loc[(train_data[\"context_timestamp_day\"] >= 20) & (train_data[\"context_timestamp_day\"] < 22), :]\n",
    "train_data_21_22 = train_data.loc[(train_data[\"context_timestamp_day\"] >= 21) & (train_data[\"context_timestamp_day\"] < 23), :]\n",
    "train_data_22_23 = train_data.loc[(train_data[\"context_timestamp_day\"] >= 22) & (train_data[\"context_timestamp_day\"] < 24), :]\n",
    "shop_feature_20 = get_shop_feature(train_data_18_19)\n",
    "shop_feature_21 = get_shop_feature(train_data_19_20)\n",
    "shop_feature_22 = get_shop_feature(train_data_20_21)\n",
    "shop_feature_23 = get_shop_feature(train_data_21_22)\n",
    "shop_feature_24 = get_shop_feature(train_data_22_23)\n",
    "train_feature_data_20 = pd.merge(left=train_feature_data.loc[train_feature_data[\"context_timestamp_day\"] == 20,:], right=shop_feature_20, how=\"left\", on=\"shop_id\")\n",
    "train_feature_data_21 = pd.merge(left=train_feature_data.loc[train_feature_data[\"context_timestamp_day\"] == 21,:], right=shop_feature_21, how=\"left\", on=\"shop_id\")\n",
    "train_feature_data_22 = pd.merge(left=train_feature_data.loc[train_feature_data[\"context_timestamp_day\"] == 22,:], right=shop_feature_22, how=\"left\", on=\"shop_id\")\n",
    "train_feature_data_23 = pd.merge(left=train_feature_data.loc[train_feature_data[\"context_timestamp_day\"] == 23,:], right=shop_feature_23, how=\"left\", on=\"shop_id\")\n",
    "train_feature_data_24 = pd.merge(left=train_feature_data.loc[train_feature_data[\"context_timestamp_day\"] == 24,:], right=shop_feature_24, how=\"left\", on=\"shop_id\")\n",
    "train_feature_data= pd.concat([train_feature_data_20, train_feature_data_21, train_feature_data_22, train_feature_data_23, train_feature_data_24])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn import metrics\n",
    "import xgboost as xgb\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "#import time\n",
    "import datetime\n",
    "\n",
    "test_data = pd.read_csv(\"G:/program_monkey/ML/tianchi/alimama_ad/test.txt\",sep=\" \")\n",
    "train_data = pd.read_csv(\"G:/program_monkey/ML/tianchi/alimama_ad/train.txt\",sep=\" \")\n",
    "factor_data = [\"item_category_list\",\"user_gender_id\",\"user_occupation_id\"]\n",
    "numeric_data =[\"item_price_level\",\"item_sales_level\",\"item_collected_level\",\n",
    "               \"item_pv_level\",\"user_age_level\",\"user_star_level\",\"shop_review_num_level\",\n",
    "               \"shop_review_positive_rate\",\"shop_star_level\",\"shop_score_service\",\n",
    "               \"shop_score_delivery\",\"shop_score_description\"]\n",
    "timestamp = \"context_timestamp\"\n",
    "#train_data[[timestamp,\"is_trade\"]]\n",
    "#feature_begin = 4\n",
    "\n",
    "train_data[\"context_timestamp_day\"] = train_data[\"context_timestamp\"].apply(lambda x:datetime.datetime.fromtimestamp(x).day)\n",
    "train_feature_drop = [\"context_timestamp\",\"is_trade\", \"shop_id\", \"context_timestamp_day\"]\n",
    "#last_day_begin_time = \"2018-09-24 00:00:00\"\n",
    "time_array = datetime.datetime.strptime(\"2018-09-24 00:00:00\", \"%Y-%m-%d %H:%M:%S\")\n",
    "last_day_timestamp = int(time_array.timestamp())\n",
    "#second_day_begin_time = \"2018-09-19 00:00:00\"\n",
    "time_array = datetime.datetime.strptime(\"2018-09-19 00:00:00\", \"%Y-%m-%d %H:%M:%S\")\n",
    "second_day_timestamp = int(time_array.timestamp())\n",
    "\n",
    "#获取样本自带组合特征\n",
    "train_feature_data = get_feature(train_data, data_type=\"train\", csv_read=True, csv_write=False)\n",
    "train_feature_data = pd.concat([train_data[train_feature_drop],train_feature_data], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " 1.函数接口（建模，提特征等） 2.历史结果记录    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def xgb_build(params, train_feature_data, num_boost_round):\n",
    "    #前6天作为训练集，最后一天作为验证集\n",
    "    train_x = train_feature_data.ix[train_feature_data[timestamp] < last_day_timestamp, :]\n",
    "    test_x = train_feature_data.ix[train_feature_data[timestamp] >= last_day_timestamp, :]\n",
    "    train_x.drop(train_feature_drop, inplace=True, axis=1)\n",
    "    test_x.drop(train_feature_drop, inplace=True, axis=1)\n",
    "    train_y = train_feature_data.loc[train_feature_data[timestamp] < last_day_timestamp, [\"is_trade\"]]\n",
    "    test_y = train_feature_data.loc[train_feature_data[timestamp] >= last_day_timestamp, [\"is_trade\"]]\n",
    "\n",
    "    #生成xgboost可以识别的训练数据和验证数据\n",
    "    dtrain = xgb.DMatrix(data=train_x, label=train_y)\n",
    "    dtest = xgb.DMatrix(data=test_x)\n",
    "\n",
    "    watchlist = [(dtrain,'train')]\n",
    "    bst = xgb.train(params=params, dtrain=dtrain, num_boost_round=num_boost_round, evals=watchlist)\n",
    "    test_pred = bst.predict(data=dtest)\n",
    "    logloss_prob = metrics.log_loss(y_true=test_y, y_pred=test_pred)\n",
    "    print(\"logloss_prob:\",logloss_prob)\n",
    "    print(\"xgb build success\")\n",
    "    return bst\n",
    "\n",
    "def online_pred(xgb_bst, test_feature_data, result_file):\n",
    "    dtest = xgb.DMatrix(data=test_feature_data) \n",
    "    test_pred = xgb_bst.predict(data=dtest)\n",
    "\n",
    "    result = test_data[[\"instance_id\"]]\n",
    "    result[\"predicted_score\"] = test_pred\n",
    "    result.to_csv(result_file, sep=\" \", index=False, line_terminator='\\n')\n",
    "    print(\"online_pred success\")\n",
    "    print(\"result write to %s\"%result_file)\n",
    "    \n",
    "def online_rebuild_pred(train_feature_data,train_label_data, test_feature_data, params, num_boost_round, result_file):\n",
    "    #global factor_data\n",
    "    train_x = train_feature_data\n",
    "    train_y = train_label_data\n",
    "    dtrain = xgb.DMatrix(data=train_x, label=train_y)\n",
    "    dtest = xgb.DMatrix(data=test_feature_data) \n",
    "    watchlist = [(dtrain,'train')]\n",
    "    bst = xgb.train(params=params, dtrain=dtrain, num_boost_round=num_boost_round, evals=watchlist)\n",
    "    test_pred = bst.predict(data=dtest)\n",
    "\n",
    "    result = test_data[[\"instance_id\"]]\n",
    "    result[\"predicted_score\"] = test_pred\n",
    "    result.to_csv(result_file, sep=\" \", index=False, line_terminator='\\n')\n",
    "    print(\"online_rebuild_pred success\")\n",
    "    print(\"result write to %s\"%result_file)\n",
    "    \n",
    "def get_feature(input_data, data_type, csv_read=True, csv_write=False):\n",
    "    if data_type == \"train\":\n",
    "        category_feature_file = \"train_category_feature.csv\"\n",
    "    elif data_type == \"test\":\n",
    "        category_feature_file = \"test_category_feature.csv\"\n",
    "        \n",
    "    if csv_read == True:\n",
    "        #从旧文件中读取\n",
    "        category_feature_data = pd.read_csv(category_feature_file)\n",
    "    else:\n",
    "        #线上预测\n",
    "        category_feature_data = item_category_pro(input_data)\n",
    "        if csv_write == True:\n",
    "            category_feature.to_csv(category_feature_file, index=False)\n",
    "\n",
    "    factor_feature_data = pd.get_dummies(input_data[factor_data], columns=factor_data)\n",
    "    numeric_feature_data = input_data[numeric_data]\n",
    "    feature_data = pd.concat([factor_feature_data, numeric_feature_data, category_feature_data], axis=1)\n",
    "    return feature_data\n",
    "\n",
    "def do_xgb_cv(params, train_feature_data):\n",
    "    \n",
    "    train_x = train_feature_data.ix[train_feature_data[timestamp] < last_day_timestamp, :]\n",
    "    test_x = train_feature_data.ix[train_feature_data[timestamp] >= last_day_timestamp, :]\n",
    "    train_x.drop(train_feature_drop, inplace=True, axis=1)\n",
    "    test_x.drop(train_feature_drop, inplace=True, axis=1)\n",
    "    train_y = train_feature_data.loc[train_feature_data[timestamp] < last_day_timestamp, [\"is_trade\"]]\n",
    "    test_y = train_feature_data.loc[train_feature_data[timestamp] >= last_day_timestamp, [\"is_trade\"]]\n",
    "\n",
    "    #生成xgboost可以识别的训练数据和验证数据\n",
    "    dtrain = xgb.DMatrix(data=train_x, label=train_y)\n",
    "    xgb_cv_result = xgb.cv(params=params, dtrain=dtrain, num_boost_round=1000, nfold = 5, metrics='logloss', early_stopping_rounds=50)\n",
    "    print(xgb_cv_result)\n",
    "    print(xgb_cv_result.shape[0])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_shop_feature(input_data):\n",
    "    shop_data = input_data\n",
    "    #shop_data = train_data\n",
    "    shop_id = shop_data.loc[:,[\"shop_id\"]].drop_duplicates()\n",
    "    shop_data_is_trade = shop_data.loc[shop_data[\"is_trade\"] == 1, :]\n",
    "    shop_group = shop_data.groupby(\"shop_id\")\n",
    "    shop_y1_group = shop_data_is_trade.groupby(\"shop_id\")\n",
    "\n",
    "    tf0 = shop_group[[\"item_id\"]].count()\n",
    "    tf0.rename(columns={\"item_id\":\"shop_count\"},inplace=True)\n",
    "\n",
    "    tf1_1 = tf0.copy()\n",
    "    tf1_1[\"shop_item_dist_count\"] = shop_group[\"item_id\"].agg(lambda x:x.unique().size)\n",
    "    tf1_2 = shop_y1_group[[\"item_id\"]].count()\n",
    "    tf1_2.rename(columns={\"item_id\": \"shop_y1_item_count\"},inplace=True)\n",
    "    tf1_2[\"shop_y1_item_dist_count\"] = shop_y1_group[\"item_id\"].agg(lambda x:x.unique().size)\n",
    "    tf1_1.reset_index(inplace=True)\n",
    "    tf1_2.reset_index(inplace=True)\n",
    "    tf1_tmp = pd.merge(left=tf1_1, right=tf1_2, how=\"left\", on=\"shop_id\")\n",
    "    tf1_tmp[\"shop_y1_item_count_div_item_count\"] = tf1_tmp[\"shop_y1_item_count\"] / tf1_tmp[\"shop_count\"]\n",
    "    tf1_tmp[\"shop_y1_item_count_div_item_dist_count\"] = tf1_tmp[\"shop_y1_item_count\"] / tf1_tmp[\"shop_item_dist_count\"]\n",
    "    tf1_tmp[\"shop_y1_item_dist_count_div_item_dist_count\"] = tf1_tmp[\"shop_y1_item_dist_count\"] / tf1_tmp[\"shop_item_dist_count\"]\n",
    "    tf1 = tf1_tmp.drop(labels=\"shop_count\", axis=1)\n",
    "    #tf1 = tf1_tmp.loc[:,[\"shop_id\", \"shop_y1_item_count_div_item_count\", \"shop_y1_item_count_div_item_dist_count\", \"shop_y1_item_dist_count_div_item_dist_count\"]]\n",
    "\n",
    "\n",
    "    tf2_1 = tf0.copy()\n",
    "    tf2_1[\"shop_user_dist_count\"] = shop_group[\"user_id\"].agg(lambda x:x.unique().size)\n",
    "    tf2_2 = shop_y1_group[[\"user_id\"]].count()\n",
    "    tf2_2.rename(columns={\"user_id\": \"shop_y1_user_count\"},inplace=True)\n",
    "    tf2_2[\"shop_y1_user_dist_count\"] = shop_y1_group[\"user_id\"].agg(lambda x:x.unique().size)\n",
    "    tf2_1.reset_index(inplace=True)\n",
    "    tf2_2.reset_index(inplace=True)\n",
    "    tf2_tmp = pd.merge(left=tf2_1, right=tf2_2, how=\"left\", on=\"shop_id\")\n",
    "    tf2_tmp[\"shop_y1_user_count_div_user_count\"] = tf2_tmp[\"shop_y1_user_count\"] / tf2_tmp[\"shop_count\"]\n",
    "    tf2_tmp[\"shop_y1_user_count_div_user_dist_count\"] = tf2_tmp[\"shop_y1_user_count\"] / tf2_tmp[\"shop_user_dist_count\"]\n",
    "    tf2_tmp[\"shop_y1_user_dist_count_div_user_dist_count\"] = tf2_tmp[\"shop_y1_user_dist_count\"] / tf2_tmp[\"shop_user_dist_count\"]\n",
    "    tf2 = tf2_tmp.drop(labels=\"shop_count\", axis=1)\n",
    "    #tf2 = tf2_tmp.loc[:,[\"shop_id\", \"shop_y1_user_count_div_user_count\", \"shop_y1_user_count_div_user_dist_count\", \"shop_y1_user_dist_count_div_user_dist_count\"]]\n",
    "\n",
    "\n",
    "    tf3_1 = tf0.copy()\n",
    "    tf3_1[\"shop_item_brand_dist_count\"] = shop_group[\"item_brand_id\"].agg(lambda x:x[x!=-1].unique().size)\n",
    "    tf3_2 = shop_y1_group[[\"item_brand_id\"]].count()\n",
    "    tf3_2.rename(columns={\"item_brand_id\": \"shop_y1_item_brand_count\"},inplace=True)\n",
    "    tf3_2[\"shop_y1_item_brand_dist_count\"] = shop_y1_group[\"item_brand_id\"].agg(lambda x:x[x!=-1].unique().size)\n",
    "    tf3_1.reset_index(inplace=True)\n",
    "    tf3_2.reset_index(inplace=True)\n",
    "    tf3_tmp = pd.merge(left=tf3_1, right=tf3_2, how=\"left\", on=\"shop_id\")\n",
    "    tf3_tmp[\"shop_y1_item_brand_count_div_item_brand_count\"] = tf3_tmp[\"shop_y1_item_brand_count\"] / tf3_tmp[\"shop_count\"]\n",
    "    tf3_tmp[\"shop_y1_item_brand_count_div_item_brand_dist_count\"] = tf3_tmp[\"shop_y1_item_brand_count\"] / tf3_tmp[\"shop_item_brand_dist_count\"]\n",
    "    tf3_tmp[\"shop_y1_item_brand_dist_count_div_item_brand_dist_count\"] = tf3_tmp[\"shop_y1_item_brand_dist_count\"] / tf3_tmp[\"shop_item_brand_dist_count\"]\n",
    "    tf3 = tf3_tmp.drop(labels=\"shop_count\", axis=1)\n",
    "    #tf3 = tf3_tmp.loc[:,[\"shop_id\", \"shop_y1_item_brand_count_div_item_brand_count\", \"shop_y1_item_brand_count_div_item_brand_dist_count\", \"shop_y1_item_brand_dist_count_div_item_brand_dist_count\"]]\n",
    "\n",
    "    tf4_1 = tf0.copy()\n",
    "    tf4_group = shop_data.loc[shop_data[\"item_city_id\"] != -1,:].groupby(\"shop_id\")\n",
    "    tf4_1[\"shop_item_city_dist_count\"] = tf4_group[\"item_city_id\"].agg(lambda x:x[x!=-1].unique().size)\n",
    "    tf4_2 = shop_y1_group[[\"item_city_id\"]].count()\n",
    "    tf4_2.rename(columns={\"item_city_id\": \"shop_y1_item_city_count\"},inplace=True)\n",
    "    tf4_2[\"shop_y1_item_city_dist_count\"] = shop_y1_group[\"item_city_id\"].agg(lambda x:x[x!=-1].unique().size)\n",
    "    tf4_1.reset_index(inplace=True)\n",
    "    tf4_2.reset_index(inplace=True)\n",
    "    tf4_tmp = pd.merge(left=tf4_1, right=tf4_2, how=\"left\", on=\"shop_id\")\n",
    "    tf4_tmp[\"shop_y1_item_city_count_div_item_city_count\"] = tf4_tmp[\"shop_y1_item_city_count\"] / tf4_tmp[\"shop_count\"]\n",
    "    tf4_tmp[\"shop_y1_item_city_count_div_item_city_dist_count\"] = tf4_tmp[\"shop_y1_item_city_count\"] / tf4_tmp[\"shop_item_city_dist_count\"]\n",
    "    tf4_tmp[\"shop_y1_item_city_dist_count_div_item_city_dist_count\"] = tf4_tmp[\"shop_y1_item_city_dist_count\"] / tf4_tmp[\"shop_item_city_dist_count\"]\n",
    "    tf4 = tf4_tmp.drop(labels=\"shop_count\", axis=1)\n",
    "    #tf4 = tf4_tmp.loc[:,[\"shop_id\", \"shop_y1_item_city_count_div_item_city_count\", \"shop_y1_item_city_count_div_item_city_dist_count\", \"shop_y1_item_city_dist_count_div_item_city_dist_count\"]]\n",
    "\n",
    "    tf5_1 = tf0.copy()\n",
    "    tf5_group = shop_data.loc[shop_data[\"predict_category_property\"] != -1,:].groupby(\"shop_id\")\n",
    "    tf5_1[\"shop_predict_cp_dist_count\"] = tf5_group[\"predict_category_property\"].agg(lambda x:x[x!=-1].unique().size)\n",
    "    tf5_2 = shop_y1_group[[\"predict_category_property\"]].count()\n",
    "    tf5_2.rename(columns={\"predict_category_property\": \"shop_y1_predict_cp_count\"},inplace=True)\n",
    "    tf5_2[\"shop_y1_predict_cp_dist_count\"] = shop_y1_group[\"predict_category_property\"].agg(lambda x:x[x!=-1].unique().size)\n",
    "    tf5_1.reset_index(inplace=True)\n",
    "    tf5_2.reset_index(inplace=True)\n",
    "    tf5_tmp = pd.merge(left=tf5_1, right=tf5_2, how=\"left\", on=\"shop_id\")\n",
    "    tf5_tmp[\"shop_y1_predict_cp_count_div_predict_cp_count\"] = tf5_tmp[\"shop_y1_predict_cp_count\"] / tf5_tmp[\"shop_count\"]\n",
    "    tf5_tmp[\"shop_y1_predict_cp_count_div_predict_cp_dist_count\"] = tf5_tmp[\"shop_y1_predict_cp_count\"] / tf5_tmp[\"shop_predict_cp_dist_count\"]\n",
    "    tf5_tmp[\"shop_y1_predict_cp_dist_count_div_predict_cp_dist_count\"] = tf5_tmp[\"shop_y1_predict_cp_dist_count\"] / tf5_tmp[\"shop_predict_cp_dist_count\"]\n",
    "    tf5 = tf5_tmp.drop(labels=\"shop_count\", axis=1)\n",
    "\n",
    "    #店铺中广告商品的价格等级,销量等级，被收藏次数的等级，展示次数的等级的平均值\n",
    "    #店铺中交易成功的广告商品的价格等级,销量等级，被收藏次数的等级，展示次数的等级的平均值\n",
    "    #shop_id = shop_data.loc[:,[\"shop_id\"]].drop_duplicates()\n",
    "    #shop_data_is_trade = shop_data.loc[shop_data[\"is_trade\"] == 1, :]\n",
    "\n",
    "    tmp_data = shop_data.loc[shop_data[\"item_sales_level\"] !=-1, [\"shop_id\", \"item_id\", \"item_sales_level\"]].drop_duplicates()\n",
    "    tn1_1 = tmp_data.groupby(\"shop_id\")[[\"item_sales_level\"]].mean().reset_index()\n",
    "    tn1_1.rename(columns={\"item_sales_level\":\"shop_item_sales_level_mean\"}, inplace=True)\n",
    "    tmp_data = shop_data_is_trade.loc[shop_data_is_trade[\"item_sales_level\"] !=-1, [\"shop_id\", \"item_id\", \"item_sales_level\"]].drop_duplicates()\n",
    "    tn1_2_group = tmp_data.groupby(\"shop_id\")\n",
    "    tn1_2 = tn1_2_group[[\"item_sales_level\"]].mean()\n",
    "    tn1_2.rename(columns={\"item_sales_level\":\"shop_y1_item_sales_level_mean\"}, inplace=True)\n",
    "    #tn1_2[\"shop_y1_item_sales_level_min\"] = tn1_2_group[[\"item_sales_level\"]].min()\n",
    "    #tn1_2[\"shop_y1_item_sales_level_max\"] = tn1_2_group[[\"item_sales_level\"]].max()\n",
    "    tn1_2.reset_index(inplace=True)\n",
    "    tn1_tmp = pd.merge(left=shop_id, right=tn1_1, how=\"left\", on=\"shop_id\")\n",
    "    tn1_tmp = pd.merge(left=tn1_tmp, right=tn1_2, how=\"left\", on=\"shop_id\")\n",
    "    tn1_tmp.fillna(value=0, inplace=True)\n",
    "    tn1 = tn1_tmp\n",
    "\n",
    "    tmp_data = shop_data.loc[shop_data[\"item_price_level\"] !=-1, [\"shop_id\", \"item_id\", \"item_price_level\"]].drop_duplicates()\n",
    "    tn2_1 = tmp_data.groupby(\"shop_id\")[[\"item_price_level\"]].mean().reset_index()\n",
    "    tn2_1.rename(columns={\"item_price_level\":\"shop_item_price_level_mean\"}, inplace=True)\n",
    "    tmp_data = shop_data_is_trade.loc[shop_data_is_trade[\"item_price_level\"] !=-1, [\"shop_id\", \"item_id\", \"item_price_level\"]].drop_duplicates()\n",
    "    tn2_2_group = tmp_data.groupby(\"shop_id\")\n",
    "    tn2_2 = tn2_2_group[[\"item_price_level\"]].mean()\n",
    "    tn2_2.rename(columns={\"item_price_level\":\"shop_y1_item_price_level_mean\"}, inplace=True)\n",
    "    #tn2_2[\"shop_y1_item_price_level_min\"] = tn2_2_group[[\"item_price_level\"]].min()\n",
    "    #tn2_2[\"shop_y1_item_price_level_max\"] = tn2_2_group[[\"item_price_level\"]].max()\n",
    "    tn2_2.reset_index(inplace=True)\n",
    "    tn2_tmp = pd.merge(left=shop_id, right=tn2_1, how=\"left\", on=\"shop_id\")\n",
    "    tn2_tmp = pd.merge(left=tn2_tmp, right=tn2_2, how=\"left\", on=\"shop_id\")\n",
    "    tn2_tmp.fillna(value=0, inplace=True)\n",
    "    tn2 = tn2_tmp\n",
    "\n",
    "    tmp_data = shop_data.loc[shop_data[\"item_collected_level\"] !=-1, [\"shop_id\", \"item_id\", \"item_collected_level\"]].drop_duplicates()\n",
    "    tn3_1 = tmp_data.groupby(\"shop_id\")[[\"item_collected_level\"]].mean().reset_index()\n",
    "    tn3_1.rename(columns={\"item_collected_level\":\"shop_item_collected_level_mean\"}, inplace=True)\n",
    "    tmp_data = shop_data_is_trade.loc[shop_data_is_trade[\"item_collected_level\"] !=-1, [\"shop_id\", \"item_id\", \"item_collected_level\"]].drop_duplicates()\n",
    "    tn3_2_group = tmp_data.groupby(\"shop_id\")\n",
    "    tn3_2 = tn3_2_group[[\"item_collected_level\"]].mean()\n",
    "    tn3_2.rename(columns={\"item_collected_level\":\"shop_y1_item_collected_level_mean\"}, inplace=True)\n",
    "    #tn3_2[\"shop_y1_item_collected_level_min\"] = tn3_2_group[[\"item_collected_level\"]].min()\n",
    "    #tn3_2[\"shop_y1_item_collected_level_max\"] = tn3_2_group[[\"item_collected_level\"]].max()\n",
    "    tn3_2.reset_index(inplace=True)\n",
    "    tn3_tmp = pd.merge(left=shop_id, right=tn3_1, how=\"left\", on=\"shop_id\")\n",
    "    tn3_tmp = pd.merge(left=tn3_tmp, right=tn3_2, how=\"left\", on=\"shop_id\")\n",
    "    tn3_tmp.fillna(value=0, inplace=True)\n",
    "    tn3 = tn3_tmp\n",
    "\n",
    "    tmp_data = shop_data.loc[shop_data[\"item_pv_level\"] !=-1, [\"shop_id\", \"item_id\", \"item_pv_level\"]].drop_duplicates()\n",
    "    tn4_1 = tmp_data.groupby(\"shop_id\")[[\"item_pv_level\"]].mean().reset_index()\n",
    "    tn4_1.rename(columns={\"item_pv_level\":\"shop_item_pv_level_mean\"}, inplace=True)\n",
    "    tmp_data = shop_data_is_trade.loc[shop_data_is_trade[\"item_pv_level\"] !=-1, [\"shop_id\", \"item_id\", \"item_pv_level\"]].drop_duplicates()\n",
    "    tn4_2_group = tmp_data.groupby(\"shop_id\")\n",
    "    tn4_2 = tn4_2_group[[\"item_pv_level\"]].mean()\n",
    "    tn4_2.rename(columns={\"item_pv_level\":\"shop_y1_item_pv_level_mean\"}, inplace=True)\n",
    "    #tn4_2[\"shop_y1_item_pv_level_min\"] = tn4_2_group[[\"item_pv_level\"]].min()\n",
    "    #tn4_2[\"shop_y1_item_pv_level_max\"] = tn4_2_group[[\"item_pv_level\"]].max()\n",
    "    tn4_2.reset_index(inplace=True)\n",
    "    tn4_tmp = pd.merge(left=shop_id, right=tn4_1, how=\"left\", on=\"shop_id\")\n",
    "    tn4_tmp = pd.merge(left=tn4_tmp, right=tn4_2, how=\"left\", on=\"shop_id\")\n",
    "    tn4_tmp.fillna(value=0, inplace=True)\n",
    "    tn4 = tn4_tmp\n",
    "\n",
    "    tmp_data = shop_data.loc[shop_data[\"user_age_level\"] !=-1, :]\n",
    "    tn5_1 = tmp_data.groupby(\"shop_id\")[[\"user_age_level\"]].mean().reset_index()\n",
    "    tn5_1.rename(columns={\"user_age_level\":\"shop_user_age_level_mean\"}, inplace=True)\n",
    "    tmp_data = shop_data_is_trade.loc[shop_data_is_trade[\"user_age_level\"] !=-1,:]\n",
    "    tn5_2_group = tmp_data.groupby(\"shop_id\")\n",
    "    tn5_2 = tn5_2_group[[\"user_age_level\"]].mean()\n",
    "    tn5_2.rename(columns={\"user_age_level\":\"shop_y1_user_age_level_mean\"}, inplace=True)\n",
    "   # tn5_2[\"shop_y1_user_age_level_min\"] = tn5_2_group[[\"user_age_level\"]].min()\n",
    "    #tn5_2[\"shop_y1_user_age_level_max\"] = tn5_2_group[[\"user_age_level\"]].max()\n",
    "    tn5_2.reset_index(inplace=True)\n",
    "    tn5_tmp = pd.merge(left=shop_id, right=tn5_1, how=\"left\", on=\"shop_id\")\n",
    "    tn5_tmp = pd.merge(left=tn5_tmp, right=tn5_2, how=\"left\", on=\"shop_id\")\n",
    "    tn5_tmp.fillna(value=0, inplace=True)\n",
    "    tn5 = tn5_tmp\n",
    "\n",
    "    tmp_data = shop_data.loc[shop_data[\"user_star_level\"] !=-1, :]\n",
    "    tn6_1 = tmp_data.groupby(\"shop_id\")[[\"user_star_level\"]].mean().reset_index()\n",
    "    tn6_1.rename(columns={\"user_star_level\":\"shop_user_star_level_mean\"}, inplace=True)\n",
    "    tmp_data = shop_data_is_trade.loc[shop_data_is_trade[\"user_star_level\"] !=-1,:]\n",
    "    tn6_2_group = tmp_data.groupby(\"shop_id\")\n",
    "    tn6_2 = tn6_2_group[[\"user_star_level\"]].mean()\n",
    "    tn6_2.rename(columns={\"user_star_level\":\"shop_y1_user_star_level_mean\"}, inplace=True)\n",
    "    #tn6_2[\"shop_y1_user_star_level_min\"] = tn6_2_group[[\"user_star_level\"]].min()\n",
    "    #tn6_2[\"shop_y1_user_star_level_max\"] = tn6_2_group[[\"user_star_level\"]].max()\n",
    "    tn6_2.reset_index(inplace=True)\n",
    "    tn6_tmp = pd.merge(left=shop_id, right=tn6_1, how=\"left\", on=\"shop_id\")\n",
    "    tn6_tmp = pd.merge(left=tn6_tmp, right=tn6_2, how=\"left\", on=\"shop_id\")\n",
    "    tn6_tmp.fillna(value=0, inplace=True)\n",
    "    tn6 = tn6_tmp\n",
    "\n",
    "    tmp_data = shop_data.loc[shop_data[\"context_page_id\"] !=-1, :]\n",
    "    tn7_1 = tmp_data.groupby(\"shop_id\")[[\"context_page_id\"]].mean().reset_index()\n",
    "    tn7_1.rename(columns={\"context_page_id\":\"shop_context_page_id_mean\"}, inplace=True)\n",
    "    tmp_data = shop_data_is_trade.loc[shop_data_is_trade[\"context_page_id\"] !=-1,:]\n",
    "    tn7_2_group = tmp_data.groupby(\"shop_id\")\n",
    "    tn7_2 = tn7_2_group[[\"context_page_id\"]].mean()\n",
    "    tn7_2.rename(columns={\"context_page_id\":\"shop_y1_context_page_id_mean\"}, inplace=True)\n",
    "    #tn7_2[\"shop_y1_context_page_id_min\"] = tn7_2_group[[\"context_page_id\"]].min()\n",
    "   # tn7_2[\"shop_y1_context_page_id_max\"] = tn7_2_group[[\"context_page_id\"]].max()\n",
    "    tn7_2.reset_index(inplace=True)\n",
    "    tn7_tmp = pd.merge(left=shop_id, right=tn7_1, how=\"left\", on=\"shop_id\")\n",
    "    tn7_tmp = pd.merge(left=tn7_tmp, right=tn7_2, how=\"left\", on=\"shop_id\")\n",
    "    tn7_tmp.fillna(value=0, inplace=True)\n",
    "    tn7 = tn7_tmp\n",
    "\n",
    "    shop_feature = tf0.reset_index()\n",
    "    shop_feature = pd.merge(left=shop_feature, right=tf1, how=\"left\", on=\"shop_id\")\n",
    "    shop_feature = pd.merge(left=shop_feature, right=tf2, how=\"left\", on=\"shop_id\")\n",
    "    shop_feature = pd.merge(left=shop_feature, right=tf3, how=\"left\", on=\"shop_id\")\n",
    "    shop_feature = pd.merge(left=shop_feature, right=tf4, how=\"left\", on=\"shop_id\")\n",
    "    shop_feature = pd.merge(left=shop_feature, right=tf5, how=\"left\", on=\"shop_id\")\n",
    "    shop_feature = pd.merge(left=shop_feature, right=tn1, how=\"left\", on=\"shop_id\")\n",
    "    shop_feature = pd.merge(left=shop_feature, right=tn2, how=\"left\", on=\"shop_id\")\n",
    "    shop_feature = pd.merge(left=shop_feature, right=tn3, how=\"left\", on=\"shop_id\")\n",
    "    shop_feature = pd.merge(left=shop_feature, right=tn4, how=\"left\", on=\"shop_id\")\n",
    "    shop_feature = pd.merge(left=shop_feature, right=tn5, how=\"left\", on=\"shop_id\")\n",
    "    shop_feature = pd.merge(left=shop_feature, right=tn6, how=\"left\", on=\"shop_id\")\n",
    "    shop_feature = pd.merge(left=shop_feature, right=tn7, how=\"left\", on=\"shop_id\")\n",
    "    #shop_feature = pd.merge(left=shop_feature, right=t8, how=\"left\", on=\"shop_id\")\n",
    "    return shop_feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "20180403\n",
    "v1.0 没有进行特征工程，提取numeric和factor(onehot)使用xgboos简单调参后建模\n",
    "线下分数(随机划分)：logloss_prob: 0.08956929329219732\n",
    "last day:0.08253\n",
    "线上分数：0.08280\n",
    "20180404\n",
    "v2.0 在v1.0基础上加入了广告类目属性和上下文预测的类目属性对比特征集\n",
    "线下分数(随机划分)：logloss_prob: 0.08961649265749788\n",
    "线下分数:0.08243\n",
    "线上分数：0.08269\n",
    "20180410\n",
    "v2.1在4号的基础上对所有数据重新建模（不设验证集），相当于用7天预测最后一天，效果有所提升\n",
    "线下分数: 0.08243\n",
    "线上分数：0.08217\n",
    "1.说明广告类目属性对比特征集起到作用了\n",
    "2.不能用前6天的数据预测第8天（线上那一天），可以尝试用线上那天之前的6天进行预测\n",
    "20180411\n",
    "v2.2在v2.1的基础上用上线那天之前的6天进行预测\n",
    "线下分数: 0.08243\n",
    "线上分数：0.08211\n",
    "20180412\n",
    "v3.0 加入了部分商店特征（转化率），出现过拟合，初步原因可能是使用了预测数据集的商店特征（预先知道了是否要购买广告）\n",
    "线下分数: 0.07922\n",
    "线上分数：0.08953\n",
    "20180414\n",
    "v3.1 加入了全部商店特征（转化率），解决了过拟合的问题\n",
    "解决方案：历史转换率仅仅提取样本前2天的数据，训练集样本为20,21，22，23号数据，验证集样本为24号数据，测试集为25号数据\n",
    "每天的样本特征加入了前两天的转化率，这样使得训练集，验证集，测试集的数据分布一致\n",
    "线下分数: 0.08220\n",
    "线上分数：0.08219\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
