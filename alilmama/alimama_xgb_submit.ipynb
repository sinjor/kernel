{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:46: DeprecationWarning: \n",
      ".ix is deprecated. Please use\n",
      ".loc for label based indexing or\n",
      ".iloc for positional indexing\n",
      "\n",
      "See the documentation here:\n",
      "http://pandas.pydata.org/pandas-docs/stable/indexing.html#ix-indexer-is-deprecated\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttrain-logloss:0.605157\n",
      "[1]\ttrain-logloss:0.533121\n",
      "[2]\ttrain-logloss:0.473155\n",
      "[3]\ttrain-logloss:0.42264\n",
      "[4]\ttrain-logloss:0.379634\n",
      "[5]\ttrain-logloss:0.342766\n",
      "[6]\ttrain-logloss:0.310977\n",
      "[7]\ttrain-logloss:0.283401\n",
      "[8]\ttrain-logloss:0.259382\n",
      "[9]\ttrain-logloss:0.23844\n",
      "[10]\ttrain-logloss:0.220089\n",
      "[11]\ttrain-logloss:0.204005\n",
      "[12]\ttrain-logloss:0.189869\n",
      "[13]\ttrain-logloss:0.177426\n",
      "[14]\ttrain-logloss:0.166453\n",
      "[15]\ttrain-logloss:0.156797\n",
      "[16]\ttrain-logloss:0.148266\n",
      "[17]\ttrain-logloss:0.140746\n",
      "[18]\ttrain-logloss:0.13412\n",
      "[19]\ttrain-logloss:0.128282\n",
      "[20]\ttrain-logloss:0.123136\n",
      "[21]\ttrain-logloss:0.11859\n",
      "[22]\ttrain-logloss:0.114576\n",
      "[23]\ttrain-logloss:0.111051\n",
      "[24]\ttrain-logloss:0.107946\n",
      "[25]\ttrain-logloss:0.105211\n",
      "[26]\ttrain-logloss:0.102812\n",
      "[27]\ttrain-logloss:0.100705\n",
      "[28]\ttrain-logloss:0.09885\n",
      "[29]\ttrain-logloss:0.097224\n",
      "[30]\ttrain-logloss:0.095808\n",
      "[31]\ttrain-logloss:0.094571\n",
      "[32]\ttrain-logloss:0.093537\n",
      "[33]\ttrain-logloss:0.092589\n",
      "[34]\ttrain-logloss:0.09176\n",
      "[35]\ttrain-logloss:0.091027\n",
      "[36]\ttrain-logloss:0.090387\n",
      "[37]\ttrain-logloss:0.089842\n",
      "[38]\ttrain-logloss:0.089357\n",
      "[39]\ttrain-logloss:0.088939\n",
      "[40]\ttrain-logloss:0.088569\n",
      "[41]\ttrain-logloss:0.088252\n",
      "[42]\ttrain-logloss:0.087974\n",
      "[43]\ttrain-logloss:0.087728\n",
      "[44]\ttrain-logloss:0.087517\n",
      "[45]\ttrain-logloss:0.087315\n",
      "[46]\ttrain-logloss:0.087145\n",
      "[47]\ttrain-logloss:0.086997\n",
      "[48]\ttrain-logloss:0.086872\n",
      "[49]\ttrain-logloss:0.086757\n",
      "[50]\ttrain-logloss:0.086651\n",
      "[51]\ttrain-logloss:0.086559\n",
      "[52]\ttrain-logloss:0.086471\n",
      "[53]\ttrain-logloss:0.086404\n",
      "[54]\ttrain-logloss:0.08634\n",
      "[55]\ttrain-logloss:0.08628\n",
      "[56]\ttrain-logloss:0.086227\n",
      "[57]\ttrain-logloss:0.086178\n",
      "[58]\ttrain-logloss:0.086129\n",
      "[59]\ttrain-logloss:0.086083\n",
      "[60]\ttrain-logloss:0.086046\n",
      "[61]\ttrain-logloss:0.086011\n",
      "[62]\ttrain-logloss:0.085972\n",
      "[63]\ttrain-logloss:0.085945\n",
      "[64]\ttrain-logloss:0.085917\n",
      "[65]\ttrain-logloss:0.085884\n",
      "[66]\ttrain-logloss:0.085854\n",
      "[67]\ttrain-logloss:0.085822\n",
      "[68]\ttrain-logloss:0.085794\n",
      "[69]\ttrain-logloss:0.085749\n",
      "[70]\ttrain-logloss:0.08572\n",
      "[71]\ttrain-logloss:0.085691\n",
      "[72]\ttrain-logloss:0.085668\n",
      "[73]\ttrain-logloss:0.085644\n",
      "[74]\ttrain-logloss:0.085624\n",
      "[75]\ttrain-logloss:0.085606\n",
      "[76]\ttrain-logloss:0.085573\n",
      "[77]\ttrain-logloss:0.08555\n",
      "[78]\ttrain-logloss:0.085526\n",
      "[79]\ttrain-logloss:0.085501\n",
      "[80]\ttrain-logloss:0.085473\n",
      "[81]\ttrain-logloss:0.085455\n",
      "[82]\ttrain-logloss:0.085422\n",
      "[83]\ttrain-logloss:0.085375\n",
      "[84]\ttrain-logloss:0.085358\n",
      "[85]\ttrain-logloss:0.085336\n",
      "[86]\ttrain-logloss:0.08531\n",
      "[87]\ttrain-logloss:0.085292\n",
      "[88]\ttrain-logloss:0.085253\n",
      "[89]\ttrain-logloss:0.085234\n",
      "[90]\ttrain-logloss:0.085201\n",
      "[91]\ttrain-logloss:0.08517\n",
      "[92]\ttrain-logloss:0.085141\n",
      "[93]\ttrain-logloss:0.085114\n",
      "[94]\ttrain-logloss:0.085094\n",
      "logloss_prob: 0.08924543772564368\n",
      "xgb build success\n"
     ]
    }
   ],
   "source": [
    "params={'booster':'gbtree',\n",
    "    'eta': 0.1,\n",
    "    'n_estimators':103,\n",
    "    'max_depth':5, #3 10\n",
    "    'min_child_weight':3,#1 6\n",
    "    'gamma':0.3,\n",
    "    'subsample':0.8,\n",
    "    'colsample_bytree':0.8,\n",
    "    'objective': 'binary:logistic',\n",
    "    'nthread':8,\n",
    "    'scale_pos_weight':1,\n",
    "    'eval_metric': 'logloss',\n",
    "    'lambda':0,\n",
    "    'seed':0,\n",
    "    'silent':0}\n",
    "xgb_bst = xgb_build(params, train_feature_data, num_boost_round=95)\n",
    "\n",
    "#线上预测\n",
    "# test_feature_data = get_feature(test_data, data_type=\"test\", csv_read=True, csv_write=False)\n",
    "# test_feature_data = pd.concat([test_data[[\"shop_id\"]],test_feature_data], axis=1)\n",
    "# test_shop_data = train_data.ix[train_data.context_timestamp > second_day_timestamp, :]\n",
    "# test_shop_feature = get_shop_feature(test_shop_data)\n",
    "# test_feature_data = pd.merge(left=test_feature_data, right=test_shop_feature, how=\"left\", on=\"shop_id\")\n",
    "# test_feature_data = test_feature_data.iloc[:, 1:]\n",
    "# test_feature_data = get_feature(test_data, data_type=\"test\", csv_read=True, csv_write=False)\n",
    "\n",
    "#使用训练模型进行预测\n",
    "#online_pred(xgb_bst, test_feature_data, result_file=\"result_test.csv\")\n",
    "#重新建模预测\n",
    "#online_rebuild_pred(train_feature_data, test_feature_data, params=params, num_boost_round=95, result_file=\"result_v30_0412.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\Anaconda3\\lib\\site-packages\\sklearn\\cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n",
      "E:\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:130: DeprecationWarning: \n",
      ".ix is deprecated. Please use\n",
      ".loc for label based indexing or\n",
      ".iloc for positional indexing\n",
      "\n",
      "See the documentation here:\n",
      "http://pandas.pydata.org/pandas-docs/stable/indexing.html#ix-indexer-is-deprecated\n"
     ]
    }
   ],
   "source": [
    "from sklearn import datasets\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn import metrics\n",
    "import xgboost as xgb\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "test_data = pd.read_csv(\"G:/program_monkey/ML/tianchi/alimama_ad/test.txt\",sep=\" \")\n",
    "train_data = pd.read_csv(\"G:/program_monkey/ML/tianchi/alimama_ad/train.txt\",sep=\" \")\n",
    "factor_data = [\"item_category_list\",\"user_gender_id\",\"user_occupation_id\"]\n",
    "numeric_data =[\"item_price_level\",\"item_sales_level\",\"item_collected_level\",\n",
    "               \"item_pv_level\",\"user_age_level\",\"user_star_level\",\"shop_review_num_level\",\n",
    "               \"shop_review_positive_rate\",\"shop_star_level\",\"shop_score_service\",\n",
    "               \"shop_score_delivery\",\"shop_score_description\"]\n",
    "timestamp = \"context_timestamp\"\n",
    "train_data[[timestamp,\"is_trade\"]]\n",
    "feature_begin = 3\n",
    "#last_day_begin_time = \"2018-09-24 00:00:00\"\n",
    "time_array = time.strptime(\"2018-09-24 00:00:00\", \"%Y-%m-%d %H:%M:%S\")\n",
    "last_day_timestamp = int(time.mktime(time_array))\n",
    "#second_day_begin_time = \"2018-09-19 00:00:00\"\n",
    "time_array = time.strptime(\"2018-09-19 00:00:00\", \"%Y-%m-%d %H:%M:%S\")\n",
    "second_day_timestamp = int(time.mktime(time_array))\n",
    "\n",
    "train_feature_data = get_feature(train_data, data_type=\"train\", csv_read=True, csv_write=False)\n",
    "train_feature_data = pd.concat([train_data[[timestamp,\"is_trade\", \"shop_id\"]],train_feature_data], axis=1)\n",
    "#商店特征需要严格遵守训练集的时间窗口，不能使用测试集的数据，否则容易导致过拟合\n",
    "shop_data = train_data.ix[train_data.context_timestamp < last_day_timestamp, :]\n",
    "shop_feature = get_shop_feature(shop_data)\n",
    "train_feature_data = pd.merge(left=train_feature_data, right=shop_feature, how=\"left\", on=\"shop_id\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " 1.函数接口（建模，提特征等） 2.历史结果记录    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def xgb_build(params, train_feature_data, num_boost_round):\n",
    "    #前6天作为训练集，最后一天作为验证集\n",
    "    train_x = train_feature_data.ix[train_feature_data[timestamp] < last_day_timestamp, feature_begin:]\n",
    "    test_x = train_feature_data.ix[train_feature_data[timestamp] >= last_day_timestamp, feature_begin:]\n",
    "    train_y = train_feature_data.loc[train_feature_data[timestamp] < last_day_timestamp, [\"is_trade\"]]\n",
    "    test_y = train_feature_data.loc[train_feature_data[timestamp] >= last_day_timestamp, [\"is_trade\"]]\n",
    "\n",
    "    #生成xgboost可以识别的训练数据和验证数据\n",
    "    dtrain = xgb.DMatrix(data=train_x, label=train_y)\n",
    "    dtest = xgb.DMatrix(data=test_x)\n",
    "\n",
    "    watchlist = [(dtrain,'train')]\n",
    "    bst = xgb.train(params=params, dtrain=dtrain, num_boost_round=num_boost_round, evals=watchlist)\n",
    "    test_pred = bst.predict(data=dtest)\n",
    "    logloss_prob = metrics.log_loss(y_true=test_y, y_pred=test_pred)\n",
    "    print(\"logloss_prob:\",logloss_prob)\n",
    "    print(\"xgb build success\")\n",
    "    return bst\n",
    "\n",
    "def online_pred(xgb_bst, test_feature_data, result_file):\n",
    "    dtest = xgb.DMatrix(data=test_feature_data) \n",
    "    test_pred = xgb_bst.predict(data=dtest)\n",
    "\n",
    "    result = test_data[[\"instance_id\"]]\n",
    "    result[\"predicted_score\"] = test_pred\n",
    "    result.to_csv(result_file, sep=\" \", index=False, line_terminator='\\n')\n",
    "    print(\"online_pred success\")\n",
    "    print(\"result write to %s\"%result_file)\n",
    "    \n",
    "def online_rebuild_pred(train_feature_data, test_feature_data, params, num_boost_round, result_file):\n",
    "    #global factor_data\n",
    "    train_x = train_feature_data.ix[train_feature_data[timestamp] > second_day_timestamp, feature_begin:]\n",
    "    train_y = train_feature_data.loc[train_feature_data[timestamp] > second_day_timestamp, [\"is_trade\"]]\n",
    "\n",
    "    #train_x = train_feature_data.ix[:, 2:]\n",
    "    #train_y = train_feature_data.loc[:, [\"is_trade\"]]\n",
    "    dtrain = xgb.DMatrix(data=train_x, label=train_y)\n",
    "    dtest = xgb.DMatrix(data=test_feature_data) \n",
    "    watchlist = [(dtrain,'train')]\n",
    "    bst = xgb.train(params=params, dtrain=dtrain, num_boost_round=num_boost_round, evals=watchlist)\n",
    "    test_pred = bst.predict(data=dtest)\n",
    "\n",
    "    result = test_data[[\"instance_id\"]]\n",
    "    result[\"predicted_score\"] = test_pred\n",
    "    #print(result.head())\n",
    "    result.to_csv(result_file, sep=\" \", index=False, line_terminator='\\n')\n",
    "    print(\"online_rebuild_pred success\")\n",
    "    print(\"result write to %s\"%result_file)\n",
    "    \n",
    "def get_feature(input_data, data_type, csv_read=True, csv_write=False):\n",
    "    if data_type == \"train\":\n",
    "        category_feature_file = \"train_category_feature.csv\"\n",
    "    elif data_type == \"test\":\n",
    "        category_feature_file = \"test_category_feature.csv\"\n",
    "        \n",
    "    if csv_read == True:\n",
    "        #从旧文件中读取\n",
    "        category_feature_data = pd.read_csv(category_feature_file)\n",
    "    else:\n",
    "        #线上预测\n",
    "        category_feature_data = item_category_pro(input_data)\n",
    "        if csv_write == True:\n",
    "            category_feature.to_csv(category_feature_file, index=False)\n",
    "\n",
    "    factor_feature_data = pd.get_dummies(input_data[factor_data], columns=factor_data)\n",
    "    numeric_feature_data = input_data[numeric_data]\n",
    "    feature_data = pd.concat([factor_feature_data, numeric_feature_data, category_feature_data], axis=1)\n",
    "    return feature_data\n",
    "\n",
    "def do_xgb_cv(params, train_feature_data):\n",
    "    \n",
    "    train_x = train_feature_data.ix[train_feature_data[timestamp] < last_day_timestamp, feature_begin:]\n",
    "    test_x = train_feature_data.ix[train_feature_data[timestamp] >= last_day_timestamp, feature_begin:]\n",
    "    train_y = train_feature_data.loc[train_feature_data[timestamp] < last_day_timestamp, [\"is_trade\"]]\n",
    "    test_y = train_feature_data.loc[train_feature_data[timestamp] >= last_day_timestamp, [\"is_trade\"]]\n",
    "\n",
    "    #生成xgboost可以识别的训练数据和验证数据\n",
    "    dtrain = xgb.DMatrix(data=train_x, label=train_y)\n",
    "    xgb_cv_result = xgb.cv(params=params, dtrain=dtrain, num_boost_round=1000, nfold = 5, metrics='logloss', early_stopping_rounds=50)\n",
    "    print(xgb_cv_result)\n",
    "    print(xgb_cv_result.shape[0])\n",
    "    \n",
    "def get_shop_feature(input_data):\n",
    "    shop_data = input_data\n",
    "    shop_id = shop_data.loc[:,[\"shop_id\"]].drop_duplicates()\n",
    "    shop_data_is_trade = shop_data.loc[shop_data[\"is_trade\"] == 1, :]\n",
    "    t1_group = shop_data.loc[:,[\"shop_id\", \"item_id\", \"user_id\", \"item_brand_id\"]].groupby(\"shop_id\")\n",
    "    t1_1 = t1_group[[\"item_id\"]].count()\n",
    "    t1_1.rename(columns={\"item_id\":\"shop_item_count\"},inplace=True)\n",
    "    t1_1[\"shop_item_dist_count\"] = t1_group[\"item_id\"].agg(lambda x:x.unique().size)\n",
    "    t1_1[\"shop_user_dist_count\"] = t1_group[\"user_id\"].agg(lambda x:x.unique().size)\n",
    "    t1_1[\"shop_item_brand_dist_count\"] = t1_group[\"item_brand_id\"].agg(lambda x:x[x!=-1].unique().size)\n",
    "    t1_2_group = shop_data_is_trade.loc[:,[\"shop_id\", \"item_id\",\"user_id\", \"item_brand_id\"]].groupby(\"shop_id\")\n",
    "    t1_2 = t1_2_group[[\"item_id\"]].count()\n",
    "    t1_2.rename(columns={\"item_id\":\"shop_y1_count\"},inplace=True)\n",
    "    t1_2[\"shop_y1_item_dist_count\"] = t1_2_group[\"item_id\"].agg(lambda x:x.unique().size)\n",
    "    t1_2[\"shop_y1_user_dist_count\"] = t1_2_group[\"user_id\"].agg(lambda x:x.unique().size)\n",
    "    t1_2[\"shop_y1_item_brand_dist_count\"] = t1_group[\"item_brand_id\"].agg(lambda x:x[x!=-1].unique().size)\n",
    "    #t1_1 = pd.merge(left=shop_id, right=t1_1, how=\"left\", on=\"shop_id\")\n",
    "    t1_1.reset_index(inplace=True)\n",
    "    t1_2.reset_index(inplace=True)\n",
    "    t1_tmp = pd.merge(left=t1_1, right=t1_2, how=\"left\", on=\"shop_id\")\n",
    "    #t1_tmp.fillna(value=0, inplace=True)\n",
    "    t1_tmp[\"shop_y1_item_count_div_item_count\"] = t1_tmp[\"shop_y1_count\"] / t1_tmp[\"shop_item_count\"]\n",
    "    t1_tmp[\"shop_y1_item_dist_count_div_item_dist_count\"] = t1_tmp[\"shop_y1_item_dist_count\"] / t1_tmp[\"shop_item_dist_count\"]\n",
    "    t1_tmp[\"shop_y1_user_dist_count_div_user_dist_count\"] = t1_tmp[\"shop_y1_user_dist_count\"] / t1_tmp[\"shop_user_dist_count\"]\n",
    "    t1_tmp[\"shop_y1_brand_dist_count_div_item_brand_dist_count\"] = t1_tmp[\"shop_y1_item_brand_dist_count\"] / t1_tmp[\"shop_item_brand_dist_count\"]\n",
    "    t1_tmp.fillna(value=0, inplace=True)\n",
    "    t1 = t1_tmp\n",
    "    #display(t1_tmp)\n",
    "\n",
    "    #店铺中广告商品的价格等级,销量等级，被收藏次数的等级，展示次数的等级的平均值\n",
    "    #店铺中交易成功的广告商品的价格等级,销量等级，被收藏次数的等级，展示次数的等级的平均值\n",
    "    shop_id = shop_data.loc[:,[\"shop_id\"]].drop_duplicates()\n",
    "    #shop_data_is_trade = shop_data.loc[shop_data[\"is_trade\"] == 1, :]\n",
    "    tmp_data = shop_data.loc[shop_data[\"item_price_level\"] !=-1, [\"shop_id\", \"item_id\", \"item_price_level\"]].drop_duplicates()\n",
    "    t2_1 = tmp_data.groupby(\"shop_id\")[[\"item_price_level\"]].mean().reset_index()\n",
    "    t2_1.rename(columns={\"item_price_level\":\"shop_item_price_level_mean\"}, inplace=True)\n",
    "    tmp_data = shop_data.loc[shop_data[\"item_sales_level\"] !=-1, [\"shop_id\", \"item_id\", \"item_sales_level\"]].drop_duplicates()\n",
    "    t2_2 = tmp_data.groupby(\"shop_id\")[[\"item_sales_level\"]].mean().reset_index()\n",
    "    t2_2.rename(columns={\"item_sales_level\":\"shop_item_sales_level_mean\"}, inplace=True)\n",
    "    tmp_data = shop_data.loc[shop_data[\"item_collected_level\"] !=-1, [\"shop_id\", \"item_id\", \"item_collected_level\"]].drop_duplicates()\n",
    "    t2_3 = tmp_data.groupby(\"shop_id\")[[\"item_collected_level\"]].mean().reset_index()\n",
    "    t2_3.rename(columns={\"item_collected_level\":\"shop_item_collected_level_mean\"}, inplace=True)\n",
    "    tmp_data = shop_data.loc[shop_data[\"item_collected_level\"] !=-1, [\"shop_id\", \"item_id\", \"item_pv_level\"]].drop_duplicates()\n",
    "    t2_4 = tmp_data.groupby(\"shop_id\")[[\"item_pv_level\"]].mean().reset_index()\n",
    "    t2_4.rename(columns={\"item_pv_level\":\"shop_item_pv_level_mean\"}, inplace=True)\n",
    "\n",
    "    tmp_data = shop_data_is_trade.loc[shop_data_is_trade[\"item_price_level\"] !=-1, [\"shop_id\", \"item_id\", \"item_price_level\"]].drop_duplicates()\n",
    "    t2_5 = tmp_data.groupby(\"shop_id\")[[\"item_price_level\"]].mean().reset_index()\n",
    "    t2_5.rename(columns={\"item_price_level\":\"shop_y1_item_price_level_mean\"}, inplace=True)\n",
    "    tmp_data = shop_data_is_trade.loc[shop_data_is_trade[\"item_sales_level\"] !=-1, [\"shop_id\", \"item_id\", \"item_sales_level\"]].drop_duplicates()\n",
    "    t2_6 = tmp_data.groupby(\"shop_id\")[[\"item_sales_level\"]].mean().reset_index()\n",
    "    t2_6.rename(columns={\"item_sales_level\":\"shop_y1_item_sales_level_mean\"}, inplace=True)\n",
    "    tmp_data = shop_data_is_trade.loc[shop_data_is_trade[\"item_collected_level\"] !=-1, [\"shop_id\", \"item_id\", \"item_collected_level\"]].drop_duplicates()\n",
    "    t2_7 = tmp_data.groupby(\"shop_id\")[[\"item_collected_level\"]].mean().reset_index()\n",
    "    t2_7.rename(columns={\"item_collected_level\":\"shop_y1_item_collected_level_mean\"}, inplace=True)\n",
    "    tmp_data = shop_data_is_trade.loc[shop_data_is_trade[\"item_pv_level\"] !=-1, [\"shop_id\", \"item_id\", \"item_pv_level\"]].drop_duplicates()\n",
    "    t2_8 = tmp_data.groupby(\"shop_id\")[[\"item_pv_level\"]].mean().reset_index()\n",
    "    t2_8.rename(columns={\"item_pv_level\":\"shop_y1_item_pv_level_mean\"}, inplace=True)\n",
    "    t2_tmp = pd.merge(left=shop_id, right=t2_1, how=\"left\", on=\"shop_id\")\n",
    "    t2_tmp = pd.merge(left=t2_tmp, right=t2_2, how=\"left\", on=\"shop_id\")\n",
    "    t2_tmp = pd.merge(left=t2_tmp, right=t2_3, how=\"left\", on=\"shop_id\")\n",
    "    t2_tmp = pd.merge(left=t2_tmp, right=t2_4, how=\"left\", on=\"shop_id\")\n",
    "    t2_tmp = pd.merge(left=t2_tmp, right=t2_5, how=\"left\", on=\"shop_id\")\n",
    "    t2_tmp = pd.merge(left=t2_tmp, right=t2_6, how=\"left\", on=\"shop_id\")\n",
    "    t2_tmp = pd.merge(left=t2_tmp, right=t2_7, how=\"left\", on=\"shop_id\")\n",
    "    t2_tmp = pd.merge(left=t2_tmp, right=t2_8, how=\"left\", on=\"shop_id\")\n",
    "    t2_tmp.fillna(value=0, inplace=True)\n",
    "    t2 = t2_tmp\n",
    "    shop_feature = pd.merge(left=t1, right=t2, how=\"left\", on=\"shop_id\")\n",
    "    return shop_feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "20180403\n",
    "v1.0 没有进行特征工程，提取numeric和factor(onehot)使用xgboos简单调参后建模\n",
    "线下分数(随机划分)：logloss_prob: 0.08956929329219732\n",
    "last day:0.08253\n",
    "线上分数：0.08280\n",
    "20180404\n",
    "v2.0 在v1.0基础上加入了广告类目属性和上下文预测的类目属性对比特征集\n",
    "线下分数(随机划分)：logloss_prob: 0.08961649265749788\n",
    "线下分数:0.08243\n",
    "线上分数：0.08269\n",
    "20180410\n",
    "v2.1在4号的基础上对所有数据重新建模（不设验证集），相当于用7天预测最后一天，效果有所提升\n",
    "线下分数: 0.08243\n",
    "线上分数：0.08217\n",
    "1.说明广告类目属性对比特征集起到作用了\n",
    "2.不能用前6天的数据预测第8天（线上那一天），可以尝试用线上那天之前的6天进行预测\n",
    "20180411\n",
    "v2.2在v2.1的基础上用上线那天之前的6天进行预测\n",
    "线下分数: 0.08243\n",
    "线上分数：0.08211\n",
    "20180412\n",
    "v3.0 加入了部分商店特征（转化率），出现过拟合，初步原因可能是使用了预测数据集的商店特征（预先知道了是否要购买广告）\n",
    "线下分数: 0.07922\n",
    "线上分数：0.08953\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
